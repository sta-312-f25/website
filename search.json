[
  {
    "objectID": "troubleshooting.html",
    "href": "troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Problem: I cannot log in.\n\nDouble-check that you are using your school email and the correct password.\n\nProblem: My session is stuck in “Queued”.\n\nRefresh your page\n\nWait a few minutes; this can happen when servers are busy.\n\nIf it does not start after 2 minutes, try logging out and back in.\n\nProblem: RStudio crashes or disconnects.\n\nTry reducing the number of hours when launching your session.\n\nMake sure you have a stable internet connection.\n\nProblem: Quarto is not rendering.\n\nCheck that you saved your file with the .qmd extension.\n\nVerify that Quarto is installed by running quarto --version in the RStudio terminal.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "slides/partial-interp-slides.html#what-does-sse-tell-us",
    "href": "slides/partial-interp-slides.html#what-does-sse-tell-us",
    "title": "Untitled",
    "section": "What Does SSE Tell Us?",
    "text": "What Does SSE Tell Us?\n\nMeasures total unexplained variation\n\nSmaller SSE = better fit\n\nUnits are (units of y)²"
  },
  {
    "objectID": "slides/partial-interp-slides.html#sse-alone-isnt-enough",
    "href": "slides/partial-interp-slides.html#sse-alone-isnt-enough",
    "title": "Untitled",
    "section": "SSE alone isn’t enough",
    "text": "SSE alone isn’t enough\n\nDepends on sample size and scale of y\n\nNeed context for interpretation"
  },
  {
    "objectID": "slides/partial-interp-slides.html#from-sse-to-more-useful-metrics",
    "href": "slides/partial-interp-slides.html#from-sse-to-more-useful-metrics",
    "title": "Untitled",
    "section": "From SSE to More Useful Metrics",
    "text": "From SSE to More Useful Metrics\n\nMean Squared Error (MSE): \\[\\text{MSE} = \\frac{\\text{SSE}}{n-p} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p}\\]"
  },
  {
    "objectID": "slides/partial-interp-slides.html#why-n-p-instead-of-n",
    "href": "slides/partial-interp-slides.html#why-n-p-instead-of-n",
    "title": "Untitled",
    "section": "Why \\((n-p)\\) instead of \\(n\\)?",
    "text": "Why \\((n-p)\\) instead of \\(n\\)?\n\n\\(p\\) = number of parameters estimated\n\nCorrects for degrees of freedom used (provides unbiased estimate of error variance)"
  },
  {
    "objectID": "slides/partial-interp-slides.html#interpreting-coefficients",
    "href": "slides/partial-interp-slides.html#interpreting-coefficients",
    "title": "Untitled",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\nSimple Linear Regression: \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\)\n\n\\(\\beta_0\\) (intercept): Expected value of \\(y\\) when \\(x = 0\\)\n\\(\\beta_1\\) (slope): Expected change in \\(y\\) for a 1-unit increase in \\(x\\)"
  },
  {
    "objectID": "slides/partial-interp-slides.html#interpreting-coefficients-1",
    "href": "slides/partial-interp-slides.html#interpreting-coefficients-1",
    "title": "Untitled",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\nMultiple Regression: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\)\n\n\\(\\beta_1\\): Expected change in \\(y\\) for a 1-unit increase in \\(x_1\\), holding \\(x_2\\) constant\n\\(\\beta_2\\): Expected change in \\(y\\) for a 1-unit increase in \\(x_2\\), holding \\(x_1\\) constant"
  },
  {
    "objectID": "slides/partial-interp-slides.html#you-try-coefficient-interpretation",
    "href": "slides/partial-interp-slides.html#you-try-coefficient-interpretation",
    "title": "Untitled",
    "section": "You Try: Coefficient Interpretation",
    "text": "You Try: Coefficient Interpretation\n\nGiven the regression: \\(\\text{Salary} = 40000 + 2000 \\times \\text{YearsExperience} + 5000 \\times \\text{HasDegree}\\)\nWhere HasDegree = 1 if person has degree, 0 otherwise\nInterpret each coefficient:\n\nWhat does 40000 represent?\n\nWhat does 2000 represent?\n\nWhat does 5000 represent?\n\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/partial-interp-slides.html#practical-exercise",
    "href": "slides/partial-interp-slides.html#practical-exercise",
    "title": "Untitled",
    "section": " Practical Exercise",
    "text": "Practical Exercise\n\nLog in to RStudio Pro.\nUsing the teengamb dataset from the faraway package,\nPredict gambling expenditure using sex, status, and income\nDo this 3 ways:\n\n\nusing the derivation from our derivative of the SSE\nusing QR decomposition\nusing the lm function"
  },
  {
    "objectID": "slides/07-slides.html#topics-covered",
    "href": "slides/07-slides.html#topics-covered",
    "title": "Midterm Review",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nMatrix fundamentals for regression\nLeast squares estimation\nQR decomposition\nGauss-Markov theorem\nSums of squares decomposition\nHypothesis testing"
  },
  {
    "objectID": "slides/07-slides.html#basic-matrix-notation",
    "href": "slides/07-slides.html#basic-matrix-notation",
    "title": "Midterm Review",
    "section": "Basic Matrix Notation",
    "text": "Basic Matrix Notation\nThe Linear Model: \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\nWhere:\n\n\n\\(\\mathbf{y}\\) is an \\(n \\times 1\\) vector of responses\n\\(\\mathbf{X}\\) is an \\(n \\times p\\) design matrix\n\\(\\boldsymbol{\\beta}\\) is a \\(p \\times 1\\) vector of parameters\n\\(\\boldsymbol{\\varepsilon}\\) is an \\(n \\times 1\\) vector of errors"
  },
  {
    "objectID": "slides/07-slides.html#the-design-matrix",
    "href": "slides/07-slides.html#the-design-matrix",
    "title": "Midterm Review",
    "section": "The Design Matrix",
    "text": "The Design Matrix\n\\[\\mathbf{X} = \\begin{bmatrix}1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\1 & x_{n1} & x_{n2} & \\cdots & x_{np}\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/07-slides.html#hat-notation",
    "href": "slides/07-slides.html#hat-notation",
    "title": "Midterm Review",
    "section": "Hat Notation",
    "text": "Hat Notation\nParameters vs. Estimates:\n\n\\(\\beta_0, \\beta_1\\) = true (unknown) parameters\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) = estimated parameters from data\n\nObserved vs. Predicted:\n\n\\(y_i\\) = observed response values\n\n\\(\\hat{y}_i\\) = predicted values from model"
  },
  {
    "objectID": "slides/07-slides.html#residuals",
    "href": "slides/07-slides.html#residuals",
    "title": "Midterm Review",
    "section": "Residuals",
    "text": "Residuals\nDefinition: Difference between observed and predicted values\n\\[\\hat\\varepsilon_i = y_i - \\hat{y}_i\\]\nIn matrix form:\n\\[\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{y} - \\hat{\\mathbf{y}}\\]"
  },
  {
    "objectID": "slides/07-slides.html#what-are-we-minimizing",
    "href": "slides/07-slides.html#what-are-we-minimizing",
    "title": "Midterm Review",
    "section": "What Are We Minimizing?",
    "text": "What Are We Minimizing?\nSum of Squared Errors:\n\\[\\text{SSE} = (\\mathbf{y}- \\mathbf{X}\\boldsymbol\\beta)^T(\\mathbf{y}- \\mathbf{X}\\boldsymbol\\beta)\\]"
  },
  {
    "objectID": "slides/07-slides.html#deriving-the-ols-solution",
    "href": "slides/07-slides.html#deriving-the-ols-solution",
    "title": "Midterm Review",
    "section": "Deriving the OLS Solution",
    "text": "Deriving the OLS Solution\nTake derivative with respect to \\(\\boldsymbol{\\beta}\\) and set to zero:\n\\[\\frac{\\partial \\text{SSE}}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\\]\nRearrange: \\[\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\\]\nThese are the normal equations"
  },
  {
    "objectID": "slides/07-slides.html#the-ols-estimator",
    "href": "slides/07-slides.html#the-ols-estimator",
    "title": "Midterm Review",
    "section": "The OLS Estimator",
    "text": "The OLS Estimator\nSolve for \\(\\hat{\\boldsymbol{\\beta}}\\):\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nThis is the ordinary least squares estimator"
  },
  {
    "objectID": "slides/07-slides.html#geometric-interpretation",
    "href": "slides/07-slides.html#geometric-interpretation",
    "title": "Midterm Review",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\n\n\nRegression finds the projection of \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\)\n\n\\(\\hat{\\mathbf{y}}\\) is the closest point in the column space to \\(\\mathbf{y}\\)\n\nResiduals are perpendicular to the column space: \\(\\mathbf{X}^T\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0}\\)\n\nThis orthogonality guarantees minimum distance"
  },
  {
    "objectID": "slides/07-slides.html#the-hat-matrix",
    "href": "slides/07-slides.html#the-hat-matrix",
    "title": "Midterm Review",
    "section": "The Hat Matrix",
    "text": "The Hat Matrix\nDefinition: \\[\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\]\nWhat it does: \\[\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/07-slides.html#hat-matrix-properties",
    "href": "slides/07-slides.html#hat-matrix-properties",
    "title": "Midterm Review",
    "section": "Hat Matrix Properties",
    "text": "Hat Matrix Properties\nSymmetric: \\(\\mathbf{H}^T = \\mathbf{H}\\)\nIdempotent: \\(\\mathbf{H}^2 = \\mathbf{H}\\)\n\\(\\mathbf{I} - \\mathbf{H}\\) is also symmetric and idempotent\n\\[\\hat{\\boldsymbol{\\varepsilon}} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/07-slides.html#why-not-just-use-xtx-1",
    "href": "slides/07-slides.html#why-not-just-use-xtx-1",
    "title": "Midterm Review",
    "section": "Why Not Just Use \\((X^TX)^{-1}\\)?",
    "text": "Why Not Just Use \\((X^TX)^{-1}\\)?\nProblems with the traditional approach:\n\nComputing \\(\\mathbf{X}^T\\mathbf{X}\\) can be numerically unstable\n\nCondition number gets squared: \\(\\kappa(\\mathbf{X}^T\\mathbf{X}) = \\kappa(\\mathbf{X})^2\\)\n\nQR decomposition provides a more stable solution"
  },
  {
    "objectID": "slides/07-slides.html#what-is-qr-decomposition",
    "href": "slides/07-slides.html#what-is-qr-decomposition",
    "title": "Midterm Review",
    "section": "What is QR Decomposition?",
    "text": "What is QR Decomposition?\nAny matrix \\(\\mathbf{X}\\) can be decomposed as:\n\\[\\mathbf{X} = \\mathbf{Q}\\mathbf{R}\\]\nWhere:\n\n\\(\\mathbf{Q}\\) is orthogonal: \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\)\n\n\\(\\mathbf{R}\\) is upper triangular"
  },
  {
    "objectID": "slides/07-slides.html#qr-solution-to-least-squares",
    "href": "slides/07-slides.html#qr-solution-to-least-squares",
    "title": "Midterm Review",
    "section": "QR Solution to Least Squares",
    "text": "QR Solution to Least Squares\nTraditional: \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)\nWith QR: If \\(\\mathbf{X} = \\mathbf{Q}\\mathbf{R}\\), then:\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{R}^T\\mathbf{R})^{-1}\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{y} \\\\\n&= \\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}\n\\end{align}\\]\nSolve using back substitution: \\(\\mathbf{R}\\hat{\\boldsymbol{\\beta}} = \\mathbf{Q}^T\\mathbf{y}\\)"
  },
  {
    "objectID": "slides/07-slides.html#back-substitution",
    "href": "slides/07-slides.html#back-substitution",
    "title": "Midterm Review",
    "section": "Back Substitution",
    "text": "Back Substitution\nUpper triangular system is easy to solve:\n\\[\\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ 0 & r_{22} & r_{23} \\\\ 0 & 0 & r_{33} \\end{bmatrix} \\begin{bmatrix} \\hat\\beta_1 \\\\ \\hat\\beta_2 \\\\ \\hat\\beta_3 \\end{bmatrix} = \\begin{bmatrix} q_1 \\\\ q_2 \\\\ q_3 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/07-slides.html#back-substitution-1",
    "href": "slides/07-slides.html#back-substitution-1",
    "title": "Midterm Review",
    "section": "Back Substitution",
    "text": "Back Substitution\nWork backwards:\n\nSolve for \\(\\hat\\beta_3\\) from bottom row\nSubstitute into second row, solve for \\(\\hat\\beta_2\\)\nSubstitute into first row, solve for \\(\\hat\\beta_1\\)"
  },
  {
    "objectID": "slides/07-slides.html#qr-advantages",
    "href": "slides/07-slides.html#qr-advantages",
    "title": "Midterm Review",
    "section": "QR Advantages",
    "text": "QR Advantages\n\nOnly need back substitution, not full matrix inversion\n\nMore numerically stable\n\nAvoids squaring the condition number\n\nHat matrix: \\(\\mathbf{H} = \\mathbf{Q}\\mathbf{Q}^T\\)"
  },
  {
    "objectID": "slides/07-slides.html#properties-of-random-vectors",
    "href": "slides/07-slides.html#properties-of-random-vectors",
    "title": "Midterm Review",
    "section": "Properties of Random Vectors",
    "text": "Properties of Random Vectors\nExpected value (linearity): \\[E[\\mathbf{A}\\mathbf{Y} + \\mathbf{b}] = \\mathbf{A}E[\\mathbf{Y}] + \\mathbf{b}\\]\nVariance-covariance transformation: \\[\\text{Var}(\\mathbf{A}\\mathbf{Y} + \\mathbf{b}) = \\mathbf{A}\\text{Var}(\\mathbf{Y})\\mathbf{A}^T\\]"
  },
  {
    "objectID": "slides/07-slides.html#gauss-markov-assumptions",
    "href": "slides/07-slides.html#gauss-markov-assumptions",
    "title": "Midterm Review",
    "section": "Gauss-Markov Assumptions",
    "text": "Gauss-Markov Assumptions\n\nLinearity: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\)\nZero mean errors: \\(E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\)\nHomoscedasticity & independence: \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\)\nFull rank: \\(\\mathbf{X}\\) has full column rank"
  },
  {
    "objectID": "slides/07-slides.html#the-gauss-markov-theorem",
    "href": "slides/07-slides.html#the-gauss-markov-theorem",
    "title": "Midterm Review",
    "section": "The Gauss-Markov Theorem",
    "text": "The Gauss-Markov Theorem\nTheorem: Under the GM assumptions, OLS is BLUE\nBLUE = Best Linear Unbiased Estimator\n\nLinear: \\(\\hat{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{y}\\) for some matrix \\(\\mathbf{C}\\)\n\nUnbiased: \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\nBest: Smallest variance among all linear unbiased estimators"
  },
  {
    "objectID": "slides/07-slides.html#ols-is-unbiased",
    "href": "slides/07-slides.html#ols-is-unbiased",
    "title": "Midterm Review",
    "section": "OLS is Unbiased",
    "text": "OLS is Unbiased\nShow: \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "slides/07-slides.html#why-ols-has-minimum-variance",
    "href": "slides/07-slides.html#why-ols-has-minimum-variance",
    "title": "Midterm Review",
    "section": "Why OLS Has Minimum Variance",
    "text": "Why OLS Has Minimum Variance\nBe able to demonstrate why!"
  },
  {
    "objectID": "slides/07-slides.html#the-three-key-quantities",
    "href": "slides/07-slides.html#the-three-key-quantities",
    "title": "Midterm Review",
    "section": "The Three Key Quantities",
    "text": "The Three Key Quantities\nTotal Sum of Squares (TSS): \\[\\text{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\mathbf{y}^T\\mathbf{y} - n\\bar{y}^2\\]"
  },
  {
    "objectID": "slides/07-slides.html#the-three-key-quantities-1",
    "href": "slides/07-slides.html#the-three-key-quantities-1",
    "title": "Midterm Review",
    "section": "The Three Key Quantities",
    "text": "The Three Key Quantities\nRegression Sum of Squares (SS\\(_{\\text{Reg}}\\)): \\[\\text{SS}_{\\text{Reg}} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 = \\mathbf{y}^T\\mathbf{H}\\mathbf{y} - n\\bar{y}^2\\]"
  },
  {
    "objectID": "slides/07-slides.html#the-three-key-quantities-2",
    "href": "slides/07-slides.html#the-three-key-quantities-2",
    "title": "Midterm Review",
    "section": "The Three Key Quantities",
    "text": "The Three Key Quantities\nSum of Squared Errors (SSE): \\[\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\mathbf{y}^T(\\mathbf{I} - \\mathbf{H})\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/07-slides.html#interpreting-the-quantities",
    "href": "slides/07-slides.html#interpreting-the-quantities",
    "title": "Midterm Review",
    "section": "Interpreting the Quantities",
    "text": "Interpreting the Quantities\nTSS: Total variation in the data\n“How spread out are my y-values?”\nSS\\(_{\\text{Reg}}\\): Variation explained by the model\n“How much variation did our model capture?”\nSSE: Variation left unexplained\n“How much did we miss with our model?”"
  },
  {
    "objectID": "slides/07-slides.html#the-fundamental-identity",
    "href": "slides/07-slides.html#the-fundamental-identity",
    "title": "Midterm Review",
    "section": "The Fundamental Identity",
    "text": "The Fundamental Identity\nTotal variation = Explained + Unexplained\n\\[\\text{TSS} = \\text{SS}_{\\text{Reg}} + \\text{SSE}\\]"
  },
  {
    "objectID": "slides/07-slides.html#coefficient-of-determination-r²",
    "href": "slides/07-slides.html#coefficient-of-determination-r²",
    "title": "Midterm Review",
    "section": "Coefficient of Determination (R²)",
    "text": "Coefficient of Determination (R²)\nDefinition: Proportion of variation explained by the model\n\\[R^2 = \\frac{\\text{SS}_{\\text{Reg}}}{\\text{TSS}} = 1 - \\frac{\\text{SSE}}{\\text{TSS}}\\]"
  },
  {
    "objectID": "slides/07-slides.html#estimating-sigma2",
    "href": "slides/07-slides.html#estimating-sigma2",
    "title": "Midterm Review",
    "section": "Estimating \\(\\sigma^2\\)",
    "text": "Estimating \\(\\sigma^2\\)\nTrue error variance: \\[\\text{Var}(\\varepsilon_i) = \\sigma^2\\]\nUnbiased estimator: \\[\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}\\]\nwhere \\(n-p\\) are the degrees of freedom (observations minus parameters)\nAlso called Mean Square Error (MSE)"
  },
  {
    "objectID": "slides/07-slides.html#testing-single-coefficients",
    "href": "slides/07-slides.html#testing-single-coefficients",
    "title": "Midterm Review",
    "section": "Testing Single Coefficients",
    "text": "Testing Single Coefficients\nNull hypothesis: \\(H_0: \\beta_j = 0\\)\nTest statistic: \\[t = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)} \\sim t_{n-p}\\]"
  },
  {
    "objectID": "slides/07-slides.html#testing-single-coefficients-1",
    "href": "slides/07-slides.html#testing-single-coefficients-1",
    "title": "Midterm Review",
    "section": "Testing Single Coefficients",
    "text": "Testing Single Coefficients\n\\[\\text{se}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}\\]\nP-value: \\(P(|T| \\geq |t|)\\) where \\(T \\sim t_{n-p}\\)"
  },
  {
    "objectID": "slides/07-slides.html#where-does-se-come-from",
    "href": "slides/07-slides.html#where-does-se-come-from",
    "title": "Midterm Review",
    "section": "Where Does SE Come From?",
    "text": "Where Does SE Come From?\nRecall: \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\)\n\nFor individual coefficient \\(j\\): \\[\\text{Var}(\\hat{\\beta}_j) = \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\]\nEstimate by replacing \\(\\sigma^2\\) with \\(\\hat{\\sigma}^2\\): \\[\\widehat{\\text{Var}}(\\hat{\\beta}_j) = \\hat{\\sigma}^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\]\n\nStandard error is the square root: \\[\\text{se}(\\hat{\\beta}_j) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\beta}_j)}\\]"
  },
  {
    "objectID": "slides/07-slides.html#overall-f-test",
    "href": "slides/07-slides.html#overall-f-test",
    "title": "Midterm Review",
    "section": "Overall F-Test",
    "text": "Overall F-Test\nTests whether the model is useful at all\nNull: \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} = 0\\)\n(all slopes equal zero, only intercept remains)\nTest statistic: \\[F = \\frac{\\text{SS}_{\\text{Reg}}/(p-1)}{\\text{SSE}/(n-p)} \\sim F_{p-1, n-p}\\]"
  },
  {
    "objectID": "slides/07-slides.html#f-test-interpretation",
    "href": "slides/07-slides.html#f-test-interpretation",
    "title": "Midterm Review",
    "section": "F-Test Interpretation",
    "text": "F-Test Interpretation\n\\[F = \\frac{\\text{Mean Square Regression}}{\\text{Mean Square Error}}\\]\nLarge F: Model explains a lot relative to noise\nSmall F: Model doesn’t explain much more than noise"
  },
  {
    "objectID": "slides/07-slides.html#connection-to-r²",
    "href": "slides/07-slides.html#connection-to-r²",
    "title": "Midterm Review",
    "section": "Connection to R²",
    "text": "Connection to R²\nAlternative F-statistic form: \\[F = \\frac{R^2/(p-1)}{(1-R^2)/(n-p)}\\]\nThis shows the F-test is testing whether R² is significantly different from 0"
  },
  {
    "objectID": "slides/07-slides.html#general-linear-hypotheses",
    "href": "slides/07-slides.html#general-linear-hypotheses",
    "title": "Midterm Review",
    "section": "General Linear Hypotheses",
    "text": "General Linear Hypotheses\nFramework: \\(H_0: \\mathbf{C}\\boldsymbol{\\beta} = \\mathbf{d}\\)\nWhere:\n\n\\(\\mathbf{C}\\) is a contrast matrix (\\(q \\times p\\))\n\\(\\mathbf{d}\\) is a vector of constants\n\\(q\\) is the number of restrictions"
  },
  {
    "objectID": "slides/07-slides.html#general-linear-hypotheses-1",
    "href": "slides/07-slides.html#general-linear-hypotheses-1",
    "title": "Midterm Review",
    "section": "General Linear Hypotheses",
    "text": "General Linear Hypotheses\nTest statistic: \\[F = \\frac{(\\mathbf{C}\\hat{\\boldsymbol{\\beta}} - \\mathbf{d})^T[\\mathbf{C}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{C}^T]^{-1}(\\mathbf{C}\\hat{\\boldsymbol{\\beta}} - \\mathbf{d})/q}{\\text{SSE}/(n-p)}\\]\nUnder \\(H_0\\): \\(F \\sim F_{q, n-p}\\)"
  },
  {
    "objectID": "slides/07-slides.html#examples-of-linear-hypotheses",
    "href": "slides/07-slides.html#examples-of-linear-hypotheses",
    "title": "Midterm Review",
    "section": "Examples of Linear Hypotheses",
    "text": "Examples of Linear Hypotheses\nSingle coefficient: \\(H_0: \\beta_1 = 0\\) \\[\\mathbf{C} = [0, 1, 0, 0], \\quad \\mathbf{d} = 0\\]\nEquality of coefficients: \\(H_0: \\beta_1 = \\beta_2\\) \\[\\mathbf{C} = [0, 1, -1, 0], \\quad \\mathbf{d} = 0\\]"
  },
  {
    "objectID": "slides/07-slides.html#examples-of-linear-hypotheses-1",
    "href": "slides/07-slides.html#examples-of-linear-hypotheses-1",
    "title": "Midterm Review",
    "section": "Examples of Linear Hypotheses",
    "text": "Examples of Linear Hypotheses\nMultiple restrictions: \\(H_0: \\beta_2 = \\beta_3 = 0\\) \\[\\mathbf{C} = \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}, \\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/07-slides.html#the-anova-table",
    "href": "slides/07-slides.html#the-anova-table",
    "title": "Midterm Review",
    "section": "The ANOVA Table",
    "text": "The ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSum of Squares\nMean Square\nF\n\n\n\n\nRegression\n\\(p-1\\)\nSS\\(_{\\text{Reg}}\\)\nMS\\(_{\\text{Reg}}\\)\nMS\\(_{\\text{Reg}}\\)/MSE\n\n\nError\n\\(n-p\\)\nSSE\nMSE\n\n\n\nTotal\n\\(n-1\\)\nTSS"
  },
  {
    "objectID": "slides/07-slides.html#mean-squares",
    "href": "slides/07-slides.html#mean-squares",
    "title": "Midterm Review",
    "section": "Mean Squares",
    "text": "Mean Squares\n\nMS\\(_{\\text{Reg}}\\) = SS\\(_{\\text{Reg}}\\)/(p-1)\nMSE = SSE/(n-p) = \\(\\hat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides/07-slides.html#understanding-p-values",
    "href": "slides/07-slides.html#understanding-p-values",
    "title": "Midterm Review",
    "section": "Understanding P-values",
    "text": "Understanding P-values\nDefinition: Probability of observing a test statistic as extreme or more extreme than what we observed, assuming \\(H_0\\) is true\nFor t-tests (two-sided): \\[\\text{p-value} = P(|T| \\geq |t|) = 2 \\times P(T \\geq |t|)\\]"
  },
  {
    "objectID": "slides/07-slides.html#understanding-p-values-1",
    "href": "slides/07-slides.html#understanding-p-values-1",
    "title": "Midterm Review",
    "section": "Understanding P-values",
    "text": "Understanding P-values\nDefinition: Probability of observing a test statistic as extreme or more extreme than what we observed, assuming \\(H_0\\) is true\nFor F-tests (one-sided): \\[\\text{p-value} = P(F_{q,n-p} \\geq f)\\]"
  },
  {
    "objectID": "slides/05-slides.html#what-is-a-random-vector",
    "href": "slides/05-slides.html#what-is-a-random-vector",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "What is a Random Vector?",
    "text": "What is a Random Vector?\n\nDefinition: A collection of random variables in a vector\n\\[\\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/05-slides.html#expected-value-properties",
    "href": "slides/05-slides.html#expected-value-properties",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Expected Value Properties",
    "text": "Expected Value Properties\n\nLinearity property:\n\\[E[\\mathbf{A}\\mathbf{Y} + \\mathbf{b}] = \\mathbf{A}E[\\mathbf{Y}] + \\mathbf{b}\\]\n\n\nWhy this works: Expectation distributes over linear combinations\n\n\nKey assumption: \\(\\mathbf{A}\\) is constant, not random"
  },
  {
    "objectID": "slides/05-slides.html#variance-covariance-matrix",
    "href": "slides/05-slides.html#variance-covariance-matrix",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Variance-Covariance Matrix",
    "text": "Variance-Covariance Matrix\n\nDefinition:\n\\[\\text{Var}(\\mathbf{Y}) = E[(\\mathbf{Y} - E[\\mathbf{Y}])(\\mathbf{Y} - E[\\mathbf{Y}])^T]\\]\n\n\nThis creates an \\(n \\times n\\) matrix"
  },
  {
    "objectID": "slides/05-slides.html#structure-of-var-cov-matrix",
    "href": "slides/05-slides.html#structure-of-var-cov-matrix",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Structure of Var-Cov Matrix",
    "text": "Structure of Var-Cov Matrix\n\n\\[\\text{Var}(\\mathbf{Y}) = \\begin{bmatrix} \\text{Var}(Y_1) & \\text{Cov}(Y_1, Y_2) & \\cdots \\\\\\text{Cov}(Y_2, Y_1) & \\text{Var}(Y_2) & \\cdots \\\\\\vdots & \\vdots & \\ddots\\end{bmatrix}\\]\n\n\nDiagonal: individual variances\nOff-diagonal: covariances between pairs"
  },
  {
    "objectID": "slides/05-slides.html#variance-properties",
    "href": "slides/05-slides.html#variance-properties",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Variance Properties",
    "text": "Variance Properties\n\nKey transformation rule:\n\\[\\text{Var}(\\mathbf{A}\\mathbf{Y} + \\mathbf{b}) = \\mathbf{A}\\text{Var}(\\mathbf{Y})\\mathbf{A}^T\\]\n\n\nNote: Constants \\(\\mathbf{b}\\) don’t affect variance! But constant \\(\\mathbf{A}\\) does."
  },
  {
    "objectID": "slides/05-slides.html#variance-intuition",
    "href": "slides/05-slides.html#variance-intuition",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Variance Intuition",
    "text": "Variance Intuition\n\nThink of it as: \\((\\mathbf{A} \\times \\text{variability} \\times \\mathbf{A}^T)\\)\n\n\nMatrix \\(\\mathbf{A}\\) transforms the variables\nVariance gets “stretched” by \\(\\mathbf{A}\\) on both sides"
  },
  {
    "objectID": "slides/05-slides.html#you-try-simple-variance",
    "href": "slides/05-slides.html#you-try-simple-variance",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "You Try: Simple Variance",
    "text": "You Try: Simple Variance\n\nGiven: \\(\\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\end{bmatrix}\\) with \\(\\text{Var}(\\mathbf{Y}) = \\begin{bmatrix} 4 & 1 \\\\ 1 & 9 \\end{bmatrix}\\)\n\n\nFind: \\(\\text{Var}(2Y_1 + 3Y_2)\\)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/05-slides.html#you-try-setup",
    "href": "slides/05-slides.html#you-try-setup",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "You Try: Setup",
    "text": "You Try: Setup\n\nExpress as: \\(\\mathbf{A}\\mathbf{Y}\\) where \\(\\mathbf{A} = [2, 3]\\)\n\n\nApply formula:\n\\[\\text{Var}(2Y_1 + 3Y_2) = \\mathbf{A}\\text{Var}(\\mathbf{Y})\\mathbf{A}^T\\]"
  },
  {
    "objectID": "slides/05-slides.html#you-try-calculation",
    "href": "slides/05-slides.html#you-try-calculation",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "You Try: Calculation",
    "text": "You Try: Calculation\n\n\\[[2, 3] \\begin{bmatrix} 4 & 1 \\\\ 1 & 9 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\]\n\n\n\\[= [2, 3] \\begin{bmatrix} 11 \\\\ 29 \\end{bmatrix} = 109\\]"
  },
  {
    "objectID": "slides/05-slides.html#verification",
    "href": "slides/05-slides.html#verification",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": " Verification",
    "text": "Verification\n\n# Using matrix formula\nSigma &lt;- matrix(c(4, 1, 1, 9), 2, 2)\nA &lt;- matrix(c(2, 3), 1, 2)\nA %*% Sigma %*% t(A)\n\n     [,1]\n[1,]  109"
  },
  {
    "objectID": "slides/05-slides.html#simulation-check",
    "href": "slides/05-slides.html#simulation-check",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": " Simulation Check",
    "text": "Simulation Check\n\n# Verify by simulation\nset.seed(919)\nlibrary(MASS)\nY_sim &lt;- mvrnorm(100000, mu = c(0, 0), Sigma = Sigma)\nlinear_combo &lt;- 2 * Y_sim[,1] + 3 * Y_sim[,2]\nvar(linear_combo)\n\n[1] 108.8609"
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-model",
    "href": "slides/05-slides.html#linear-regression-model",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nThe model:\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\n\nWe will call this “ordinary least squares” (OLS)"
  },
  {
    "objectID": "slides/05-slides.html#the-big-question",
    "href": "slides/05-slides.html#the-big-question",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "The Big Question",
    "text": "The Big Question\n\nAmong all linear, unbiased estimators of \\(\\boldsymbol{\\beta}\\)…\n\n\nWhich one has the smallest variance?"
  },
  {
    "objectID": "slides/05-slides.html#gauss-markov-assumptions",
    "href": "slides/05-slides.html#gauss-markov-assumptions",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Gauss-Markov Assumptions",
    "text": "Gauss-Markov Assumptions\n\nAssumption 1: Linearity\nThe model is \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\)\n\n\nAssumption 2: Zero mean errors\n\\(E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/05-slides.html#more-gm-assumptions",
    "href": "slides/05-slides.html#more-gm-assumptions",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "More GM Assumptions",
    "text": "More GM Assumptions\n\nAssumption 3: Constant variance & independence\n\\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\)\n\n\nAssumption 4: Full rank\n\\(\\mathbf{X}\\) has full column rank (no perfect multicollinearity)"
  },
  {
    "objectID": "slides/05-slides.html#what-is-assumption-3",
    "href": "slides/05-slides.html#what-is-assumption-3",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "What is Assumption 3?",
    "text": "What is Assumption 3?\n\nHomoscedasticity: All errors have same variance \\(\\sigma^2\\)\n\\(\\text{Var}(\\varepsilon_i) = \\sigma^2 \\text{ for all } i\\)\n\n\nIndependence: Errors are uncorrelated\n\\(\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0 \\text{ for } i \\neq j\\)"
  },
  {
    "objectID": "slides/05-slides.html#linear-unbiased-estimators",
    "href": "slides/05-slides.html#linear-unbiased-estimators",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Linear Unbiased Estimators",
    "text": "Linear Unbiased Estimators\n\nLinear estimator: \\(\\tilde{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{y}\\)\nwhere \\(\\mathbf{C}\\) doesn’t depend on \\(\\mathbf{y}\\)\n\n\nUnbiased: \\(E[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "slides/05-slides.html#examples-of-linear-estimators",
    "href": "slides/05-slides.html#examples-of-linear-estimators",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Examples of Linear Estimators",
    "text": "Examples of Linear Estimators\n\nOLS estimator:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\n\n\nAny weighted average of the data"
  },
  {
    "objectID": "slides/05-slides.html#the-gauss-markov-theorem-1",
    "href": "slides/05-slides.html#the-gauss-markov-theorem-1",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "The Gauss-Markov Theorem",
    "text": "The Gauss-Markov Theorem\n\nTheorem: Under GM assumptions, OLS is BLUE\n\n\nBLUE = Best Linear Unbiased Estimator\n“Best” = smallest variance"
  },
  {
    "objectID": "slides/05-slides.html#proof-overview",
    "href": "slides/05-slides.html#proof-overview",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Proof Overview",
    "text": "Proof Overview\n\nStep 1: Show OLS is unbiased\n\n\nStep 2: Find variance of OLS\n\n\nStep 3: Show any other linear unbiased estimator has larger variance"
  },
  {
    "objectID": "slides/05-slides.html#starting-point",
    "href": "slides/05-slides.html#starting-point",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Starting Point",
    "text": "Starting Point\n\nWe want to show: \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\n\nStart with the OLS formula:\n\\[E[\\hat{\\boldsymbol{\\beta}}] = E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}]\\]"
  },
  {
    "objectID": "slides/05-slides.html#substitute-the-model",
    "href": "slides/05-slides.html#substitute-the-model",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Substitute the Model",
    "text": "Substitute the Model\n\nReplace \\(\\mathbf{y}\\) with \\(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\):\n\n\n\\[E[\\hat{\\boldsymbol{\\beta}}] = E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})]\\]"
  },
  {
    "objectID": "slides/05-slides.html#distribute-the-matrix",
    "href": "slides/05-slides.html#distribute-the-matrix",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Distribute the Matrix",
    "text": "Distribute the Matrix\n\nMultiply through: \\[= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\varepsilon}]\\]"
  },
  {
    "objectID": "slides/05-slides.html#simplify-the-first-term",
    "href": "slides/05-slides.html#simplify-the-first-term",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Simplify the First Term",
    "text": "Simplify the First Term\n\nNote that: \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X} = \\mathbf{I}\\)\n\n\nSo we get: \\[= E[\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\varepsilon}]\\]"
  },
  {
    "objectID": "slides/05-slides.html#use-linearity-of-expectation",
    "href": "slides/05-slides.html#use-linearity-of-expectation",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Use Linearity of Expectation",
    "text": "Use Linearity of Expectation\n\nExpectation of a sum = sum of expectations: \\[= \\boldsymbol{\\beta} + E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\varepsilon}]\\]"
  },
  {
    "objectID": "slides/05-slides.html#important-note-β-as-fixed-parameter",
    "href": "slides/05-slides.html#important-note-β-as-fixed-parameter",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Important Note: β as Fixed Parameter",
    "text": "Important Note: β as Fixed Parameter\n\nIn classical regression: \\(\\beta\\) is a fixed but unknown parameter\n\n\nRandomness comes from: \\(\\varepsilon\\) (and therefore y), not from \\(\\beta\\)\n\n\nThis is why: we can pull \\(\\beta\\) out of expectations like a constant"
  },
  {
    "objectID": "slides/05-slides.html#move-constants-out",
    "href": "slides/05-slides.html#move-constants-out",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Move Constants Out",
    "text": "Move Constants Out\n\nSince \\(\\mathbf{X}\\) is fixed (not random): \\[= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T E[\\boldsymbol{\\varepsilon}]\\]"
  },
  {
    "objectID": "slides/05-slides.html#apply-zero-mean-assumption",
    "href": "slides/05-slides.html#apply-zero-mean-assumption",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Apply Zero Mean Assumption",
    "text": "Apply Zero Mean Assumption\n\nFrom Assumption 2: \\(E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\)\n\n\nTherefore: \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{0} = \\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "slides/05-slides.html#unbiasedness-complete",
    "href": "slides/05-slides.html#unbiasedness-complete",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Unbiasedness: Complete ✓",
    "text": "Unbiasedness: Complete ✓\n\nWe have shown: \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\n\nOLS is unbiased under GM assumptions"
  },
  {
    "objectID": "slides/05-slides.html#you-try-ols-variance",
    "href": "slides/05-slides.html#you-try-ols-variance",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "You Try: OLS Variance",
    "text": "You Try: OLS Variance\n\nCalculate \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}})\\) where:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\n\n\n\n\n−+\n08:00"
  },
  {
    "objectID": "slides/05-slides.html#hint-express-in-terms-of-errors",
    "href": "slides/05-slides.html#hint-express-in-terms-of-errors",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Hint: Express in Terms of Errors",
    "text": "Hint: Express in Terms of Errors\n\nFrom our unbiasedness proof, we found: \\[\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\varepsilon}\\]"
  },
  {
    "objectID": "slides/05-slides.html#use-variance-properties",
    "href": "slides/05-slides.html#use-variance-properties",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Use Variance Properties",
    "text": "Use Variance Properties\n\nSince \\(\\boldsymbol{\\beta}\\) is constant (has zero variance): \\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\text{Var}((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\varepsilon})\\]"
  },
  {
    "objectID": "slides/05-slides.html#apply-matrix-variance-formula",
    "href": "slides/05-slides.html#apply-matrix-variance-formula",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Apply Matrix Variance Formula",
    "text": "Apply Matrix Variance Formula\n\nLet \\(\\mathbf{A} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\), then: \\[\\text{Var}(\\mathbf{A}\\boldsymbol{\\varepsilon}) = \\mathbf{A}\\text{Var}(\\boldsymbol{\\varepsilon})\\mathbf{A}^T\\]"
  },
  {
    "objectID": "slides/05-slides.html#substitute-error-variance",
    "href": "slides/05-slides.html#substitute-error-variance",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Substitute Error Variance",
    "text": "Substitute Error Variance\n\nFrom Assumption 3: \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\)\n\n\nTherefore: \\[= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\cdot \\sigma^2\\mathbf{I} \\cdot \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\]"
  },
  {
    "objectID": "slides/05-slides.html#simplify",
    "href": "slides/05-slides.html#simplify",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Simplify",
    "text": "Simplify\n\nPull out the scalar \\(\\sigma^2\\): \\[= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\]\n\n\nFinal answer: \\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\]"
  },
  {
    "objectID": "slides/05-slides.html#the-challenge",
    "href": "slides/05-slides.html#the-challenge",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "The Challenge",
    "text": "The Challenge\n\nGoal: Show any other linear unbiased estimator has larger variance\n\n\nStrategy: Consider any linear unbiased estimator \\(\\tilde{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{y}\\)"
  },
  {
    "objectID": "slides/05-slides.html#unbiasedness-constraint",
    "href": "slides/05-slides.html#unbiasedness-constraint",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Unbiasedness Constraint",
    "text": "Unbiasedness Constraint\n\nFor \\(\\tilde{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{y}\\) to be unbiased: \\[E[\\mathbf{C}\\mathbf{y}] = E[\\mathbf{C}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})] = \\mathbf{C}\\mathbf{X}\\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "slides/05-slides.html#the-key-constraint",
    "href": "slides/05-slides.html#the-key-constraint",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "The Key Constraint",
    "text": "The Key Constraint\n\nThis must equal \\(\\boldsymbol{\\beta}\\) for any value of \\(\\boldsymbol{\\beta}\\)\n\n\nTherefore we need: \\(\\mathbf{C}\\mathbf{X} = \\mathbf{I}\\)"
  },
  {
    "objectID": "slides/05-slides.html#clever-decomposition",
    "href": "slides/05-slides.html#clever-decomposition",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Clever Decomposition",
    "text": "Clever Decomposition\n\nWrite any unbiased \\(\\mathbf{C}\\) as: \\(\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}\\)\n\n\nwhere \\(\\mathbf{D}\\) is some matrix"
  },
  {
    "objectID": "slides/05-slides.html#why-this-decomposition-is-clever",
    "href": "slides/05-slides.html#why-this-decomposition-is-clever",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Why This Decomposition is Clever",
    "text": "Why This Decomposition is Clever\n\nIntuition: Any estimator = OLS + some deviation\n\n\nKey insight: The deviation \\(\\mathbf{D}\\) can only add variance, never reduce it\n\n\nMathematical power: Separates what we know (OLS) from the unknown part"
  },
  {
    "objectID": "slides/05-slides.html#why-this-decomposition-works",
    "href": "slides/05-slides.html#why-this-decomposition-works",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Why This Decomposition Works",
    "text": "Why This Decomposition Works\n\nCheck the constraint \\(\\mathbf{C}\\mathbf{X} = \\mathbf{I}\\): \\([(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}]\\mathbf{X} = \\mathbf{I} + \\mathbf{D}\\mathbf{X}\\)"
  },
  {
    "objectID": "slides/05-slides.html#constraint-on-d",
    "href": "slides/05-slides.html#constraint-on-d",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Constraint on D",
    "text": "Constraint on D\n\nFor the constraint to hold, we need: \\[\\mathbf{D}\\mathbf{X} = \\mathbf{0}\\]\n\n\nThis is the key restriction on \\(\\mathbf{D}\\)"
  },
  {
    "objectID": "slides/05-slides.html#variance-of-alternative-estimator",
    "href": "slides/05-slides.html#variance-of-alternative-estimator",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Variance of Alternative Estimator",
    "text": "Variance of Alternative Estimator\n\nStart with: \\[\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) = \\text{Var}(\\mathbf{C}\\mathbf{y}) = \\mathbf{C}\\text{Var}(\\mathbf{y})\\mathbf{C}^T\\]"
  },
  {
    "objectID": "slides/05-slides.html#substitute-error-variance-1",
    "href": "slides/05-slides.html#substitute-error-variance-1",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Substitute Error Variance",
    "text": "Substitute Error Variance\n\nSince \\(\\text{Var}(\\mathbf{y}) = \\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\): \\[= \\sigma^2\\mathbf{C}\\mathbf{C}^T\\]"
  },
  {
    "objectID": "slides/05-slides.html#substitute-our-decomposition",
    "href": "slides/05-slides.html#substitute-our-decomposition",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Substitute Our Decomposition",
    "text": "Substitute Our Decomposition\n\nReplace \\(\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}\\): \\[\\mathbf{C}\\mathbf{C}^T = [(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}][(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}]^T\\]"
  },
  {
    "objectID": "slides/05-slides.html#expand-the-product",
    "href": "slides/05-slides.html#expand-the-product",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Expand the Product",
    "text": "Expand the Product\n\nThis gives us four terms: \\[= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{D}^T\\] \\[+ \\mathbf{D}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} + \\mathbf{D}\\mathbf{D}^T\\]"
  },
  {
    "objectID": "slides/05-slides.html#cross-terms-vanish",
    "href": "slides/05-slides.html#cross-terms-vanish",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Cross Terms Vanish",
    "text": "Cross Terms Vanish\n\nSince \\(\\mathbf{D}\\mathbf{X} = \\mathbf{0}\\):\n\n\\(\\mathbf{D}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} = \\mathbf{0}\\)\n\n\n\n\n\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{D}^T = (\\mathbf{D}\\mathbf{X})^T(\\mathbf{X}^T\\mathbf{X})^{-1} = \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/05-slides.html#simplified-result",
    "href": "slides/05-slides.html#simplified-result",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Simplified Result",
    "text": "Simplified Result\n\nAfter cancellation: \\[\\mathbf{C}\\mathbf{C}^T = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} + \\mathbf{D}\\mathbf{D}^T\\]\n\n\nWhich simplifies to: \\[= (\\mathbf{X}^T\\mathbf{X})^{-1} + \\mathbf{D}\\mathbf{D}^T\\]"
  },
  {
    "objectID": "slides/05-slides.html#the-final-comparison",
    "href": "slides/05-slides.html#the-final-comparison",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "The Final Comparison",
    "text": "The Final Comparison\n\nTherefore: \\[\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) = \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1} + \\mathbf{D}\\mathbf{D}^T]\\]"
  },
  {
    "objectID": "slides/05-slides.html#key-mathematical-fact",
    "href": "slides/05-slides.html#key-mathematical-fact",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Key Mathematical Fact",
    "text": "Key Mathematical Fact\n\n\\(\\mathbf{D}\\mathbf{D}^T\\) is positive semi-definite\n\n\nThis means: \\(\\mathbf{D}\\mathbf{D}^T \\geq \\mathbf{0}\\) (in matrix sense)"
  },
  {
    "objectID": "slides/05-slides.html#conclusion-ols-is-best",
    "href": "slides/05-slides.html#conclusion-ols-is-best",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Conclusion: OLS is Best",
    "text": "Conclusion: OLS is Best\n\nSince \\(\\mathbf{D}\\mathbf{D}^T \\geq \\mathbf{0}\\): \\[\\text{Var}(\\tilde{\\boldsymbol{\\beta}}) \\geq \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} = \\text{Var}(\\hat{\\boldsymbol{\\beta}})\\]"
  },
  {
    "objectID": "slides/05-slides.html#proof-complete-ols-is-blue",
    "href": "slides/05-slides.html#proof-complete-ols-is-blue",
    "title": "Random Vectors and the Gauss-Markov Theorem",
    "section": "Proof Complete: OLS is BLUE",
    "text": "Proof Complete: OLS is BLUE\n\nWe have shown:\n\nOLS is linear: \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)\n\n\n\n\nOLS is unbiased: \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\n\n\nAnd OLS has minimum variance among all linear unbiased estimators"
  },
  {
    "objectID": "slides/03-slides.html#before-you-fit-a-model",
    "href": "slides/03-slides.html#before-you-fit-a-model",
    "title": "Linear Regression Fundamentals",
    "section": "Before You Fit a Model",
    "text": "Before You Fit a Model\n\n📖 Understand the content matter\nAs a statistician I collaborate frequently with subject matter experts to ensure that I understand the context of the problem at hand.\n\n\n❓ Understand the objective\nIt is crucial to understand what the objectives are. Ideally, these are set a priori, or if exploratory analyses are being done that is very explicit from beginning to end."
  },
  {
    "objectID": "slides/03-slides.html#before-you-fit-a-model-1",
    "href": "slides/03-slides.html#before-you-fit-a-model-1",
    "title": "Linear Regression Fundamentals",
    "section": "Before You Fit a Model",
    "text": "Before You Fit a Model\n\n📏 Understand where the data came from\nWas this observational or experimental data? Is any data missing? What are the units? Are there data entry issues?\n\n\n🧹 Get the data into a tidy, analyzable form\nOften we get data in a form that is not easily analyzable. In this class, we will be focusing mostly on statistical methodology once the data is in an analyzable format, but just because it is analyzable doesn’t mean the analysis choice is obvious."
  },
  {
    "objectID": "slides/03-slides.html#before-you-fit-a-model-2",
    "href": "slides/03-slides.html#before-you-fit-a-model-2",
    "title": "Linear Regression Fundamentals",
    "section": "Before You Fit a Model",
    "text": "Before You Fit a Model\n💃 Determine the appropriate model\nIn this class we are focusing on Linear Models. Linear models are not always appropriate. You must examine your data to determine whether a linear model is a good choice."
  },
  {
    "objectID": "slides/03-slides.html#is-a-linear-model-appropriate",
    "href": "slides/03-slides.html#is-a-linear-model-appropriate",
    "title": "Linear Regression Fundamentals",
    "section": "Is a Linear Model Appropriate?",
    "text": "Is a Linear Model Appropriate?\n\nOutcome variable, \\(y\\) is continuous\nExplanatory variable(s), \\(X = \\{X_1, ..., X_p\\}\\) can take any form\nObservations are independent\nThe residuals are homoscedastic (Equal variance)\nThe residuals are normally distributed\nThe relationship between \\(X\\) and \\(y\\) is linear"
  },
  {
    "objectID": "slides/03-slides.html#calculating-a-sample-average",
    "href": "slides/03-slides.html#calculating-a-sample-average",
    "title": "Linear Regression Fundamentals",
    "section": "Calculating a sample average",
    "text": "Calculating a sample average\n\\[\\bar{Y}=\\frac{1}{N}\\sum_{i=1}^NY_i\\]\n\nHow could we write this using matrices?"
  },
  {
    "objectID": "slides/03-slides.html#calculating-a-sample-average-1",
    "href": "slides/03-slides.html#calculating-a-sample-average-1",
    "title": "Linear Regression Fundamentals",
    "section": "Calculating a sample average",
    "text": "Calculating a sample average\n\\[\\mathbf{A} = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/03-slides.html#calculating-a-sample-average-2",
    "href": "slides/03-slides.html#calculating-a-sample-average-2",
    "title": "Linear Regression Fundamentals",
    "section": "Calculating a sample average",
    "text": "Calculating a sample average\n\\[\\frac{1}{N}\\mathbf{A}^TY = \\frac{1}{N}\\begin{bmatrix}1 & 1 & ... & 1\\end{bmatrix}\\begin{bmatrix}Y_1\\\\Y_2\\\\\\vdots\\\\Y_n\\end{bmatrix}=\\bar{Y}\\]"
  },
  {
    "objectID": "slides/03-slides.html#in-r",
    "href": "slides/03-slides.html#in-r",
    "title": "Linear Regression Fundamentals",
    "section": " In R",
    "text": "In R\n\nset.seed(1)\n\ny &lt;- rnorm(100)\nN &lt;- length(y)\nY &lt;- matrix(y, nrow = N)\nA &lt;- matrix(1, nrow = N)\nt(A) %*% Y / N\n\n          [,1]\n[1,] 0.1088874\n\nmean(Y)\n\n[1] 0.1088874"
  },
  {
    "objectID": "slides/03-slides.html#calculating-a-variance",
    "href": "slides/03-slides.html#calculating-a-variance",
    "title": "Linear Regression Fundamentals",
    "section": "Calculating a Variance",
    "text": "Calculating a Variance\n\\[\\frac{1}{N}\\sum_{i=1}^N(Y_i-\\bar{Y})^2\\]\n\nHow could we write this using matrices"
  },
  {
    "objectID": "slides/03-slides.html#calculating-a-variance-1",
    "href": "slides/03-slides.html#calculating-a-variance-1",
    "title": "Linear Regression Fundamentals",
    "section": "Calculating a Variance",
    "text": "Calculating a Variance\n\\[\\mathbf{e} = \\begin{bmatrix}Y_1 - \\bar{Y}\\\\\\vdots \\\\Y_N-\\bar{Y}\\end{bmatrix}, \\frac{1}{N}\\mathbf{e}^T\\mathbf{e} = \\frac{1}{N}\\sum_{i=1}^N(Y_i-\\bar{Y})^2\\]"
  },
  {
    "objectID": "slides/03-slides.html#in-r-1",
    "href": "slides/03-slides.html#in-r-1",
    "title": "Linear Regression Fundamentals",
    "section": " In R",
    "text": "In R\n\nMultiplying a transpose of a matrix with another matrix is very common\n\nBecause R was made by and for statisticians, there is a function for this: crossprod\n\n\ncrossprod(A, Y) / N\n\n          [,1]\n[1,] 0.1088874"
  },
  {
    "objectID": "slides/03-slides.html#in-r-2",
    "href": "slides/03-slides.html#in-r-2",
    "title": "Linear Regression Fundamentals",
    "section": " In R",
    "text": "In R\n\nYou can just put one matrix if you want the transpose of a matrix multiplied by itself\n\n\nY_bar &lt;- crossprod(A, Y) / N\ne &lt;- y - c(Y_bar)\ncrossprod(e) / N\n\n          [,1]\n[1,] 0.7986945"
  },
  {
    "objectID": "slides/03-slides.html#the-linear-model",
    "href": "slides/03-slides.html#the-linear-model",
    "title": "Linear Regression Fundamentals",
    "section": "The Linear Model",
    "text": "The Linear Model\n\n\nStandard form: \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\n\n\n\nWhere:\n\n\\(\\mathbf{y}\\) is an \\(n \\times 1\\) vector of responses\n\n\\(\\mathbf{X}\\) is an \\(n \\times p\\) design matrix\n\n\\(\\boldsymbol{\\beta}\\) is a \\(p \\times 1\\) vector of parameters\n\n\\(\\boldsymbol{\\varepsilon}\\) is an \\(n \\times 1\\) vector of errors"
  },
  {
    "objectID": "slides/03-slides.html#the-design-matrix",
    "href": "slides/03-slides.html#the-design-matrix",
    "title": "Linear Regression Fundamentals",
    "section": "The Design Matrix",
    "text": "The Design Matrix\n\nSimple linear regression: \\[\\mathbf{X} = \\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/03-slides.html#the-design-matrix-1",
    "href": "slides/03-slides.html#the-design-matrix-1",
    "title": "Linear Regression Fundamentals",
    "section": "The Design Matrix",
    "text": "The Design Matrix\nMultiple regression: \\[\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/03-slides.html#what-about-hats",
    "href": "slides/03-slides.html#what-about-hats",
    "title": "Linear Regression Fundamentals",
    "section": "What About Hats?",
    "text": "What About Hats?\n\nHat notation indicates estimates or predicted values\n\n\nParameters vs. Estimates:\n\n\\(\\beta_0, \\beta_1\\) = true (unknown) parameters\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) = estimated parameters from data\n\n\n\n\nObserved vs. Predicted:\n\n\\(y_i\\) = observed response values\n\\(\\hat{y}_i\\) = predicted values from model"
  },
  {
    "objectID": "slides/03-slides.html#what-is-a-residual",
    "href": "slides/03-slides.html#what-is-a-residual",
    "title": "Linear Regression Fundamentals",
    "section": "What is a Residual?",
    "text": "What is a Residual?\n\nDefinition: A residual is the difference between an observed value and its predicted value from the model\n\n\nFormula:\n\\[\\hat\\varepsilon_i = y_i-\\hat{y}_i\\]"
  },
  {
    "objectID": "slides/03-slides.html#you-try-calculating-residuals",
    "href": "slides/03-slides.html#you-try-calculating-residuals",
    "title": "Linear Regression Fundamentals",
    "section": "You Try: Calculating Residuals",
    "text": "You Try: Calculating Residuals\n\nProblem: Given the regression equation \\(\\hat{y} = 2.3 + 1.5x\\) and the data points below, calculate the residual for each observation:\n\n\n\nx\ny\n\\(\\hat{y}\\)\n\\(\\hat{\\varepsilon}\\)\n\n\n\n\n1\n4.2\n?\n?\n\n\n3\n6.8\n?\n?\n\n\n5\n11.1\n?\n?\n\n\n\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/03-slides.html#lets-try-it-in-r",
    "href": "slides/03-slides.html#lets-try-it-in-r",
    "title": "Linear Regression Fundamentals",
    "section": "Let’s Try It in R",
    "text": "Let’s Try It in R\nProblem: Given the same regression equation \\(\\hat{y} = 2.3 + 1.5x\\) and \\(y = [4.2, 6.8, 11.1]\\), \\(x = [1, 3, 5]\\):\n\nPut the x values in a design matrix called X and the outcome in a vector called y\nPut the coefficients in a vector called beta\n\nMultiply them to get \\(\\hat{y}\\)\nSubtract from y to get residuals\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/03-slides.html#lets-try-it-in-r-solution",
    "href": "slides/03-slides.html#lets-try-it-in-r-solution",
    "title": "Linear Regression Fundamentals",
    "section": "Let’s Try It in R Solution",
    "text": "Let’s Try It in R Solution\n\n\n# 1. Create design matrix X\nX &lt;- matrix(c(1, 1, \n              1, 3, \n              1, 5),\n            byrow = TRUE, ncol = 2)\n# Create y\ny &lt;- c(4.2, 6.8, 11.1)\n\n# 2. Create beta vector\nbeta &lt;- c(2.3, 1.5)\n\n# 3. Calculate y_hat\ny_hat &lt;- X %*% beta\n\n# 4. Calculate residuals\nresiduals &lt;- y - y_hat\n\nresiduals\n\n\n     [,1]\n[1,]  0.4\n[2,]  0.0\n[3,]  1.3"
  },
  {
    "objectID": "slides/03-slides.html#the-goal-minimize-squared-errors",
    "href": "slides/03-slides.html#the-goal-minimize-squared-errors",
    "title": "Linear Regression Fundamentals",
    "section": "The Goal: Minimize Squared Errors",
    "text": "The Goal: Minimize Squared Errors\n\nSum of squared errors:\n\\[\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\hat\\varepsilon_i^2\\]\n\n\n\\[\\text{SSE} = (\\mathbf{y}- \\mathbf{X}\\boldsymbol\\beta)^T(\\mathbf{y}- \\mathbf{X}\\boldsymbol\\beta)\\]"
  },
  {
    "objectID": "slides/03-slides.html#you-try",
    "href": "slides/03-slides.html#you-try",
    "title": "Linear Regression Fundamentals",
    "section": "You Try",
    "text": "You Try\n\nProblem: Verify that these two expressions for SSE are the same:\nIndividual terms: \\[\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nMatrix form: \\[\\text{SSE} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]\nTask: Expand the matrix form and show it equals the summation form"
  },
  {
    "objectID": "slides/03-slides.html#visualizing-squared-residuals",
    "href": "slides/03-slides.html#visualizing-squared-residuals",
    "title": "Linear Regression Fundamentals",
    "section": " Visualizing Squared Residuals",
    "text": "Visualizing Squared Residuals"
  },
  {
    "objectID": "slides/03-slides.html#visualizing-squared-residuals-1",
    "href": "slides/03-slides.html#visualizing-squared-residuals-1",
    "title": "Linear Regression Fundamentals",
    "section": " Visualizing Squared Residuals",
    "text": "Visualizing Squared Residuals"
  },
  {
    "objectID": "slides/03-slides.html#application-exercise",
    "href": "slides/03-slides.html#application-exercise",
    "title": "Linear Regression Fundamentals",
    "section": " Application Exercise",
    "text": "Application Exercise\n\nGo to lucy.shinyapps.io/least-squares/.\nThis shows a scatter plot of 10 data points with a line estimating the relationship between x and y. Drag the blue points to change the line.\n\nSee if you can find a line that minimizes the sum of square errors\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/03-slides.html#what-were-really-doing",
    "href": "slides/03-slides.html#what-were-really-doing",
    "title": "Linear Regression Fundamentals",
    "section": "What We’re Really Doing",
    "text": "What We’re Really Doing\n\nThe regression equation: \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\n\n\nThe goal: Find \\(\\boldsymbol{\\beta}\\) that makes \\(\\mathbf{X}\\boldsymbol{\\beta}\\) as close as possible to \\(\\mathbf{y}\\)"
  },
  {
    "objectID": "slides/03-slides.html#understanding-the-vector-space",
    "href": "slides/03-slides.html#understanding-the-vector-space",
    "title": "Linear Regression Fundamentals",
    "section": "Understanding the Vector Space",
    "text": "Understanding the Vector Space\n\nIf we have \\(n\\) observations, we work in \\(n\\)-dimensional space\n\n\n\\(\\mathbf{y}\\) is a vector with \\(n\\) components (one value per observation)\n\n\n\\(\\mathbf{X}\\boldsymbol{\\beta}\\) is also a vector with \\(n\\) components (one prediction per observation)\n\n\nBoth vectors live in the same \\(n\\)-dimensional space"
  },
  {
    "objectID": "slides/03-slides.html#what-is-column-space",
    "href": "slides/03-slides.html#what-is-column-space",
    "title": "Linear Regression Fundamentals",
    "section": "What is Column Space?",
    "text": "What is Column Space?\n\nIn words: All possible predictions your model can make\n\n\nThink of it as: Every combination of your predictor variables"
  },
  {
    "objectID": "slides/03-slides.html#column-space-with-numbers",
    "href": "slides/03-slides.html#column-space-with-numbers",
    "title": "Linear Regression Fundamentals",
    "section": "Column Space with Numbers",
    "text": "Column Space with Numbers\n\nYour data: \\(x = [1, 2, 3]\\) with 3 observations\n\n\nDesign matrix: \\(\\mathbf{X} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix}\\)\n\n\nColumn space contains: All vectors of the form \\(\\beta_0 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} + \\beta_1 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/03-slides.html#examples-of-vectors-in-column-space",
    "href": "slides/03-slides.html#examples-of-vectors-in-column-space",
    "title": "Linear Regression Fundamentals",
    "section": "Examples of Vectors in Column Space",
    "text": "Examples of Vectors in Column Space\n\nExample 1: Choose \\(\\beta_0=0, \\beta_1=2\\) gives \\([2, 4, 6]\\)\n\n\nIn words: Intercept = 0, slope = 2, so predictions are [2, 4, 6]\n\n\nExample 2: Choose \\(\\beta_0=5, \\beta_1=0\\) gives \\([5, 5, 5]\\)\n\n\nIn words: Intercept = 5, slope = 0, so all predictions equal 5"
  },
  {
    "objectID": "slides/03-slides.html#the-fundamental-problem",
    "href": "slides/03-slides.html#the-fundamental-problem",
    "title": "Linear Regression Fundamentals",
    "section": "The Fundamental Problem",
    "text": "The Fundamental Problem\n\nYour actual observed data: \\(\\mathbf{y} = [2.1, 3.9, 5.8]\\)\n\n\nQuestion: Is there some \\(\\beta_0, \\beta_1\\) such that \\(\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{y}\\) exactly?\n\n\nIn other words: Is \\([2.1, 3.9, 5.8]\\) exactly equal to \\(\\beta_0[1,1,1] + \\beta_1[1,2,3]\\)?"
  },
  {
    "objectID": "slides/03-slides.html#why-we-usually-cant-hit-exactly",
    "href": "slides/03-slides.html#why-we-usually-cant-hit-exactly",
    "title": "Linear Regression Fundamentals",
    "section": "Why We Usually Can’t Hit Exactly",
    "text": "Why We Usually Can’t Hit Exactly\n\nAnswer: Usually no! Real data has irreducible error\n\nYour observed \\(\\mathbf{y}\\) typically doesn’t lie perfectly in the column space*\n\nIn words: Your data points don’t lie exactly on any straight line"
  },
  {
    "objectID": "slides/03-slides.html#the-geometric-solution",
    "href": "slides/03-slides.html#the-geometric-solution",
    "title": "Linear Regression Fundamentals",
    "section": "The Geometric Solution",
    "text": "The Geometric Solution\n\nSince we can’t hit \\(\\mathbf{y}\\) exactly, let’s get as close as possible\nFind the point in the column space that is closest to \\(\\mathbf{y}\\)\n\nThis closest point is the orthogonal projection of \\(\\mathbf{y}\\) onto the column space"
  },
  {
    "objectID": "slides/03-slides.html#the-residual-vector",
    "href": "slides/03-slides.html#the-residual-vector",
    "title": "Linear Regression Fundamentals",
    "section": "The Residual Vector",
    "text": "The Residual Vector\n\nDefinition: \\(\\hat\\varepsilon = \\mathbf{y} - \\hat{\\mathbf{y}}\\)\n\nIn words: The difference between what we observed and what we predicted\n\nThis represents the part of \\(\\mathbf{y}\\) we cannot explain with our model"
  },
  {
    "objectID": "slides/03-slides.html#key-geometric-property",
    "href": "slides/03-slides.html#key-geometric-property",
    "title": "Linear Regression Fundamentals",
    "section": "Key Geometric Property",
    "text": "Key Geometric Property\n\nThe residual vector is perpendicular to the column space\nMathematical notation: \\(\\hat\\varepsilon \\perp \\text{Col}(\\mathbf{X})\\)\n\nWhy this matters: Perpendicularity guarantees we found the closest point"
  },
  {
    "objectID": "slides/03-slides.html#why-perpendicular-means-closest",
    "href": "slides/03-slides.html#why-perpendicular-means-closest",
    "title": "Linear Regression Fundamentals",
    "section": "Why Perpendicular Means Closest?",
    "text": "Why Perpendicular Means Closest?\n\nImagine dropping a ball onto a table from above\n\nThe shortest path is straight down (perpendicular to the table)\n\nAny diagonal path would be longer\n\nThe same principle applies in higher dimensions"
  },
  {
    "objectID": "slides/03-slides.html#from-geometry-to-algebra",
    "href": "slides/03-slides.html#from-geometry-to-algebra",
    "title": "Linear Regression Fundamentals",
    "section": "From Geometry to Algebra",
    "text": "From Geometry to Algebra\n\nGeometric fact: \\(\\hat\\varepsilon \\perp \\text{Col}(\\mathbf{X})\\)\n\nThis means \\(\\hat\\varepsilon\\) is perpendicular to every vector in the column space\n\nSince columns of \\(\\mathbf{X}\\) span the column space, \\(\\hat\\varepsilon\\) is perpendicular to each column"
  },
  {
    "objectID": "slides/03-slides.html#mathematical-expression-of-perpendicularity",
    "href": "slides/03-slides.html#mathematical-expression-of-perpendicularity",
    "title": "Linear Regression Fundamentals",
    "section": "Mathematical Expression of Perpendicularity",
    "text": "Mathematical Expression of Perpendicularity\n\nIf \\(\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p]\\), then:\n\n\n\\(\\mathbf{x}_1^T\\hat\\varepsilon = 0\\), \\(\\mathbf{x}_2^T\\hat\\varepsilon = 0\\), …, \\(\\mathbf{x}_p^T\\hat\\varepsilon = 0\\)\n\n\nStacking these equations gives us: \\(\\mathbf{X}^T\\hat\\varepsilon = \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/03-slides.html#the-normal-equations",
    "href": "slides/03-slides.html#the-normal-equations",
    "title": "Linear Regression Fundamentals",
    "section": "The Normal Equations",
    "text": "The Normal Equations\n\nStarting from: \\(\\mathbf{X}^T\\hat\\varepsilon = \\mathbf{0}\\)\n\n\nSubstitute \\(\\hat\\varepsilon = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\):\n\\[\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}\\]\n\n\nIn words: “The residuals are perpendicular to every column of \\(\\mathbf{X}\\)”"
  },
  {
    "objectID": "slides/03-slides.html#you-try-matrix-algebra",
    "href": "slides/03-slides.html#you-try-matrix-algebra",
    "title": "Linear Regression Fundamentals",
    "section": "You Try: Matrix Algebra",
    "text": "You Try: Matrix Algebra\n\nStarting with: \\(\\mathbf{A}\\mathbf{x} + \\mathbf{b} = \\mathbf{c}\\)\nSolve for \\(\\mathbf{x}\\) step by step:\n\nFirst, isolate the \\(\\mathbf{A}\\mathbf{x}\\) term\n\nThen multiply both sides by \\(\\mathbf{A}^{-1}\\) (on the left!)\n\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/03-slides.html#expanding-the-normal-equations",
    "href": "slides/03-slides.html#expanding-the-normal-equations",
    "title": "Linear Regression Fundamentals",
    "section": "Expanding the Normal Equations",
    "text": "Expanding the Normal Equations\n\nStart with: \\(\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}\\)\n\n\nDistribute: \\(\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\\)\n\n\nMove the second term to the right side: \\(\\mathbf{X}^T\\mathbf{y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "slides/03-slides.html#solving-for-beta",
    "href": "slides/03-slides.html#solving-for-beta",
    "title": "Linear Regression Fundamentals",
    "section": "Solving for Beta",
    "text": "Solving for Beta\n\nWe have: \\(\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}\\)\n\n\nTo solve for \\(\\hat{\\boldsymbol{\\beta}}\\), multiply both sides by \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\):\n\n\nResult: \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)\n\n\nThis is the least squares solution!"
  },
  {
    "objectID": "slides/03-slides.html#the-hat-matrix",
    "href": "slides/03-slides.html#the-hat-matrix",
    "title": "Linear Regression Fundamentals",
    "section": "The Hat Matrix",
    "text": "The Hat Matrix\n\nDefinition: \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\n\nWhat it does: \\(\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\\)\n\n\nIn words: Takes your observed data and produces the closest possible predictions\n\n\nNickname: “Puts the hat on \\(\\mathbf{y}\\)” to get \\(\\hat{\\mathbf{y}}\\)"
  },
  {
    "objectID": "slides/03-slides.html#hat-matrix-properties",
    "href": "slides/03-slides.html#hat-matrix-properties",
    "title": "Linear Regression Fundamentals",
    "section": "Hat Matrix Properties",
    "text": "Hat Matrix Properties\n\nSymmetric: \\(\\mathbf{H}^T = \\mathbf{H}\\)\n\n\nIdempotent: \\(\\mathbf{H}^2 = \\mathbf{H}\\)"
  },
  {
    "objectID": "slides/03-slides.html#you-try-hat-matrix-property",
    "href": "slides/03-slides.html#you-try-hat-matrix-property",
    "title": "Linear Regression Fundamentals",
    "section": "You Try: Hat Matrix Property",
    "text": "You Try: Hat Matrix Property\n\nVerify that: \\(\\mathbf{H}^2 = \\mathbf{H}\\)\nHint: Substitute the definition \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\) and multiply it out\nRemember: \\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/03-slides.html#why-least-squares",
    "href": "slides/03-slides.html#why-least-squares",
    "title": "Linear Regression Fundamentals",
    "section": "Why “Least Squares”?",
    "text": "Why “Least Squares”?\n\nWe minimize: \\(\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\n\n\nIn words: Sum of squared differences between observed and predicted\n\n\nThis equals: \\(||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2\\)\n\n\nIn words: Squared distance between the observed vector and prediction vector"
  },
  {
    "objectID": "slides/03-slides.html#key-geometric-insights",
    "href": "slides/03-slides.html#key-geometric-insights",
    "title": "Linear Regression Fundamentals",
    "section": "Key Geometric Insights",
    "text": "Key Geometric Insights\n\n1. Regression is projection: Finding the closest point in the column space to \\(\\mathbf{y}\\)\n\n\n2. Orthogonality is key: Residuals perpendicular to column space guarantees minimum distance\n\n\n3. Hat matrix is the projection operator: \\(\\mathbf{H}\\) projects onto column space"
  },
  {
    "objectID": "slides/03-slides.html#you-try-1",
    "href": "slides/03-slides.html#you-try-1",
    "title": "Linear Regression Fundamentals",
    "section": " You Try",
    "text": "You Try\n\nTry this in R:\n1. x &lt;- 1:10; y &lt;- 2*x + rnorm(10)\n2. X &lt;- cbind(1, x) # Design matrix\n3. H &lt;- X %*% solve(crossprod(X)) %*% t(X) # Hat matrix\n4. Check: all.equal(H %*% H, H) # Verify idempotent\n5. e &lt;- y - H %*% y # Residuals\n6. Check: t(X) %*% e # Should be approximately zero\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/01-slides.html#section",
    "href": "slides/01-slides.html#section",
    "title": "Welcome to Linear Models",
    "section": "",
    "text": "“truth … is much too complicated to allow anything but approximations”\n- John von Neumann\n\n\n\n\nJohn von Neumann, wearer of funny hats via https://farkasdilemma.wordpress.com/2013/01/04/john-von-neumannm-wearer-of-funny-hats/"
  },
  {
    "objectID": "slides/01-slides.html#section-1",
    "href": "slides/01-slides.html#section-1",
    "title": "Welcome to Linear Models",
    "section": "",
    "text": "“All models are wrong, but some are useful”\n- George Box\n\n\n   \n\nDavidMCEddy at en.wikipedia  CC BY-SA 3.0 , via Wikimedia Commons"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model",
    "href": "slides/01-slides.html#is-this-a-linear-model",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?"
  },
  {
    "objectID": "slides/01-slides.html#what-is-the-equation",
    "href": "slides/01-slides.html#what-is-the-equation",
    "title": "Welcome to Linear Models",
    "section": "What is the equation?",
    "text": "What is the equation?\n\n\\[y = \\beta_0 + \\beta_1 x + \\varepsilon\\]"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model-1",
    "href": "slides/01-slides.html#is-this-a-linear-model-1",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?\n\n\n Where is \\(\\beta_0\\)"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model-2",
    "href": "slides/01-slides.html#is-this-a-linear-model-2",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?\n\n\n Where is \\(\\beta_0\\)"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model-3",
    "href": "slides/01-slides.html#is-this-a-linear-model-3",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?\n\n\n Where is \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model-4",
    "href": "slides/01-slides.html#is-this-a-linear-model-4",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?\n\n\n Where is \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/01-slides.html#what-is-the-equation-1",
    "href": "slides/01-slides.html#what-is-the-equation-1",
    "title": "Welcome to Linear Models",
    "section": "What is the equation?",
    "text": "What is the equation?\n\n\\[y = \\beta_0 + \\beta_1 x + \\varepsilon\\]"
  },
  {
    "objectID": "slides/01-slides.html#what-is-the-equation-2",
    "href": "slides/01-slides.html#what-is-the-equation-2",
    "title": "Welcome to Linear Models",
    "section": "What is the equation?",
    "text": "What is the equation?\n\n\\[\nX = \\begin{bmatrix}1 & x_1\\\\\n\\vdots & \\vdots \\\\ 1 & x_n\\end{bmatrix},\\quad\n\\beta = \\begin{bmatrix}\\beta_0\\\\ \\beta_1\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/01-slides.html#what-does-the-data-look-like",
    "href": "slides/01-slides.html#what-does-the-data-look-like",
    "title": "Welcome to Linear Models",
    "section": "What does the “data” look like?",
    "text": "What does the “data” look like?\n\n\n\n  (Intercept)        x\n1           1 1.920595\n2           1 3.093363\n3           1 5.301387\n4           1 8.990286\n5           1 1.218501\n6           1 8.882287"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model-5",
    "href": "slides/01-slides.html#is-this-a-linear-model-5",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?"
  },
  {
    "objectID": "slides/01-slides.html#what-is-the-equation-3",
    "href": "slides/01-slides.html#what-is-the-equation-3",
    "title": "Welcome to Linear Models",
    "section": "What is the equation?",
    "text": "What is the equation?\n\\[y = \\beta_0 + \\sum_{j=1}^{J} \\beta_j B_j(x) + \\varepsilon\\]\n\nwhere \\(B_j(x)\\) are known spline basis functions\nthe model is linear in the \\(\\beta_j\\)"
  },
  {
    "objectID": "slides/01-slides.html#what-does-the-data-look-like-1",
    "href": "slides/01-slides.html#what-does-the-data-look-like-1",
    "title": "Welcome to Linear Models",
    "section": "What does the “data” look like?",
    "text": "What does the “data” look like?\n\n\n\n  (Intercept) ns(x, df = 3)1 ns(x, df = 3)2 ns(x, df = 3)3\n1           1   -0.095842465      0.4743374     -0.3103069\n2           1    0.006273703      0.5498909     -0.3597333\n3           1    0.444366108      0.4236598     -0.2337245\n4           1    0.133806217      0.3714757      0.4916902\n5           1   -0.103895176      0.3825795     -0.2502798\n6           1    0.164715184      0.3660870      0.4649746"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model-6",
    "href": "slides/01-slides.html#is-this-a-linear-model-6",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?"
  },
  {
    "objectID": "slides/01-slides.html#is-this-a-linear-model-7",
    "href": "slides/01-slides.html#is-this-a-linear-model-7",
    "title": "Welcome to Linear Models",
    "section": "Is this a linear model?",
    "text": "Is this a linear model?"
  },
  {
    "objectID": "slides/01-slides.html#what-is-the-equation-4",
    "href": "slides/01-slides.html#what-is-the-equation-4",
    "title": "Welcome to Linear Models",
    "section": "What is the equation?",
    "text": "What is the equation?\n\\[y = \\sum_{k=1}^{K} \\beta_k \\, \\mathbf{1}\\{x \\in R_k\\} + \\varepsilon\\]\n\nwhere \\(R_1,\\ldots,R_K\\) are each of the regions\nthis is linear in the parameters \\(\\beta_k\\)"
  },
  {
    "objectID": "slides/01-slides.html#what-does-the-data-look-like-2",
    "href": "slides/01-slides.html#what-does-the-data-look-like-2",
    "title": "Welcome to Linear Models",
    "section": "What does the “data” look like?",
    "text": "What does the “data” look like?\n\n\n\n  ind1 ind2 ind3 ind4 ind5 ind6 ind7 ind8\n1    0    0    0    0    0    0    0    1\n2    0    0    0    0    0    0    1    0\n3    0    0    0    0    0    0    1    0\n4    0    0    0    0    0    0    0    1\n5    0    1    0    0    0    0    0    0\n6    0    0    1    0    0    0    0    0"
  },
  {
    "objectID": "rstudio-pro.html",
    "href": "rstudio-pro.html",
    "title": "Logging into RStudio Pro",
    "section": "",
    "text": "Follow these steps to access RStudio Pro:\n\nMake sure you are connected to the WFU Network. This means you must be on the eduroam wireless network, or connected through the VPN if you are off-campus.\n\nIf you are off-campus, go through the VPN first: WFU VPN Instructions.\n\nGo to login.deac.wfu.edu and sign in with your school email and password.\nOnce logged in, click the RStudio icon.\nSet your working directory to /deac/sta/classes/sta312/YOUR-USERNAME where YOUR-USERNAME is replaced by the first part of your email address, i.e., mine is mcgowald because my email is mcgowald@wfu.edu\nSet the number of hours to how long you plan to work on the project (I usually enter 4).\nClick Launch.\nThe session will show as Queued for a moment. Once it is ready, the status will turn green and say Ready.\nClick the Connect to RStudio Server button. You should now be in RStudio Pro.\n\n\nNote: You are not required to use this setup (i.e., you can download RStudio and all the necessary packages locally). However, the focus of this class is not on technical setup, so we will not be able to spend time troubleshooting issues if you are not using the official setup described here.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Using the RStudio Pro Server"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 312: Linear Models",
    "section": "",
    "text": "Course Info\nLocation: Carswell 208\nTime: W/F 9:30a-10:45a\n\n\n Office Hours\nInstructor: Dr. Lucy D’Agostino McGowan\nEmail: mcgowald@wfu.edu\nHours: Bookable\nOffice: Manchester 342\nMath & Stats Center: mathandstatscenter@wfu.edu\nLocation: Math & Stats Center\n\n\n\n Texts\nThe main text book is: Linear Models with R by Julian Faraway\n\n\n Materials\nThis class is very hands on; be sure to bring a fully charged laptop to every class."
  },
  {
    "objectID": "help-quarto.html",
    "href": "help-quarto.html",
    "title": "Creating a New Quarto Document in RStudio",
    "section": "",
    "text": "To create a new Quarto document:\n\nBe sure that you are in the correct project for the exercise or assignment. If you need help creating a new project here are some tips.\nIn RStudio, go to the menu bar and select File &gt; New File &gt; Quarto Document.\nEnter a title for your document and select the output format (e.g., HTML, PDF, Word).\nClick Create. A new Quarto file (.qmd) will open.\nSave the file to your project directory\nUse the toolbar or the Render button to build your document.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Creating a Quarto Document"
    ]
  },
  {
    "objectID": "ex/keyterms.html",
    "href": "ex/keyterms.html",
    "title": "Study List for Linear Models Exam",
    "section": "",
    "text": "This list may not be exhaustive but should be a good start, I encourage you to go through our slides, exercises, and notes from the class to best prepare for the upcoming midterm.\n\n\nOLS Estimator:\n\n\\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)\n\nNormal Equations:\n\n\\(\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}\\)\n\nHat Matrix:\n\n\\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\n\\(\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\\)\n\nProperties: \\(\\mathbf{H}^T = \\mathbf{H}\\) (symmetric), \\(\\mathbf{H}^2 = \\mathbf{H}\\) (idempotent)\n\nVariance of OLS:\n\n\\(\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\)\n\n\\(\\text{Var}(\\hat{\\beta}_j) = \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\)\n\nSums of Squares:\n\nTSS = \\(\\mathbf{y}^T\\mathbf{y} - n\\bar{y}^2\\) = \\(\\sum_{i=1}^n(y_i - \\bar{y})^2\\)\n\nSS\\(_{\\text{Reg}}\\) = \\(\\mathbf{y}^T\\mathbf{H}\\mathbf{y} - n\\bar{y}^2\\)\n\nSSE = \\(\\mathbf{y}^T(\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\)\n\nFundamental Identity: TSS = SS\\(_{\\text{Reg}}\\) + SSE\n\nCoefficient of Determination:\n\n\\(R^2 = \\frac{\\text{SS}_{\\text{Reg}}}{\\text{TSS}} = 1 - \\frac{\\text{SSE}}{\\text{TSS}}\\)\n\nVariance Estimation:\n\n\\(\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}\\) (MSE)\n\nTest Statistics:\n\nt-statistic: \\(t = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)}\\) where \\(\\text{se}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}\\)\n\nF-statistic: \\(F = \\frac{\\text{SS}_{\\text{Reg}}/(p-1)}{\\text{SSE}/(n-p)} = \\frac{\\text{MS}_{\\text{Reg}}}{\\text{MSE}}\\)\n\n\n\n\n\n\\(E[\\mathbf{A}\\mathbf{Y} + \\mathbf{b}] = \\mathbf{A}E[\\mathbf{Y}] + \\mathbf{b}\\)\n\n\\(\\text{Var}(\\mathbf{A}\\mathbf{Y} + \\mathbf{b}) = \\mathbf{A}\\text{Var}(\\mathbf{Y})\\mathbf{A}^T\\)\n\n\n\n\n\nLinearity: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\)\n\nZero mean errors: \\(E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\)\n\nConstant variance & independence: \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\)\n\nFull rank: \\(\\mathbf{X}\\) has full column rank\n\n\n\n\n\nError: \\(n - p\\)\n\nRegression: \\(p - 1\\)\n\nTotal: \\(n - 1\\)\n\n\n\n\nOrthogonality Condition:\n\n\\(\\mathbf{X}^T\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0}\\) (residuals perpendicular to column space)\n\nQR Decomposition:\n\n\\(\\mathbf{R}\\boldsymbol{\\hat\\beta} = \\mathbf{Q}^T\\mathbf{y}\\)\n\nHow to backsolve to get \\(\\hat\\beta\\)\n\nBLUE:\n\nBest = minimum variance\n\nLinear = estimator is linear in \\(\\mathbf{y}\\)\n\nUnbiased = \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\nEstimator\n\nFor Unbiased Estimator \\(\\mathbf{C}\\mathbf{y}\\):\n\nMust satisfy: \\(\\mathbf{C}\\mathbf{X} = \\mathbf{I}\\)\n\nPositive Semi-Definite:\n\nMatrix \\(\\mathbf{A}\\) is PSD if \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) for all vectors \\(\\mathbf{x}\\)\n\n\\(\\mathbf{D}\\mathbf{D}^T\\) is always PSD\n\n\n\n\n\n\\((\\mathbf{AB})^T = \\mathbf{B}^T\\mathbf{A}^T\\)\n\n\\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\)\n\n\\((\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T\\)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Key Terms"
    ]
  },
  {
    "objectID": "ex/keyterms.html#key-formulas",
    "href": "ex/keyterms.html#key-formulas",
    "title": "Study List for Linear Models Exam",
    "section": "",
    "text": "OLS Estimator:\n\n\\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)\n\nNormal Equations:\n\n\\(\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}\\)\n\nHat Matrix:\n\n\\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\n\\(\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\\)\n\nProperties: \\(\\mathbf{H}^T = \\mathbf{H}\\) (symmetric), \\(\\mathbf{H}^2 = \\mathbf{H}\\) (idempotent)\n\nVariance of OLS:\n\n\\(\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\)\n\n\\(\\text{Var}(\\hat{\\beta}_j) = \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\)\n\nSums of Squares:\n\nTSS = \\(\\mathbf{y}^T\\mathbf{y} - n\\bar{y}^2\\) = \\(\\sum_{i=1}^n(y_i - \\bar{y})^2\\)\n\nSS\\(_{\\text{Reg}}\\) = \\(\\mathbf{y}^T\\mathbf{H}\\mathbf{y} - n\\bar{y}^2\\)\n\nSSE = \\(\\mathbf{y}^T(\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\)\n\nFundamental Identity: TSS = SS\\(_{\\text{Reg}}\\) + SSE\n\nCoefficient of Determination:\n\n\\(R^2 = \\frac{\\text{SS}_{\\text{Reg}}}{\\text{TSS}} = 1 - \\frac{\\text{SSE}}{\\text{TSS}}\\)\n\nVariance Estimation:\n\n\\(\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}\\) (MSE)\n\nTest Statistics:\n\nt-statistic: \\(t = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)}\\) where \\(\\text{se}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}\\)\n\nF-statistic: \\(F = \\frac{\\text{SS}_{\\text{Reg}}/(p-1)}{\\text{SSE}/(n-p)} = \\frac{\\text{MS}_{\\text{Reg}}}{\\text{MSE}}\\)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Key Terms"
    ]
  },
  {
    "objectID": "ex/keyterms.html#random-vector-properties",
    "href": "ex/keyterms.html#random-vector-properties",
    "title": "Study List for Linear Models Exam",
    "section": "",
    "text": "\\(E[\\mathbf{A}\\mathbf{Y} + \\mathbf{b}] = \\mathbf{A}E[\\mathbf{Y}] + \\mathbf{b}\\)\n\n\\(\\text{Var}(\\mathbf{A}\\mathbf{Y} + \\mathbf{b}) = \\mathbf{A}\\text{Var}(\\mathbf{Y})\\mathbf{A}^T\\)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Key Terms"
    ]
  },
  {
    "objectID": "ex/keyterms.html#gauss-markov-assumptions",
    "href": "ex/keyterms.html#gauss-markov-assumptions",
    "title": "Study List for Linear Models Exam",
    "section": "",
    "text": "Linearity: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\)\n\nZero mean errors: \\(E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\)\n\nConstant variance & independence: \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\)\n\nFull rank: \\(\\mathbf{X}\\) has full column rank",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Key Terms"
    ]
  },
  {
    "objectID": "ex/keyterms.html#degrees-of-freedom",
    "href": "ex/keyterms.html#degrees-of-freedom",
    "title": "Study List for Linear Models Exam",
    "section": "",
    "text": "Error: \\(n - p\\)\n\nRegression: \\(p - 1\\)\n\nTotal: \\(n - 1\\)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Key Terms"
    ]
  },
  {
    "objectID": "ex/keyterms.html#key-conceptual-points",
    "href": "ex/keyterms.html#key-conceptual-points",
    "title": "Study List for Linear Models Exam",
    "section": "",
    "text": "Orthogonality Condition:\n\n\\(\\mathbf{X}^T\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0}\\) (residuals perpendicular to column space)\n\nQR Decomposition:\n\n\\(\\mathbf{R}\\boldsymbol{\\hat\\beta} = \\mathbf{Q}^T\\mathbf{y}\\)\n\nHow to backsolve to get \\(\\hat\\beta\\)\n\nBLUE:\n\nBest = minimum variance\n\nLinear = estimator is linear in \\(\\mathbf{y}\\)\n\nUnbiased = \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\nEstimator\n\nFor Unbiased Estimator \\(\\mathbf{C}\\mathbf{y}\\):\n\nMust satisfy: \\(\\mathbf{C}\\mathbf{X} = \\mathbf{I}\\)\n\nPositive Semi-Definite:\n\nMatrix \\(\\mathbf{A}\\) is PSD if \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) for all vectors \\(\\mathbf{x}\\)\n\n\\(\\mathbf{D}\\mathbf{D}^T\\) is always PSD",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Key Terms"
    ]
  },
  {
    "objectID": "ex/keyterms.html#matrix-algebra-rules-youll-need",
    "href": "ex/keyterms.html#matrix-algebra-rules-youll-need",
    "title": "Study List for Linear Models Exam",
    "section": "",
    "text": "\\((\\mathbf{AB})^T = \\mathbf{B}^T\\mathbf{A}^T\\)\n\n\\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\)\n\n\\((\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T\\)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Key Terms"
    ]
  },
  {
    "objectID": "ex/ex-6.html",
    "href": "ex/ex-6.html",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "",
    "text": "Given the following data:\n\\[\\mathbf{y} = \\begin{bmatrix}10\\\\13\\\\14\\\\16\\\\18\\end{bmatrix}\\]\n\\[\\mathbf{X} = \\begin{bmatrix}1&1\\\\1&2\\\\1&3\\\\1&4\\\\1&5\\end{bmatrix}\\]\na) Calculate TSS by hand using both the definition and the matrix form.\nb) Fit the linear model and calculate the SSE and \\(\\textrm{SS}_\\textrm{Reg}\\). Verify that TSS = SSE + SS_Reg.\nc) Calculate R² and interpret its meaning in context.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 6: Sums of Squares and Hypothesis Testing"
    ]
  },
  {
    "objectID": "ex/ex-6.html#problem-1-basic-sums-of-squares-calculation",
    "href": "ex/ex-6.html#problem-1-basic-sums-of-squares-calculation",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "",
    "text": "Given the following data:\n\\[\\mathbf{y} = \\begin{bmatrix}10\\\\13\\\\14\\\\16\\\\18\\end{bmatrix}\\]\n\\[\\mathbf{X} = \\begin{bmatrix}1&1\\\\1&2\\\\1&3\\\\1&4\\\\1&5\\end{bmatrix}\\]\na) Calculate TSS by hand using both the definition and the matrix form.\nb) Fit the linear model and calculate the SSE and \\(\\textrm{SS}_\\textrm{Reg}\\). Verify that TSS = SSE + SS_Reg.\nc) Calculate R² and interpret its meaning in context.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 6: Sums of Squares and Hypothesis Testing"
    ]
  },
  {
    "objectID": "ex/ex-6.html#problem-2-matrix-algebra-proof",
    "href": "ex/ex-6.html#problem-2-matrix-algebra-proof",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Problem 2: Matrix Algebra Proof",
    "text": "Problem 2: Matrix Algebra Proof\nProve algebraically that the F-statistic can be written in terms of R² as:\n\\(F = \\frac{R^2/(p-1)}{(1-R^2)/(n-p)}\\)\nShow each algebraic step clearly.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 6: Sums of Squares and Hypothesis Testing"
    ]
  },
  {
    "objectID": "ex/ex-6.html#problem-3-simulation---t-distribution-of-coefficients",
    "href": "ex/ex-6.html#problem-3-simulation---t-distribution-of-coefficients",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Problem 3: Simulation - t-Distribution of Coefficients",
    "text": "Problem 3: Simulation - t-Distribution of Coefficients\nIn this problem, you’ll verify through simulation that \\(\\frac{\\hat{\\beta}_j - \\beta_j}{\\text{se}(\\hat{\\beta}_j)} \\sim t_{n-p}\\).\nIntroduction to purrr\n\nBefore we start, we’ll use the purrr package, which provides tools for functional programming in R. The key function we’ll use is map_dbl(), which applies a function to each element of a list or vector and outputs a numeric vector. Think of map_dbl() as a more elegant alternative to writing loops when you expect a numeric output.\nFor example: - map_dbl(1:5, sqrt) applies the square root function to numbers 1 through 5 - map_dbl(1:1000, my_simulation_function) runs your simulation function 1000 times\nThis approach is cleaner than loops and encourages you to think about your simulation as a function that gets repeated many times.\nStep-by-step simulation guide:\nStep 1: Set up simulation parameters and load libraries\n\nlibrary(purrr)\nlibrary(ggplot2)\nset.seed(1)\n\n# Simulation parameters\nn &lt;- 50          # sample size\np &lt;- 3           # number of parameters (intercept + 2 predictors)\nn_sims &lt;- 10000  # number of simulations\nbeta_true &lt;- c(2, 1.5, -0.8)  # true coefficients [intercept, x1, x2]\nsigma_true &lt;- 2  # true error standard deviation\n\n# Create design matrix (stays fixed across simulations)\nX &lt;- cbind(1, rnorm(n, 0, 1), rnorm(n, 2, 1.5))\n\nStep 2: Create a simulation function\nFill in the function below. This function should: 1. Generate random errors 2. Create the response variable using the true model 3. Fit the linear model 4. Extract the coefficient and standard error for x1 (the second coefficient) 5. Calculate and return the t-statistic\n\nsimulate_t_stat &lt;- function(sim_number) {\n  # Generate random errors with a mean = 0 and sd = 1 \n  epsilon &lt;- rnorm(n, 0, 1)\n  \n  # Create response variable: y = X * beta_true + epsilon\n  y &lt;- # YOUR CODE HERE\n  \n  # Calculate coefficient and standard error for x1\n  beta_hat &lt;- # YOUR CODE HERE (coefficient for x1)\n  se_beta &lt;- # YOUR CODE HERE (standard error for x1)\n  \n  # Calculate t-statistic: (beta_hat - true_beta) / se_beta\n  t_stat &lt;- # YOUR CODE HERE\n  \n  return(t_stat)\n}\n\nStep 3: Run simulation using purrr::map_dbl()\n\n# Use map to run simulation n_sims times\nt_stats &lt;- map_dbl(1:n_sims, simulate_t_stat)\n\nStep 4: Analyze results\n\n# Calculate degrees of freedom\ndft &lt;- n - p\n\n# Create data frame for ggplot\nsim_data &lt;- data.frame(t_statistic = t_stats)\n\n# Create comparison plot\nggplot(sim_data, aes(x = t_statistic)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = \"lightblue\") +\n  stat_function(fun = dt, args = list(df = dft), color = \"red\", linewidth = 1.2) +\n  labs(title = \n         paste(\"Simulated t-statistics vs Theoretical t-distribution (df =\", dft, \")\"),\n       subtitle = paste(\"Based on\", n_sims, \"simulations\"),\n       x = \"t-statistic\", \n       y = \"Density\") +\n  theme_minimal()\n\nQuestions to answer:\n\nWhat are the degrees of freedom for this t-distribution?\n\nDo the simulated t-statistics follow the expected distribution?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 6: Sums of Squares and Hypothesis Testing"
    ]
  },
  {
    "objectID": "ex/ex-6.html#problem-4-simulation---f-distribution-of-f-statistic",
    "href": "ex/ex-6.html#problem-4-simulation---f-distribution-of-f-statistic",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Problem 4: Simulation - F-Distribution of F-Statistic",
    "text": "Problem 4: Simulation - F-Distribution of F-Statistic\nNow verify that the overall F-statistic follows an F-distribution under the null hypothesis.\nSimulation setup:\nStep 1: Set up parameters for testing under the null\n\nset.seed(1)\n\n# Simulation parameters\nn &lt;- 30\np &lt;- 4  # intercept + 3 predictors\nn_sims &lt;- 5000\nsigma_true &lt;- 1.5\n\n# Under null hypothesis: all slope coefficients are 0\nbeta_null &lt;- c(5, 0, 0, 0)  # only intercept is non-zero\n\n# Create design matrix (fixed across simulations)\nX &lt;- cbind(1,\n           rnorm(n, 0, 1),\n           rnorm(n, 1, 2),\n           rnorm(n, -1, 1.2))\n\nStep 2: Create your F-statistic simulation function\nFill in the function below. This function should: 1. Generate random errors 2. Create response variable under the null hypothesis 3. Fit the linear model 4. Extract the overall F-statistic 5. Return the F-statistic\n\nsimulate_f_stat &lt;- function(sim_number) {\n  # Generate random errors\n  epsilon &lt;- # YOUR CODE HERE\n  \n  # Create response under null: y = X * beta_null + epsilon\n  y &lt;- # YOUR CODE HERE\n  \n  # Fit the model (be careful about how you specify the predictors)\n  fit &lt;- # YOUR CODE HERE\n  \n  # Extract overall F-statistic from model summary\n  # Hint: summary(fit)$fstatistic[1] gives the F-statistic\n  f_stat &lt;- # YOUR CODE HERE\n  \n  return(f_stat)\n}\n\nStep 3: Run simulation using purrr\n\n# Use map_dbl to get a numeric vector of F-statistics\nf_stats &lt;- # YOUR CODE HERE (use map_dbl and your function)\n\nStep 4: Compare to theoretical F-distribution with improved visualizations\n\n# Calculate degrees of freedom\ndf1 &lt;- p - 1  # numerator degrees of freedom (number of predictors excluding intercept)\ndf2 &lt;- n - p  # denominator degrees of freedom\n\n# Create data frame for ggplot\nsim_data_f &lt;- data.frame(f_statistic = f_stats)\n\n# Create comparison plot\nggplot(sim_data_f, aes(x = f_statistic)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = \"lightgreen\") +\n  stat_function(fun = df, args = list(df1 = df1, df2 = df2), color = \"red\", linewidth = 1.2) +\n  xlim(0, quantile(f_stats, 0.99)) +  # Trim extreme values \n  labs(title = paste(\"Simulated F-statistics vs Theoretical F-distribution\"),\n       subtitle = paste(\"df1 =\", df1, \", df2 =\", df2, \"| Based on\", n_sims, \"simulations\"),\n       x = \"F-statistic\", \n       y = \"Density\") +\n  theme_minimal()\n\nQuestions to answer: - What are the numerator and denominator degrees of freedom?\n- Do the simulated F-statistics match the theoretical distribution?\n- What would happen if the null hypothesis were false (i.e., if some slope coefficients were non-zero)?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 6: Sums of Squares and Hypothesis Testing"
    ]
  },
  {
    "objectID": "ex/ex-6.html#problem-5-general-linear-hypothesis-testing",
    "href": "ex/ex-6.html#problem-5-general-linear-hypothesis-testing",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Problem 5: General Linear Hypothesis Testing",
    "text": "Problem 5: General Linear Hypothesis Testing\nConsider the model: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\varepsilon\\)\na) Set up the contrast matrix \\(\\mathbf{C}\\) and vector \\(\\mathbf{d}\\) to test the hypothesis: \\(H_0: \\beta_1 + \\beta_2 = 0 \\text{ and } \\beta_3 = 2\\beta_4\\)\nb) How many degrees of freedom would the F-test have? Explain your reasoning.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 6: Sums of Squares and Hypothesis Testing"
    ]
  },
  {
    "objectID": "ex/ex-6.html#problem-6-anova-table-construction",
    "href": "ex/ex-6.html#problem-6-anova-table-construction",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Problem 6: ANOVA Table Construction",
    "text": "Problem 6: ANOVA Table Construction\nYou are given the following information from a regression analysis:\n\nn = 25 (sample size)\n\np = 5 (number of parameters including intercept)\n\nTSS = 500\n\nR² = 0.72\n\nComplete the ANOVA table:\n\n\nSource\ndf\nSum of Squares\nMean Square\nF\n\n\n\nRegression\n\n\n\n\n\n\nError\n\n\n\n\n\n\nTotal",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 6: Sums of Squares and Hypothesis Testing"
    ]
  },
  {
    "objectID": "ex/ex-4.html",
    "href": "ex/ex-4.html",
    "title": "It’s Just a Linear Model",
    "section": "",
    "text": "Background Reading: Lindeløv, J. K. “Common statistical tests are linear models”URL: https://lindeloev.github.io/tests-as-linear/",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 4: It's Just a Linear Model!"
    ]
  },
  {
    "objectID": "ex/ex-4.html#instructions",
    "href": "ex/ex-4.html#instructions",
    "title": "It’s Just a Linear Model",
    "section": "Instructions",
    "text": "Instructions\nRead through Lindeløv’s resource to understand how common statistical tests are special cases of linear models. Then complete the matrix-based exercises below using the provided data. I’ll do the first one (one-sample t-test) so you can see what I mean.\nQuestion 1: t-tests\na) One-sample t-test\nData: \\(\\mathbf{y} = \\begin{bmatrix} 2.1 \\\\ 1.8 \\\\ 2.3 \\\\ 1.9 \\\\ 2.0 \\end{bmatrix}\\)\n\nConstruct design matrix \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) such that \\(\\boldsymbol{\\beta}\\) gives the one-sample t-test result\n\nCompare your matrix setup to t.test(y) in R\n\n\nX &lt;- c(1, 1, 1, 1, 1)\ny &lt;- c(2.1, 1.8, 2.3, 1.9, 2.0)\n\n## these should give me the same answer\nsolve(crossprod(X)) %*% crossprod(X, y)\n\n     [,1]\n[1,] 2.02\n\nt.test(y)\n\n\n    One Sample t-test\n\ndata:  y\nt = 23.482, df = 4, p-value = 1.95e-05\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1.781161 2.258839\nsample estimates:\nmean of x \n     2.02 \n\n\nb) Independent two-sample t-test\n\nGroup 1: \\(\\begin{bmatrix} 1.2 \\\\ 1.5 \\\\ 1.1 \\\\ 1.4 \\end{bmatrix}\\)\n\nGroup 2: \\(\\begin{bmatrix} 2.1 \\\\ 2.3 \\\\ 2.0 \\end{bmatrix}\\)\n\n\nCombined: \\(\\mathbf{y} = \\begin{bmatrix} 1.2 \\\\ 1.5 \\\\ 1.1 \\\\ 1.4 \\\\ 2.1 \\\\ 2.3 \\\\ 2.0 \\end{bmatrix}\\)\n\nConstruct design matrix \\(\\mathbf{X}\\) (using dummy coding) and \\(\\mathbf{y}\\) such that \\(\\boldsymbol{\\beta}\\) gives the two-sample t-test result\n\nCompare to t.test(group1, group2, var.equal = TRUE) in R\n\nc) Paired t-test\n\nBefore: \\(\\begin{bmatrix} 3.2 \\\\ 2.8 \\\\ 3.1 \\\\ 2.9 \\end{bmatrix}\\), After: \\(\\begin{bmatrix} 3.0 \\\\ 2.5 \\\\ 2.8 \\\\ 2.6 \\end{bmatrix}\\)\n\n\nDifferences: \\(\\mathbf{y} = \\begin{bmatrix} -0.2 \\\\ -0.3 \\\\ -0.3 \\\\ -0.3 \\end{bmatrix}\\)\n\nConstruct design matrix \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) such that \\(\\boldsymbol{\\beta}\\) gives the paired t-test result\n\nCompare to t.test(before, after, paired = TRUE) in R\nQuestion 2: ANOVA\na) One-way ANOVA\n\nGroup A: \\(\\begin{bmatrix} 1.1 \\\\ 1.3 \\\\ 1.2 \\end{bmatrix}\\)\n\nGroup B: \\(\\begin{bmatrix} 2.0 \\\\ 2.2 \\\\ 1.9 \\\\ 2.1 \\end{bmatrix}\\)\n\nGroup C: \\(\\begin{bmatrix} 2.8 \\\\ 2.9 \\end{bmatrix}\\)\n\n\nCombined: \\(\\mathbf{y} = \\begin{bmatrix} 1.1 \\\\ 1.3 \\\\ 1.2 \\\\ 2.0 \\\\ 2.2 \\\\ 1.9 \\\\ 2.1 \\\\ 2.8 \\\\ 2.9 \\end{bmatrix}\\)\n\nConstruct design matrix \\(\\mathbf{X}\\) (using dummy coding) and \\(\\mathbf{y}\\) such that \\(\\boldsymbol{\\beta}\\) gives the one-way ANOVA result\nCompare to aov(y ~ group) in R\n\nb) Two-way ANOVA (2×2 design)\nFactors: A (Low/High), B (Control/Treatment)\n\nLow-Control: \\(\\begin{bmatrix} 1.0 \\\\ 1.2 \\end{bmatrix}\\)\n\nLow-Treatment: \\(\\begin{bmatrix} 1.5 \\\\ 1.7 \\end{bmatrix}\\)\n\nHigh-Control: \\(\\begin{bmatrix} 2.0 \\\\ 2.1 \\end{bmatrix}\\)\n\nHigh-Treatment: \\(\\begin{bmatrix} 3.0 \\\\ 3.2 \\end{bmatrix}\\)\n\n\nCombined: \\(\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ 1.2 \\\\ 1.5 \\\\ 1.7 \\\\ 2.0 \\\\ 2.1 \\\\ 3.0 \\\\ 3.2 \\end{bmatrix}\\)\n\nConstruct \\(\\mathbf{X}\\) with main effects and interaction and \\(\\mathbf{y}\\) such that \\(\\boldsymbol{\\beta}\\) gives the two-way ANOVA result\nCompare to aov(y ~ A * B) in R\nQuestion 3: Correlation/Regression\na) Simple linear regression\n\\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}\\), \\(\\mathbf{y} = \\begin{bmatrix} 2.1 \\\\ 3.9 \\\\ 6.1 \\\\ 7.8 \\end{bmatrix}\\)\n\nConstruct design matrix \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to calculate Pearson Correlation\n\nCompare to cor.test(x, y) in R\n\nb) Spearman correlation - Using same data as (a), construct \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) for rank-based analysis - Compare to cor.test(x, y, method = \"spearman\") in R",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 4: It's Just a Linear Model!"
    ]
  },
  {
    "objectID": "ex/ex-2.html",
    "href": "ex/ex-2.html",
    "title": "Matrix Problem Set",
    "section": "",
    "text": "Complete all problems showing your work. For matrix calculations, show each step clearly. You may use R to verify your answers, but show the mathematical steps first.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#instructions",
    "href": "ex/ex-2.html#instructions",
    "title": "Matrix Problem Set",
    "section": "",
    "text": "Complete all problems showing your work. For matrix calculations, show each step clearly. You may use R to verify your answers, but show the mathematical steps first.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#problem-1",
    "href": "ex/ex-2.html#problem-1",
    "title": "Matrix Problem Set",
    "section": "Problem 1",
    "text": "Problem 1\nGiven the following matrices:\n\\[\\mathbf{A} = \\begin{bmatrix} 1 & 3 & -2 \\\\ 4 & 0 & 5 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 & 1 \\\\ -1 & 3 \\\\ 0 & 4 \\end{bmatrix}, \\quad \\mathbf{C} = \\begin{bmatrix} 3 & -1 \\\\ 2 & 6 \\end{bmatrix}\\]\na) What are the dimensions of each matrix?\nb) Which of the following operations are possible? If possible, state the dimensions of the result:\n\n\\(\\mathbf{A} + \\mathbf{B}\\)\n\\(\\mathbf{A} \\times \\mathbf{B}\\)\n\\(\\mathbf{B} \\times \\mathbf{C}\\)\n\\(\\mathbf{C} \\times \\mathbf{B}\\)\n\nc) Calculate \\(\\mathbf{A} \\times \\mathbf{B}\\) (show all steps).",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#problem-2",
    "href": "ex/ex-2.html#problem-2",
    "title": "Matrix Problem Set",
    "section": "Problem 2",
    "text": "Problem 2\nGiven \\(\\mathbf{P} = \\begin{bmatrix} 2 & -1 & 4 \\\\ 3 & 0 & 1 \\end{bmatrix}\\) and \\(\\mathbf{Q} = \\begin{bmatrix} 1 & 2 \\\\ -1 & 3 \\\\ 0 & 1 \\end{bmatrix}\\)\na) Find \\(\\mathbf{P}^T\\) and \\(\\mathbf{Q}^T\\).\nb) Calculate \\(\\mathbf{P} \\times \\mathbf{Q}\\).\nc) Calculate \\((\\mathbf{P} \\times \\mathbf{Q})^T\\).\nd) Calculate \\(\\mathbf{Q}^T \\times \\mathbf{P}^T\\) and verify that \\((\\mathbf{P} \\times \\mathbf{Q})^T = \\mathbf{Q}^T \\times \\mathbf{P}^T\\).",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#problem-3",
    "href": "ex/ex-2.html#problem-3",
    "title": "Matrix Problem Set",
    "section": "Problem 3",
    "text": "Problem 3\na) Find the inverse of \\(\\mathbf{R} = \\begin{bmatrix} 3 & 1 \\\\ 2 & 1 \\end{bmatrix}\\).\nb) Verify your answer by showing that \\(\\mathbf{R} \\times \\mathbf{R}^{-1} = \\mathbf{I}\\).\nc) Explain why the matrix \\(\\mathbf{S} = \\begin{bmatrix} 2 & 4 \\\\ 1 & 2 \\end{bmatrix}\\) does not have an inverse.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#problem-4",
    "href": "ex/ex-2.html#problem-4",
    "title": "Matrix Problem Set",
    "section": "Problem 4",
    "text": "Problem 4\na) Find \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{a}^T\\mathbf{x})\\) where \\(\\mathbf{a} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 1 \\\\ 4 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix}\\).\nb) If \\(f(\\mathbf{x}) = 2x_1 - 5x_2 + 3x_3\\), write this in the form \\(\\mathbf{c}^T\\mathbf{x}\\) and find \\(\\frac{\\partial f}{\\partial \\mathbf{x}}\\).",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#problem-5",
    "href": "ex/ex-2.html#problem-5",
    "title": "Matrix Problem Set",
    "section": "Problem 5",
    "text": "Problem 5\nConsider the quadratic form \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) where \\(\\mathbf{A} = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\).\na) Expand \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) into scalar form.\nb) Find \\(\\mathbf{A}^T\\) and calculate \\(\\mathbf{A} + \\mathbf{A}^T\\).\nc) Use the derivative rule to find \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x})\\).",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#problem-6",
    "href": "ex/ex-2.html#problem-6",
    "title": "Matrix Problem Set",
    "section": "Problem 6",
    "text": "Problem 6\nLet \\(\\mathbf{B} = \\begin{bmatrix} 4 & -1 & 2 \\\\ -1 & 3 & 0 \\\\ 2 & 0 & 5 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\).\na) Verify that \\(\\mathbf{B}\\) is symmetric.\nb) Find \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^T\\mathbf{B}\\mathbf{x})\\) using the symmetric matrix rule.\nc) What would be the result if we used the general rule \\((\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) instead? Show that both methods give the same answer.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#problem-7",
    "href": "ex/ex-2.html#problem-7",
    "title": "Matrix Problem Set",
    "section": "Problem 7",
    "text": "Problem 7\nConsider the expression \\(g(\\mathbf{x}) = \\mathbf{b}^T\\mathbf{A}\\mathbf{x}\\) where: \\[\\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{A} = \\begin{bmatrix} 1 & 0 & 2 \\\\ 3 & -1 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\]\na) What is the dimension of \\(g(\\mathbf{x})\\)? (Is it a scalar, vector, or matrix?)\nb) Use the rule \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{b}^T\\mathbf{A}\\mathbf{x}) = \\mathbf{A}^T\\mathbf{b}\\) to find \\(\\frac{\\partial g}{\\partial \\mathbf{x}}\\).\nc) Calculate \\(\\mathbf{A}\\mathbf{x}\\) first, then compute \\(\\mathbf{b}^T(\\mathbf{A}\\mathbf{x})\\) to expand \\(g(\\mathbf{x})\\) into scalar form.\nd) Verify your answer from part (b) by taking partial derivatives of the scalar form from part (c).",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "ex/ex-2.html#bonus-challenge",
    "href": "ex/ex-2.html#bonus-challenge",
    "title": "Matrix Problem Set",
    "section": "Bonus Challenge",
    "text": "Bonus Challenge\nHat Matrix Connection: In linear regression, we minimize the sum of squared errors: \\[SSE = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]\nExpand this expression and identify which derivative rules from this problem set you would need to find \\(\\frac{\\partial SSE}{\\partial \\boldsymbol{\\beta}}\\). You don’t need to solve it completely, just identify the relevant derivative rules and explain how they would be applied.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 2: Matrices"
    ]
  },
  {
    "objectID": "demo/least-squares.html",
    "href": "demo/least-squares.html",
    "title": "Interactive Least Squares Visualization",
    "section": "",
    "text": "Adjust the red line using the sliders below to try to minimize the sum of squared errors. The blue squares show the squared residuals!\n\n\nIntercept \\(\\hat\\beta_0\\)\n\n1.5\n\n\nSlope \\(\\hat\\beta_1\\)\n\n1\n\n\nShow Optimal\n\n\n\n\n\n\nYour Line\ny = 1.50 + 1.00x\n\n\nOptimal Least Squares\ny = 1.50 + 1.00x\n\n\n\n\nYour SSE\n0.00\n\n\nOptimal SSE\n0.00",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Least Squares Solution"
    ]
  },
  {
    "objectID": "demo/least-squares.html#interactive-least-squares-learning-tool",
    "href": "demo/least-squares.html#interactive-least-squares-learning-tool",
    "title": "Interactive Least Squares Visualization",
    "section": "",
    "text": "Adjust the red line using the sliders below to try to minimize the sum of squared errors. The blue squares show the squared residuals!\n\n\nIntercept \\(\\hat\\beta_0\\)\n\n1.5\n\n\nSlope \\(\\hat\\beta_1\\)\n\n1\n\n\nShow Optimal\n\n\n\n\n\n\nYour Line\ny = 1.50 + 1.00x\n\n\nOptimal Least Squares\ny = 1.50 + 1.00x\n\n\n\n\nYour SSE\n0.00\n\n\nOptimal SSE\n0.00",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Least Squares Solution"
    ]
  },
  {
    "objectID": "demo/least-squares.html#key-concepts",
    "href": "demo/least-squares.html#key-concepts",
    "title": "Interactive Least Squares Visualization",
    "section": "Key Concepts",
    "text": "Key Concepts\nGoal: Find the line \\(\\hat{y} = \\hat\\beta_0 + \\hat\\beta_1x\\) that minimizes the Sum of Squared Errors (SSE)\nThe blue squares show the squared residuals - their total area equals the SSE!\nWhat to Try:\n\nAdjust the sliders to minimize the total area of blue squares\n\nTry to match the optimal (green) line\n\nNotice how the squares get smaller as you approach the optimal solution",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Least Squares Solution"
    ]
  },
  {
    "objectID": "demo/general-f-statistic.html",
    "href": "demo/general-f-statistic.html",
    "title": "Building Intuition for the F-Statistic Numerator",
    "section": "",
    "text": "The general form of the F-statistic with constraint matrix \\(C\\) is:\n\\[F = \\frac{(C\\hat{\\beta} - d)^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta} - d) / q}{\\text{MSE}}\\]\nFor all our purposes, \\(d = 0\\) (we’re testing if parameters equal zero), so we can drop it!\n\\[F = \\frac{(C\\hat{\\beta})^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta}) / q}{\\text{MSE}}\\]\n\nDenominator (MSE): This is always the same - our estimate of \\(\\sigma^2\\)\nThe division by \\(q\\): This just adjusts for the number of restrictions we’re testing\nWhat we need intuition for: \\((C\\hat{\\beta})^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta})\\)\n\nLet’s focus JUST on this “numerator of the numerator” and build some intuition!",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "General F Statistic Intuition"
    ]
  },
  {
    "objectID": "demo/general-f-statistic.html#the-general-f-test-formula",
    "href": "demo/general-f-statistic.html#the-general-f-test-formula",
    "title": "Building Intuition for the F-Statistic Numerator",
    "section": "",
    "text": "The general form of the F-statistic with constraint matrix \\(C\\) is:\n\\[F = \\frac{(C\\hat{\\beta} - d)^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta} - d) / q}{\\text{MSE}}\\]\nFor all our purposes, \\(d = 0\\) (we’re testing if parameters equal zero), so we can drop it!\n\\[F = \\frac{(C\\hat{\\beta})^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta}) / q}{\\text{MSE}}\\]\n\nDenominator (MSE): This is always the same - our estimate of \\(\\sigma^2\\)\nThe division by \\(q\\): This just adjusts for the number of restrictions we’re testing\nWhat we need intuition for: \\((C\\hat{\\beta})^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta})\\)\n\nLet’s focus JUST on this “numerator of the numerator” and build some intuition!",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "General F Statistic Intuition"
    ]
  },
  {
    "objectID": "demo/general-f-statistic.html#understanding-three-pieces",
    "href": "demo/general-f-statistic.html#understanding-three-pieces",
    "title": "Building Intuition for the F-Statistic Numerator",
    "section": "Understanding Three Pieces",
    "text": "Understanding Three Pieces\n\n\\(C\\hat{\\beta}\\) - How far are we from 0?\nThis is a vector. For example, if we’re testing \\(\\beta_1 = \\beta_2 = 0\\), then:\n\\[C\\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{bmatrix}\\]\nThis vector tells us: “How far from zero are the parameters we’re testing?”\n\n\n\\(C(X^TX)^{-1}C^T\\) - The Variance-Covariance Matrix\nThis is a \\(q \\times q\\) matrix that captures the joint uncertainty. The \\(C\\) matrices “extract” the relevant parts of the full variance-covariance matrix:\n\\(\\text{Var}(C\\hat{\\beta}) = \\sigma^2 \\cdot C(X^TX)^{-1}C^T\\)\nFor our example where \\(C\\) selects \\(\\beta_1\\) and \\(\\beta_2\\), this multiplication gives us:\n\\[= \\sigma^2 \\begin{bmatrix} \\text{Var}(\\hat{\\beta}_1) & \\text{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_2) \\\\ \\text{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_2) & \\text{Var}(\\hat{\\beta}_2) \\end{bmatrix}\\]\n\nDiagonal = how uncertain we are about each parameter\nOff-diagonal = whether parameters move together\n\nThe \\(C\\) matrices do the work of pulling out just the rows and columns we care about from the full \\((X^TX)^{-1}\\) matrix.\n\n\nThe Quadratic Form - A Standardized Distance\n\\[(C\\hat{\\beta})^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta})\\]\nThis combines everything into a single number that answers:\n\n“How far from zero are we, accounting for uncertainty and correlation?”\n\nIt’s the squared distance, which is like a correlation-adjusted measure of how extreme our estimates are. It is actually known as a Malahanobis distance.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "General F Statistic Intuition"
    ]
  },
  {
    "objectID": "demo/general-f-statistic.html#why-not-just-use-hatbeta",
    "href": "demo/general-f-statistic.html#why-not-just-use-hatbeta",
    "title": "Building Intuition for the F-Statistic Numerator",
    "section": "Why Not Just Use \\(||\\hat{\\beta}||\\)?",
    "text": "Why Not Just Use \\(||\\hat{\\beta}||\\)?\nIf we just computed \\(\\sqrt{\\hat{\\beta}_1^2 + \\hat{\\beta}_2^2}\\) (without the middle, \\([C(X^TX)^{-1}C^T]^{-1}\\)), we’d be treating all coefficients equally. But:\n\n\\(\\hat{\\beta}_1\\) might be estimated very precisely (small variance)\n\\(\\hat{\\beta}_2\\) might be estimated poorly (large variance)\nThey might be correlated!\n\nThe quadratic form weights by precision giving more importance to coefficients we estimated well.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "General F Statistic Intuition"
    ]
  },
  {
    "objectID": "demo/general-f-statistic.html#the-bottom-line",
    "href": "demo/general-f-statistic.html#the-bottom-line",
    "title": "Building Intuition for the F-Statistic Numerator",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nThe numerator \\((C\\hat{\\beta})^T [C(X^TX)^{-1}C^T]^{-1} (C\\hat{\\beta})\\) is just:\n“How far are our estimates from zero, measured in a way that accounts for their precision and correlation?”\nFor the overall F-test (testing all slopes = 0), this exactly equals \\(SS_{reg}\\), the explained sum of squares.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "General F Statistic Intuition"
    ]
  },
  {
    "objectID": "demo/hat-matrix-symmetry.html",
    "href": "demo/hat-matrix-symmetry.html",
    "title": "Hat Matrix Symmetry Visualization",
    "section": "",
    "text": "Point 1 X:  1.0\n\n\nPoint 2 X:  2.0\n\n\nPoint 3 X:  -1.0\n\n\nRandomize Points",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Hat Matrix Symmetry"
    ]
  },
  {
    "objectID": "demo/hat-matrix-symmetry.html#interactive-linear-regression-example",
    "href": "demo/hat-matrix-symmetry.html#interactive-linear-regression-example",
    "title": "Hat Matrix Symmetry Visualization",
    "section": "",
    "text": "Point 1 X:  1.0\n\n\nPoint 2 X:  2.0\n\n\nPoint 3 X:  -1.0\n\n\nRandomize Points",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Hat Matrix Symmetry"
    ]
  },
  {
    "objectID": "demo/hat-matrix-symmetry.html#hat-matrix-h",
    "href": "demo/hat-matrix-symmetry.html#hat-matrix-h",
    "title": "Hat Matrix Symmetry Visualization",
    "section": "Hat Matrix H",
    "text": "Hat Matrix H",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Hat Matrix Symmetry"
    ]
  },
  {
    "objectID": "demo/hat-matrix-symmetry.html#what-symmetry-means",
    "href": "demo/hat-matrix-symmetry.html#what-symmetry-means",
    "title": "Hat Matrix Symmetry Visualization",
    "section": "What Symmetry Means:",
    "text": "What Symmetry Means:\nHij = Hji means:\n\nHow much point i influences point j’s fitted value = How much point j influences point i’s fitted value\n\nThis reciprocal relationship holds for ALL pairs of points\nReciprocal influence is a geometric consequence of orthogonal projection",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Hat Matrix Symmetry"
    ]
  },
  {
    "objectID": "demo/hat-matrix-symmetry.html#key-observations",
    "href": "demo/hat-matrix-symmetry.html#key-observations",
    "title": "Hat Matrix Symmetry Visualization",
    "section": "Key Observations",
    "text": "Key Observations",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Demos",
      "Hat Matrix Symmetry"
    ]
  },
  {
    "objectID": "ex/ex-1.html",
    "href": "ex/ex-1.html",
    "title": "Exercise 1",
    "section": "",
    "text": "Exercise 1: Login to RStudio Pro\n\nGo to course website → Computing → Using the RStudio Server\nFollow login instructions\nLog in with your credentials\n\nCheck: You should see the RStudio interface with 4 panes.\nExercise 2: Create RStudio Project\n\nFile → New Project → New Directory → New Project\nName: ex-1\n\nClick Create Project\n\nCheck: Project name appears in top-right corner.\nExercise 3: Create Quarto Document\n\nFile → New File → Quarto Document\nTitle: “My First Analysis”\nSave as analysis.qmd\n\n\nCheck: New .qmd file is open and saved.\nExercise 4: Install faraway Package and Load Data\nRun this code in the console ONCE:\n\ninstall.packages(\"faraway\")\n\nThen run this:\n\nlibrary(faraway)\nlibrary(ggplot2)\n\n# Test it works\ndata(teengamb)\nhead(teengamb)\n\n  sex status income verbal gamble\n1   1     51   2.00      8    0.0\n2   1     28   2.50      8    0.0\n3   1     37   2.00      6    0.0\n4   1     28   7.00      4    7.3\n5   1     65   2.00      8   19.6\n6   1     61   3.47      6    0.1\n\n\nCheck: You see the teengamb dataset.\nNow run this in the console:\n\n?teengamb\n\nQuestion: What is in this dataset?\nAction: Add the code below to your .qmd file along with a short description of the data.\n\nlibrary(faraway)\nlibrary(ggplot2)\ndata(teengamb)\n\nExercise 5: Linear Model and Plot\nAdd the following code to your .qmd file. Try running it.\n\n# Fit linear model\nmodel1 &lt;- lm(gamble ~ income, data = teengamb)\nsummary(model1)\n\n\nCall:\nlm(formula = gamble ~ income, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.020 -11.874  -3.757  11.934 107.120 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.325      6.030  -1.049      0.3    \nincome         5.520      1.036   5.330 3.05e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.95 on 45 degrees of freedom\nMultiple R-squared:  0.387, Adjusted R-squared:  0.3734 \nF-statistic: 28.41 on 1 and 45 DF,  p-value: 3.045e-06\n\n# Create plot\nggplot(teengamb, aes(x = income, y = gamble)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  labs(title = \"Income vs Gambling\",\n       x = \"Income\", \n       y = \"Gambling\")\n\n\n\n\n\n\n\nCheck: You have model output and a scatter plot with trend line.\nFinish\nClick “Render” to create your HTML report. Upload your .html file in Canvas under Exercise 1",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "ex/ex-3.html",
    "href": "ex/ex-3.html",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "",
    "text": "This problem set covers the fundamental concepts of linear regression, including matrix formulations, geometric interpretations, and computational methods. Show all work and provide clear explanations for your reasoning.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-3.html#instructions",
    "href": "ex/ex-3.html#instructions",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "",
    "text": "This problem set covers the fundamental concepts of linear regression, including matrix formulations, geometric interpretations, and computational methods. Show all work and provide clear explanations for your reasoning.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-3.html#problem-1",
    "href": "ex/ex-3.html#problem-1",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "Problem 1",
    "text": "Problem 1\nConsider the following dataset with two predictors:\n\n\nObservation\nx1\nx2\ny\n\n\n\n1\n2\n1\n5.2\n\n\n2\n3\n4\n8.1\n\n\n3\n1\n2\n4.8\n\n\n4\n4\n3\n9.5\n\n\n\na) Write out the design matrix X for the multiple regression model with intercept term.\nb) Calculate \\(\\mathbf{X}^T\\mathbf{X}\\) and \\(\\mathbf{X}^T\\mathbf{y}\\) by hand. Show your matrix multiplication steps.\nc) Using your results from Part B, set up the normal equations \\(\\mathbf{X}^T\\mathbf{X}\\hat\\beta=\\mathbf{X}^T\\mathbf{y}\\) and solve for the coefficient vector \\(\\hat\\beta\\). (You can use R if you need to take an inverse).\nd) Verify your answer using R’s built-in functions.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-3.html#problem-2",
    "href": "ex/ex-3.html#problem-2",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "Problem 2",
    "text": "Problem 2\nUsing the data from Problem 1 and your estimated coefficients:\na) Calculate the predicted values \\(\\hat{y}_i\\) for each observation by hand.\nb) Calculate the residuals \\(\\hat\\varepsilon=y_i-\\hat{y}_i\\) for each observation.\nc) Compute the sum of squared errors (SSE) using both formulations:\n\nIndividual terms: SSE \\(= \\sum(y_i - \\hat{y}_i)^2\\)\n\nMatrix form: SSE \\(=  (\\mathbf{y}-\\mathbf{X}\\hat\\beta)^T(\\mathbf{y}-\\mathbf{X}\\hat\\beta)\\)\n\n\nd) Verify that your residuals sum to approximately zero and explain why this should be true geometrically.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-3.html#problem-3",
    "href": "ex/ex-3.html#problem-3",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "Problem 3",
    "text": "Problem 3\na) For simple linear regression with n = 3 observations and design matrix:\n\\[\\mathbf{X} = \\begin{bmatrix}1 & 2\\\\1 & 4\\\\1&6\\end{bmatrix}\\]\nCalculate the hat matrix (\\(\\mathbf{H}\\)) by hand (you can use R if you need to take an inverse).\nb) Verify that \\(\\mathbf{H}\\) is idempotent.\nc) Show that \\(\\mathbf{H}\\) is symmetric and interpret what this property means geometrically.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-3.html#problem-4",
    "href": "ex/ex-3.html#problem-4",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "Problem 4",
    "text": "Problem 4\nConsider the simple linear regression case with the design matrix from Problem 3.\na) Write out two specific vectors that lie in the column space of \\(\\mathbf{X}\\). Explain what these vectors represent in terms of the regression model.\nb) If the observed response vector is \\(\\mathbf{y} = \\begin{bmatrix}3\\\\7\\\\12\\end{bmatrix}\\), explain why this vector likely does NOT lie exactly in the column space of \\(\\mathbf{X}\\). What does this mean practically?\nc) Calculate the projection of \\(\\mathbf{y}\\) onto the column space (i.e., \\(\\mathbf{\\hat{y}} = \\mathbf{Hy}\\)) and show that the residual vector is orthogonal to the column space by verifying \\(\\mathbf{X}^T\\hat\\varepsilon=0\\).",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-3.html#problem-5",
    "href": "ex/ex-3.html#problem-5",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "Problem 5",
    "text": "Problem 5\na) Starting from the geometric principle that residuals must be orthogonal to the column space of \\(\\mathbf{X}\\), derive the normal equations step by step. Begin with the condition \\(\\mathbf{X}^T\\hat\\varepsilon = 0\\) and show all algebraic steps to arrive at \\(\\mathbf{X}^T\\mathbf{X}\\hat\\beta=\\mathbf{X}^T\\mathbf{y}\\).\nb) Explain why we multiply both sides by \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) to solve for \\(\\hat\\beta\\), and under what conditions this inverse might not exist.\nc) Show that the least squares estimator \\(\\hat\\beta = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) minimizes the sum of squared errors by demonstrating that this solution satisfies the orthogonality condition.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-3.html#problem-6",
    "href": "ex/ex-3.html#problem-6",
    "title": "Linear Regression Fundamentals Problem Set",
    "section": "Problem 6",
    "text": "Problem 6\nYou are given the following dataset with 10 observations:\n\n\nx\ny\n\n\n\n1\n4.8\n\n\n2\n6.2\n\n\n3\n8.1\n\n\n4\n9.9\n\n\n5\n11.5\n\n\n6\n13.8\n\n\n7\n15.2\n\n\n8\n17.1\n\n\n9\n18.9\n\n\n10\n21.3\n\n\n\na) Implement the least squares solution in R using matrix operations. Calculate \\(\\hat\\beta\\) without using lm().\nb) Compare your results with R’s lm() function and verify they match.\nc) Using the grid below, hand-draw the following elements:\n\nPlot the 10 data points from the table above\n\nDraw the true regression line: y = 3.5 + 1.8x (shown as a dashed line)\nDraw your fitted regression line from Part A (shown as a solid line)\nLabel both lines clearly in your plot\n\n\n\n\n\n\n\n\n\n(You don’t have to upload this drawing on Canvas, but make sure you do it and understand it, we will do it during the board work portion in class.)\nd) Based on your plot, comment on how well the fitted line approximates the true relationship. What does this tell you about the effectiveness of least squares estimation?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 3: Linear Regression Fundamentals"
    ]
  },
  {
    "objectID": "ex/ex-5.html",
    "href": "ex/ex-5.html",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "",
    "text": "This problem set covers random vector properties, variance-covariance matrices, and the Gauss-Markov theorem. Show all work and provide clear explanations for your reasoning. For computational problems, you may verify your answers using R, but show the mathematical work first.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#instructions",
    "href": "ex/ex-5.html#instructions",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "",
    "text": "This problem set covers random vector properties, variance-covariance matrices, and the Gauss-Markov theorem. Show all work and provide clear explanations for your reasoning. For computational problems, you may verify your answers using R, but show the mathematical work first.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#problem-1-random-vector-expected-values",
    "href": "ex/ex-5.html#problem-1-random-vector-expected-values",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "Problem 1: Random Vector Expected Values",
    "text": "Problem 1: Random Vector Expected Values\nConsider the random vector \\(\\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\end{bmatrix}\\) where \\(E[\\mathbf{Y}] = \\begin{bmatrix} 3 \\\\ -1 \\\\ 2 \\end{bmatrix}\\).\na) Let \\(\\mathbf{A} = \\begin{bmatrix} 2 & 0 & 1 \\\\ 1 & -1 & 3 \\end{bmatrix}\\). Calculate \\(E[\\mathbf{A}\\mathbf{Y}]\\) using the linearity property.\nb) What would \\(E[\\mathbf{A}\\mathbf{Y} + \\mathbf{c}]\\) be if \\(\\mathbf{c} = \\begin{bmatrix} 5 \\\\ -2 \\end{bmatrix}\\)?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#problem-2-variance-covariance-matrix-calculations",
    "href": "ex/ex-5.html#problem-2-variance-covariance-matrix-calculations",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "Problem 2: Variance-Covariance Matrix Calculations",
    "text": "Problem 2: Variance-Covariance Matrix Calculations\nGiven the random vector \\(\\mathbf{Z} = \\begin{bmatrix} Z_1 \\\\ Z_2 \\end{bmatrix}\\) with variance-covariance matrix:\n\\[\\text{Var}(\\mathbf{Z}) = \\begin{bmatrix} 9 & 2 \\\\ 2 & 4 \\end{bmatrix}\\]\na) What are \\(\\text{Var}(Z_1)\\), \\(\\text{Var}(Z_2)\\), and \\(\\text{Cov}(Z_1, Z_2)\\)?\nb) Calculate \\(\\text{Var}(3Z_1 - 2Z_2)\\) using the matrix formula \\(\\text{Var}(\\mathbf{A}\\mathbf{Z}) = \\mathbf{A}\\text{Var}(\\mathbf{Z})\\mathbf{A}^T\\).\nc) Find \\(\\text{Var}\\begin{bmatrix} Z_1 + Z_2 \\\\ 2Z_1 - Z_2 \\end{bmatrix}\\). Show your matrix multiplication steps.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#problem-3-gauss-markov-assumptions",
    "href": "ex/ex-5.html#problem-3-gauss-markov-assumptions",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "Problem 3: Gauss-Markov Assumptions",
    "text": "Problem 3: Gauss-Markov Assumptions\nConsider the linear regression model \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\) where:\n\n\\(\\mathbf{y}\\) is \\(n \\times 1\\)\n\n\\(\\mathbf{X}\\) is \\(n \\times p\\) with full column rank\n\n\\(\\boldsymbol{\\beta}\\) is \\(p \\times 1\\)\n\n\\(\\boldsymbol{\\varepsilon}\\) is \\(n \\times 1\\)\n\na) State the Gauss-Markov assumptions clearly.\nb) Explain what \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\) means in plain English. What two conditions does this impose on the error terms?\nc) If \\(\\text{Var}(\\varepsilon_1) = 4\\), \\(\\text{Var}(\\varepsilon_2) = 9\\), and \\(\\text{Cov}(\\varepsilon_1, \\varepsilon_2) = 1\\), do the errors satisfy the Gauss-Markov assumptions? Explain why or why not.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#problem-4-proving-ols-unbiasedness",
    "href": "ex/ex-5.html#problem-4-proving-ols-unbiasedness",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "Problem 4: Proving OLS Unbiasedness",
    "text": "Problem 4: Proving OLS Unbiasedness\nFor the OLS estimator \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\):\na) Starting from \\(E[\\hat{\\boldsymbol{\\beta}}] = E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}]\\), show step-by-step that OLS is unbiased. Clearly indicate where you use each Gauss-Markov assumption.\nb) In the proof, we treat \\(\\boldsymbol{\\beta}\\) as a constant rather than a random variable. Explain why this is appropriate in the classical regression framework.\nc) What would happen to your proof if \\(E[\\boldsymbol{\\varepsilon}] = \\mathbf{c}\\) for some non-zero constant vector \\(\\mathbf{c}\\) instead of \\(E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\)?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#problem-5-ols-variance-derivation",
    "href": "ex/ex-5.html#problem-5-ols-variance-derivation",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "Problem 5: OLS Variance Derivation",
    "text": "Problem 5: OLS Variance Derivation\na) Derive \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}})\\) starting from the result that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\varepsilon}\\). Show each step clearly.\nb) For the simple linear regression case with \\(\\mathbf{X} = \\begin{bmatrix} 1 & 1  \\\\ 2 & 4  \\end{bmatrix}\\), calculate \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) by hand.\nc) Using your result from part (b), what is \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}})\\) in terms of \\(\\sigma^2\\)? What is the variance of \\(\\hat\\beta_0\\)? What is the variance of \\(\\hat\\beta_1\\)?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#problem-6-understanding-the-gauss-markov-proof",
    "href": "ex/ex-5.html#problem-6-understanding-the-gauss-markov-proof",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "Problem 6: Understanding the Gauss-Markov Proof",
    "text": "Problem 6: Understanding the Gauss-Markov Proof\nThe key step in proving OLS is BLUE involves writing any linear unbiased estimator as: \\[\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{D}\\]\na) Explain in your own words why this decomposition is “clever” and what intuition it provides about comparing estimators.\nb) Show that if \\(\\mathbf{C}\\mathbf{X} = \\mathbf{I}\\) (the unbiasedness constraint), then \\(\\mathbf{D}\\mathbf{X} = \\mathbf{0}\\).\nc) In the final step of the proof, we use the fact that \\(\\mathbf{D}\\mathbf{D}^T\\) is positive semi-definite.\n\nDefine what “positive semi-definite” means\n\nExplain why any matrix of the form \\(\\mathbf{D}\\mathbf{D}^T\\) must be positive semi-definite\n\nHow does this property ensure that OLS has minimum variance?\n\nd) If \\(\\mathbf{D} = \\begin{bmatrix} 1 & 2 \\\\ 0 & -1 \\end{bmatrix}\\), calculate \\(\\mathbf{D}\\mathbf{D}^T\\) and verify that all eigenvalues are non-negative.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-5.html#bonus",
    "href": "ex/ex-5.html#bonus",
    "title": "Random Vectors and Gauss-Markov Theorem Problem Set",
    "section": "Bonus",
    "text": "Bonus\nRead this post on Is OLS BLUE or BUE?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 5: Gauss Markov Theorem"
    ]
  },
  {
    "objectID": "ex/ex-7.html",
    "href": "ex/ex-7.html",
    "title": "Confidence Intervals and Coverage",
    "section": "",
    "text": "Given the following data:\n\\[\\mathbf{y} = \\begin{bmatrix}12\\\\15\\\\18\\\\22\\\\25\\\\28\\end{bmatrix}\\]\n\\[\\mathbf{X} = \\begin{bmatrix}1&2\\\\1&4\\\\1&6\\\\1&8\\\\1&10\\\\1&12\\end{bmatrix}\\]\na) Fit the linear regression model \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\) and construct a 95% confidence interval for \\(\\beta_1\\) using the standard OLS approach.\nb) Interpret the confidence interval in context.\nc) Calculate a 90% confidence interval for \\(\\beta_1\\). How does it compare to the 95% interval?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 7: Confidence Intervals and Coverage"
    ]
  },
  {
    "objectID": "ex/ex-7.html#problem-1",
    "href": "ex/ex-7.html#problem-1",
    "title": "Confidence Intervals and Coverage",
    "section": "",
    "text": "Given the following data:\n\\[\\mathbf{y} = \\begin{bmatrix}12\\\\15\\\\18\\\\22\\\\25\\\\28\\end{bmatrix}\\]\n\\[\\mathbf{X} = \\begin{bmatrix}1&2\\\\1&4\\\\1&6\\\\1&8\\\\1&10\\\\1&12\\end{bmatrix}\\]\na) Fit the linear regression model \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\) and construct a 95% confidence interval for \\(\\beta_1\\) using the standard OLS approach.\nb) Interpret the confidence interval in context.\nc) Calculate a 90% confidence interval for \\(\\beta_1\\). How does it compare to the 95% interval?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 7: Confidence Intervals and Coverage"
    ]
  },
  {
    "objectID": "ex/ex-7.html#problem-2",
    "href": "ex/ex-7.html#problem-2",
    "title": "Confidence Intervals and Coverage",
    "section": "Problem 2",
    "text": "Problem 2\nUsing the same data from Problem 1:\na) Implement a residual bootstrap to construct a 95% confidence interval for \\(\\beta_1\\).\nb) Compare your bootstrap confidence interval to the parametric confidence interval from Problem 1. Are they similar? Why or why not?\nc) Create a histogram of your bootstrap distribution of \\(\\hat{\\beta}_1\\) and overlay the theoretical normal distribution (I’ll get you started:)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 7: Confidence Intervals and Coverage"
    ]
  },
  {
    "objectID": "ex/ex-7.html#problem-3",
    "href": "ex/ex-7.html#problem-3",
    "title": "Confidence Intervals and Coverage",
    "section": "Problem 3",
    "text": "Problem 3\nIn this problem, you’ll verify that under correct model assumptions, 95% confidence intervals actually contain the true parameter 95% of the time.\nSimulation setup:\n\nTrue model: \\(y = 2 + 1.5x + \\varepsilon\\) where \\(\\varepsilon \\sim N(0, 1)\\)\n\nSample size: n = 30\n\nGenerate x from \\(N(0, 1)\\)\n\nNumber of simulations: 5000\n\na) Write a simulation function that:\n\nGenerates data from the true model\n\nFits OLS regression\n\nConstructs a 95% confidence interval for \\(\\beta_1\\)\n\nChecks whether the true value (1.5) is contained in the interval\n\nReturns TRUE or FALSE.\n\nb) Run the simulation 5000 times and calculate the coverage probability (proportion of intervals containing the true value).\nc) What coverage probability did you obtain? Is it close to 95%? Explain why this makes sense given that all assumptions are satisfied.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 7: Confidence Intervals and Coverage"
    ]
  },
  {
    "objectID": "ex/ex-7.html#problem-4",
    "href": "ex/ex-7.html#problem-4",
    "title": "Confidence Intervals and Coverage",
    "section": "Problem 4",
    "text": "Problem 4\nNow investigate what happens when errors are not normally distributed.\nInstead of \\(\\varepsilon \\sim N(0,1)\\), let’s try a skewed distribution. Use this for your epsilon generation to make it lognormally distributed:\n\nepsilon &lt;- exp(rnorm(n)) - exp(0.5)\n\nand then complete the same simulation as above, calculating the coverage probability for the interval for \\(\\beta_1\\).\na) What coverage probability did you obtain?\nb) Create a histogram of your simulated \\(\\varepsilon\\) and describe their distribution. How does this violate OLS assumptions?\nc) Despite the non-normal errors, does the CI coverage seem reasonably close to nominal? Why might this be the case?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 7: Confidence Intervals and Coverage"
    ]
  },
  {
    "objectID": "ex/ex-7.html#problem-5",
    "href": "ex/ex-7.html#problem-5",
    "title": "Confidence Intervals and Coverage",
    "section": "Problem 5",
    "text": "Problem 5\nNow let’s investigate what happens with severe non-constant variance. Instead of generating your \\(\\varepsilon\\sim N(0,1)\\) let’s have it depend on \\(x\\) like this:\n\nsigma &lt;- exp(x / 2)  \nepsilon &lt;- rnorm(n, 0, sigma)\n\nSuch that the standard deviation of your errors increases expondentially with \\(x\\) and then complete the same simulation as above, calculating the coverage probability for the interval for \\(\\beta_1\\).\na) What coverage probability did you obtain? How does it compare to the nominal 95% level?\nb) Create a scatterplot showing one simulated dataset with x on the horizontal axis and y on the vertical axis. Add the true regression line. Describe how the variability changes across the range of x.\nc) Explain why heteroskedasticity affects confidence interval coverage. What aspect of the standard error calculation is violated?\nd) What could you do to obtain valid confidence intervals in the presence of heteroskedasticity?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 7: Confidence Intervals and Coverage"
    ]
  },
  {
    "objectID": "ex/ex-7.html#problem-6",
    "href": "ex/ex-7.html#problem-6",
    "title": "Confidence Intervals and Coverage",
    "section": "Problem 6",
    "text": "Problem 6\nLet’s now investigate what happens when the model is misspecified due to an omitted variable.\nSimulation setup:\n\nTrue model: \\(y = 2 + 1.5x_1 + 2x_2+\\varepsilon\\) where \\(\\varepsilon \\sim N(0, 1)\\)\n\nSample size: n = 30\n\nGenerate \\(x_1\\) from \\(N(0, 1)\\)\n\nGenerate \\(x_2\\) from \\(N(0, 1)\\)\n\nNumber of simulations: 5000\n\nWrite a simulation function that fits a misspecified model, omitting \\(x_2\\) (i.e., the model should be lm(y ~ x1)) and then complete the same simulation as above, calculating the coverage probability for the interval for \\(\\beta_1\\).\na) What coverage probability did you obtain? How does it compare to 95%?\nb) In the simulation code, the true effect of \\(x_1\\) on y is 1.5. When we fit the model omitting \\(x_2\\), what happens to our estimate of \\(\\beta_1\\)? Is it biased? In which direction?\nc) Calculate the average value of \\(\\hat{\\beta}_1\\) across all simulations and compare it to the true value of 1.5. This is the bias in the coefficient estimate.\nd) Explain why omitted variable bias affects confidence interval coverage. Even though the confidence intervals are correctly constructed for the model we fit, why don’t they have the correct coverage for the true parameter?",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Exercises",
      "Exercise 7: Confidence Intervals and Coverage"
    ]
  },
  {
    "objectID": "help-project.html",
    "href": "help-project.html",
    "title": "Creating or Opening a Project",
    "section": "",
    "text": "Before starting an assignment or exercise, always create or open a project:\n\nTo create a new project:\n\nIn RStudio, go to File &gt; New Project.\nChoose whether to create a project in a new directory or within an existing directory.\nNavigate to the folder where you want your project\nClick Create Project.\n\nTo open an existing project:\n\nIn RStudio, go to File &gt; Open Project.\nBrowse to the .Rproj file of the project you want to open.\nSelect it and click Open.\n\n\nThis ensures all your files and settings are organized within the correct project environment.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Creating an RStudio Project"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Help",
    "section": "",
    "text": "Welcome to the help section! This page contains resources to help you succeed in the course, from technical setup to troubleshooting common issues. Click through the sidebar on the left."
  },
  {
    "objectID": "help.html#getting-started-with-rstudio-and-quarto",
    "href": "help.html#getting-started-with-rstudio-and-quarto",
    "title": "Help",
    "section": "Getting Started with RStudio and Quarto",
    "text": "Getting Started with RStudio and Quarto\nIf you’re new to using RStudio Pro or working with Quarto documents, check out our getting started guides.\nGetting Started Guide with RStudio Pro →\nGetting Started Guide with Quarto →"
  },
  {
    "objectID": "help.html#quick-links",
    "href": "help.html#quick-links",
    "title": "Help",
    "section": "Quick Links",
    "text": "Quick Links\n\nRStudio Pro Server: Access your computing environment at login.deac.wfu.edu\nCourse Schedule: Check the schedule for upcoming assignments and due dates"
  },
  {
    "objectID": "help.html#need-additional-help",
    "href": "help.html#need-additional-help",
    "title": "Help",
    "section": "Need Additional Help?",
    "text": "Need Additional Help?\nIf you can’t find what you’re looking for in our guides, don’t hesitate to reach out."
  },
  {
    "objectID": "quarto-quick-guide.html",
    "href": "quarto-quick-guide.html",
    "title": "Quarto Quick Reference Guide",
    "section": "",
    "text": "Quarto is the next-generation version of R Markdown that combines code, results, and narrative text into dynamic documents. This guide covers the essentials you’ll need for STA 312.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#document-structure",
    "href": "quarto-quick-guide.html#document-structure",
    "title": "Quarto Quick Reference Guide",
    "section": "Document Structure",
    "text": "Document Structure\nEvery Quarto document starts with a YAML header (between --- lines) that contains metadata:\n---\ntitle: \"My Statistical Analysis\"\nauthor: \"Your Name\"\ndate: today\nformat: html\nexecute:\n  echo: true\n  warning: false\n---",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#basic-markdown-syntax",
    "href": "quarto-quick-guide.html#basic-markdown-syntax",
    "title": "Quarto Quick Reference Guide",
    "section": "Basic Markdown Syntax",
    "text": "Basic Markdown Syntax\n\nHeaders\n# Header 1 (largest)\n## Header 2\n### Header 3\n#### Header 4\n\n\nText Formatting\n\nBold text: **bold text** or __bold text__\nItalic text: *italic text* or _italic text_\nCode text: `code text`\nLinks: [link text](URL)\n\n\n\nLists\nUnordered lists:\n- Item 1\n- Item 2\n  - Sub-item\n  - Sub-item\nOrdered lists:\n1. First item\n2. Second item\n3. Third item",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#r-code-chunks",
    "href": "quarto-quick-guide.html#r-code-chunks",
    "title": "Quarto Quick Reference Guide",
    "section": "R Code Chunks",
    "text": "R Code Chunks\nThe power of Quarto comes from embedding R code directly in your document.\n\nBasic Code Chunk\nCode chucks start with {r}.\n```{r}\n# Your R code here\nx &lt;- c(1, 2, 3, 4, 5)\nmean(x)\n```\n\n\nCode Chunk Options\nYour code chunks can have options that are written in YAML style starting with a #| at the beginning of the line.\n```{r}\n#| label: data-setup\n#| echo: false\n#| message: false\n#| warning: false\n\n# Load libraries and data\nlibrary(tidyverse)\nlibrary(ggplot2)\n```\n\n\nCommon Chunk Options\n\n#| echo: false: Hide code, show output\n\n#| eval: false: Show code, don’t run it\n\n#| include: false: Run code, hide everything\n\n#| message: false: Hide messages\n\n#| warning: false: Hide warnings\n\n#| fig-width: 8 and #| fig-height: 6: Control plot dimensions",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#inline-r-code",
    "href": "quarto-quick-guide.html#inline-r-code",
    "title": "Quarto Quick Reference Guide",
    "section": "Inline R Code",
    "text": "Inline R Code\nYou can include R results directly in text using `r`:\nThe mean of our data is `r round(mean(x), 2)`.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#mathematical-expressions",
    "href": "quarto-quick-guide.html#mathematical-expressions",
    "title": "Quarto Quick Reference Guide",
    "section": "Mathematical Expressions",
    "text": "Mathematical Expressions\nQuarto uses LaTeX syntax for mathematical notation:\n\nInline Math\nUse single dollar signs for inline math: $x^2 + y^2 = z^2$\n\n\nDisplay Math\nUse double dollar signs for centered display math:\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n\n\nCommon Statistical Notation\n\n\n\n\n\n\n\n\nConcept\nLaTeX Code\nRendered Output\n\n\n\n\nMean\n$\\bar{x}$\n\\(\\bar{x}\\)\n\n\nStandard deviation\n$\\sigma$\n\\(\\sigma\\)\n\n\nRegression equation\n$y = \\beta_0 + \\beta_1 x + \\epsilon$\n\\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\n\n\nSums\n$\\sum_{i=1}^{n} x_i$\n\\(\\sum_{i=1}^{n} x_i\\)\n\n\nSquare root\n$\\sqrt{n}$\n\\(\\sqrt{n}\\)\n\n\nFractions\n$\\frac{a}{b}$\n\\(\\frac{a}{b}\\)\n\n\nSubscripts\n$x_i$\n\\(x_i\\)\n\n\nSuperscripts\n$x^2$\n\\(x^2\\)\n\n\nMatrices\n$\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n\\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#tables",
    "href": "quarto-quick-guide.html#tables",
    "title": "Quarto Quick Reference Guide",
    "section": "Tables",
    "text": "Tables\n\nSimple Tables\n| Variable | Mean | SD   |\n|----------|------|------|\n| Height   | 68.2 | 3.1  |\n| Weight   | 155  | 22.5 |\n\n\nTables from R Code\n```{r}\n#| label: summary-table\n#| tbl-cap: \"Summary Statistics\"\n\nlibrary(knitr)\nsummary_stats &lt;- data.frame(\n  Variable = c(\"Height\", \"Weight\"),\n  Mean = c(68.2, 155.0),\n  SD = c(3.1, 22.5)\n)\nkable(summary_stats)\n```",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#figures",
    "href": "quarto-quick-guide.html#figures",
    "title": "Quarto Quick Reference Guide",
    "section": "Figures",
    "text": "Figures\n\nPlots with Captions\n```{r}\n#| label: fig-scatter\n#| fig-cap: \"Relationship between height and weight\"\n#| fig-width: 6\n#| fig-height: 4\n\nggplot(data, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Height (inches)\", y = \"Weight (lbs)\")\n```",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#cross-references",
    "href": "quarto-quick-guide.html#cross-references",
    "title": "Quarto Quick Reference Guide",
    "section": "Cross-References",
    "text": "Cross-References\nYou can reference figures, tables, and equations:\n\nReference a figure: @fig-scatter\n\nReference a table: @tbl-summary\n\nReference an equation: @eq-regression\n\nFor equations with labels:\n$$y = \\beta_0 + \\beta_1 x + \\epsilon$$ {#eq-regression}",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#callout-blocks",
    "href": "quarto-quick-guide.html#callout-blocks",
    "title": "Quarto Quick Reference Guide",
    "section": "Callout Blocks",
    "text": "Callout Blocks\nQuarto provides special callout blocks for important information:\n::: {.callout-note}\nThis is a note callout.\n:::\n\n::: {.callout-warning}\nThis is a warning callout.\n:::\n\n::: {.callout-important}\nThis is an important callout.\n:::",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#rendering-your-document",
    "href": "quarto-quick-guide.html#rendering-your-document",
    "title": "Quarto Quick Reference Guide",
    "section": "Rendering Your Document",
    "text": "Rendering Your Document\nTo render your Quarto document:\n\nIn RStudio: Click the “Render” button\n\nCommand line: Run quarto render filename.qmd\n\nPreview: Use quarto preview filename.qmd for live preview",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "quarto-quick-guide.html#tips-for-statistical-writing",
    "href": "quarto-quick-guide.html#tips-for-statistical-writing",
    "title": "Quarto Quick Reference Guide",
    "section": "Tips for Statistical Writing",
    "text": "Tips for Statistical Writing\n\nAlways explain your code: Use comments (#) and narrative text\nRound numbers appropriately: Use round() function for inline code\nLabel your chunks: Makes debugging easier\nInclude figure captions: Helps with interpretation",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Computing",
      "Quarto Quick Guide"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "\n Schedule",
    "section": "",
    "text": "Note: The timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\nweek\n\n\ndate\n\n\ntopic\n\n\nprepare\n\n\nslides\n\n\nexercises\n\n\n\n\n\n1\n\n\n27 August\n\n\nWelcome to Linear Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR and Reproducible Workflows\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n29 August\n\n\nMatrix Refresher\n\n\n\n\n\n\n\n\n\n\n\n\n2\n\n\n3 September\n\n\nMatrix Workshop\n\n\n\n\n\n\n\n\n\n\n\n\n2\n\n\n5 September\n\n\nLinear Regression Fundamentals\n\n\nFaraway Sections 1.3 - 1.4\n\n\n\n\n\n\n\n\n\n\n3\n\n\n10 September\n\n\nLinear Regression Fundamentals (Workshop)\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\n12 September\n\n\nDeriving the Hat Matrix and QR Decomposition\n\n\nFaraway Sections 2.1-2.6\n\n\n\n\n\n\n\n\n\n\n4\n\n\n17 September\n\n\nNo Class (Work on Exercise 4)\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n19 September\n\n\nProperties of Random Vectors\n\n\nFaraway Section 2.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss Markov Theorem\n\n\n\n\n\n\n\n\n\n\n\n\n5\n\n\n24 September\n\n\nGauss Markov Theorem (Workshop)\n\n\n\n\n\n\n\n\n\n\n\n\n5\n\n\n26 September\n\n\nResidual Sum of Squares\n\n\nFaraway Sections 2.9, 3.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of Fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n6\n\n\n1 October\n\n\nPractical Workshop\n\n\n\n\n\n\n\n\n\n\n\n\n6\n\n\n3 October\n\n\nMidterm review\n\n\n\n\n\n\n\n\n\n\n\n\n7\n\n\n8 October\n\n\nMidterm (In Class)\n\n\n\n\n\n\n\n\n\n\n\n\n7\n\n\n10 October\n\n\nMidterm (Out of Class)\n\n\n\n\n\n\n\n\n\n\n\n\n8\n\n\n15 October\n\n\nMidterm Recap\n\n\n\n\n\n\n\n\n\n\n\n\n8\n\n\n17 October\n\n\nConfidence Intervals\n\n\nFaraway Sections 3.4-3.6\n\n\n\n\n\n\n\n\n\n\n9\n\n\n22 October\n\n\nConfidence Intervals (Workshop)\n\n\n\n\n\n\n\n\n\n\n\n\n9\n\n\n24 October\n\n\nDiagnostics\n\n\nFaraway Chapter 6\n\n\n\n\n\n\n\n\n\n\n10\n\n\n29 October\n\n\nDiagnostics (Workshop)\n\n\n\n\n\n\n\n\n\n\n\n\n10\n\n\n31 October\n\n\nTransformations\n\n\nFaraway Chapter 9\n\n\n\n\n\n\n\n\n\n\n11\n\n\n5 November\n\n\nTransformations (Workshop)\n\n\n\n\n\n\n\n\n\n\n\n\n11\n\n\n7 November\n\n\nPrediction & Prediction Intervals\n\n\nFaraway Chapter 4\n\n\n\n\n\n\n\n\n\n\n12\n\n\n12 November\n\n\nPrediction (Workshop)\n\n\n\n\n\n\n\n\n\n\n\n\n12\n\n\n14 November\n\n\nExplanation\n\n\nFaraway Chapter 5\n\n\n\n\n\n\n\n\n\n\n13\n\n\n19 November\n\n\nCausal Inference (Workshop)\n\n\n\n\n\n\n\n\n\n\n\n\n13\n\n\n21 November\n\n\nFinal Project\n\n\n\n\n\n\n\n\n\n\n\n\n14\n\n\n26 November\n\n\nThanksgiving\n\n\n\n\n\n\n\n\n\n\n\n\n14\n\n\n28 November\n\n\nThanksgiving\n\n\n\n\n\n\n\n\n\n\n\n\n15\n\n\n3 December\n\n\nFinal Presentation (Out of Class)\n\n\n\n\n\n\n\n\n\n\n\n\n15\n\n\n5 December\n\n\nFinal Presentation (Out of Class)",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Schedule"
    ]
  },
  {
    "objectID": "slides/02-slides.html#what-is-a-matrix",
    "href": "slides/02-slides.html#what-is-a-matrix",
    "title": "Matrix Algebra Fundamentals",
    "section": "What is a Matrix?",
    "text": "What is a Matrix?\n\nA matrix is a rectangular array of numbers arranged in rows and columns\nWritten using square brackets or parentheses\nEach number in the matrix is called an element or entry"
  },
  {
    "objectID": "slides/02-slides.html#what-is-a-matrix-1",
    "href": "slides/02-slides.html#what-is-a-matrix-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "What is a Matrix?",
    "text": "What is a Matrix?\n\\[\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-dimensions",
    "href": "slides/02-slides.html#matrix-dimensions",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Dimensions",
    "text": "Matrix Dimensions\n\nRows are horizontal ↔︎\n\n\nColumns are vertical ↕\n\n\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\require{color}\\colorbox{#eec8e1}{$a_{11}$} & \\colorbox{#eec8e1}{$a_{12}$} & \\colorbox{#eec8e1}{$a_{13}$} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix} \\leftarrow \\text{Row 1}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-dimensions-1",
    "href": "slides/02-slides.html#matrix-dimensions-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Dimensions",
    "text": "Matrix Dimensions\nRows are horizontal ↔︎\nColumns are vertical ↕\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\require{color}a_{11} & a_{12} & a_{13} \\\\\n\\colorbox{#eec8e1}{$a_{21}$} & \\colorbox{#eec8e1}{$a_{22}$} & \\colorbox{#eec8e1}{$a_{23}$} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix} \\leftarrow \\text{Row 2}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-dimensions-2",
    "href": "slides/02-slides.html#matrix-dimensions-2",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Dimensions",
    "text": "Matrix Dimensions\nRows are horizontal ↔︎\nColumns are vertical ↕\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\require{color}a_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\n\\colorbox{#eec8e1}{$a_{31}$} & \\colorbox{#eec8e1}{$a_{32}$} & \\colorbox{#eec8e1}{$a_{33}$}\n\\end{bmatrix} \\leftarrow \\text{Row 3}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-dimensions-3",
    "href": "slides/02-slides.html#matrix-dimensions-3",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Dimensions",
    "text": "Matrix Dimensions\nRows are horizontal ↔︎\nColumns are vertical ↕\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\colorbox{#eec8e1}{$a_{11}$} & a_{12} & a_{13} \\\\\n\\colorbox{#eec8e1}{$a_{21}$} & a_{22} & a_{23} \\\\\n\\colorbox{#eec8e1}{$a_{31}$} & a_{32} & a_{33}\n\\end{bmatrix} \\leftarrow \\text{Column 1}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-size-n-p",
    "href": "slides/02-slides.html#matrix-size-n-p",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Size: n × p",
    "text": "Matrix Size: n × p\n\nA matrix with n rows and p columns is called an n × p matrix\n\n\n\\[\\mathbf{A}_{3 \\times 3} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\\]\nThis is a 3 × 3 matrix (3 rows, 3 columns)"
  },
  {
    "objectID": "slides/02-slides.html#matrix-size-n-p-1",
    "href": "slides/02-slides.html#matrix-size-n-p-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Size: n × p",
    "text": "Matrix Size: n × p\nA matrix with n rows and p columns is called an n × p matrix\n\\[\\mathbf{B}_{2 \\times 4} = \\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & b_{14} \\\\\nb_{21} & b_{22} & b_{23} & b_{24}\n\\end{bmatrix}\\]\nThis is a 2 × 4 matrix (2 rows, 4 columns)"
  },
  {
    "objectID": "slides/02-slides.html#examples-of-different-matrix-sizes",
    "href": "slides/02-slides.html#examples-of-different-matrix-sizes",
    "title": "Matrix Algebra Fundamentals",
    "section": "Examples of Different Matrix Sizes",
    "text": "Examples of Different Matrix Sizes\n\n\nRow vector (1 × p): \\[\\mathbf{r} = \\begin{bmatrix} 1 & 3 & 5 & 7 \\end{bmatrix}\\]\n\n\n\nColumn vector (n × 1): \\[\\mathbf{c} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#examples-of-different-matrix-sizes-1",
    "href": "slides/02-slides.html#examples-of-different-matrix-sizes-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Examples of Different Matrix Sizes",
    "text": "Examples of Different Matrix Sizes\nSquare matrix (n × n): \\[\\mathbf{S} = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-addition",
    "href": "slides/02-slides.html#matrix-addition",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Addition",
    "text": "Matrix Addition\n\nMatrices can be added only if they have the same dimensions\n\n\nAdd corresponding elements: \\[\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} a_{11} + b_{11} & a_{12} + b_{12} \\\\ a_{21} + b_{21} & a_{22} + b_{22} \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-addition-1",
    "href": "slides/02-slides.html#matrix-addition-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Addition",
    "text": "Matrix Addition\n\nExample: \\[\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\]\n\n\n\\[= \\begin{bmatrix} \\color{red}{1+5} & \\color{blue}{2+6} \\\\ \\color{green}{3+7} & \\color{purple}{4+8} \\end{bmatrix} = \\begin{bmatrix} \\color{red}{6} & \\color{blue}{8} \\\\ \\color{green}{10} & \\color{purple}{12} \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#you-try-matrix-addition",
    "href": "slides/02-slides.html#you-try-matrix-addition",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Matrix Addition",
    "text": "You Try: Matrix Addition\n\nProblem: Add these matrices \\(\\mathbf{A} = \\begin{bmatrix} 3 & -1 & 2 \\\\ 0 & 4 & -3 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 1 & 2 & -1 \\\\ 5 & -2 & 1 \\end{bmatrix}\\)\n\n\nYou Try: \\(\\mathbf{A} + \\mathbf{B} = ?\\)\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/02-slides.html#lets-verify-in-r",
    "href": "slides/02-slides.html#lets-verify-in-r",
    "title": "Matrix Algebra Fundamentals",
    "section": " Let’s verify in R",
    "text": "Let’s verify in R\n\n\nA &lt;- matrix(c(3, -1, 2,\n              0, 4, -3), \n            nrow = 2, byrow = TRUE)\nB &lt;- matrix(c(1, 2, -1,\n              5, -2, 1),\n            nrow = 2, byrow = TRUE)\nA + B\n\n     [,1] [,2] [,3]\n[1,]    4    1    1\n[2,]    5    2   -2"
  },
  {
    "objectID": "slides/02-slides.html#matrix-addition-dimension-requirements",
    "href": "slides/02-slides.html#matrix-addition-dimension-requirements",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Addition: Dimension Requirements",
    "text": "Matrix Addition: Dimension Requirements\n\n\nThis works ✓\n\n\\[\\colorbox{#eec8e1}{$\\mathbf{A}_{2 \\times 3} + \\mathbf{B}_{2 \\times 3} = \\mathbf{C}_{2 \\times 3}$}\\]\n\n\n\n\nThis doesn’t work ✗\n\n\\[\\colorbox{#eec8e1}{$\\mathbf{A}_{2 \\times 3} + \\mathbf{B}_{3 \\times 2} = \\text{?}$}\\]\n\n\n\nThe matrices must have exactly the same dimensions to be added!"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-the-basics",
    "href": "slides/02-slides.html#matrix-multiplication-the-basics",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: The Basics",
    "text": "Matrix Multiplication: The Basics\n\nMatrix multiplication is NOT element-wise multiplication\nFor \\(\\mathbf{A} \\times \\mathbf{B}\\) to be defined the number of columns in A must equal number of rows in B (inner dimensions must match!)\nIf \\(\\mathbf{A}\\) is \\(m \\times n\\) and \\(\\mathbf{B}\\) is \\(n \\times p\\), then: \\[\\mathbf{A}_{m \\times n} \\times \\mathbf{B}_{n \\times p} = \\mathbf{C}_{m \\times p}\\]"
  },
  {
    "objectID": "slides/02-slides.html#you-try-matrix-dimensions",
    "href": "slides/02-slides.html#you-try-matrix-dimensions",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Matrix Dimensions",
    "text": "You Try: Matrix Dimensions\n\nProblem: Can these matrices be multiplied? If yes, what’s the resulting dimension?\n\\(\\mathbf{A}_{3 \\times 2} \\times \\mathbf{B}_{2 \\times 4} = ?\\)\n\\(\\mathbf{C}_{2 \\times 5} \\times \\mathbf{D}_{3 \\times 2} = ?\\)\n\n\nSolutions:\n\n\\(\\mathbf{A}_{3 \\times 2} \\times \\mathbf{B}_{2 \\times 4} = \\mathbf{Result}_{3 \\times 4}\\) ✓ (inner dimensions match: 2 = 2)\n\\(\\mathbf{C}_{2 \\times 5} \\times \\mathbf{D}_{3 \\times 2}\\) = undefined ✗ (inner dimensions don’t match: 5 ≠ 3)\n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}&\\\\&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-1",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}\\colorbox{#eec8e1}{}&\\\\&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-2",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-2",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\\begin{bmatrix} \\colorbox{#eec8e1}{1} & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} \\colorbox{#eec8e1}{5} & 6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}\\colorbox{#eec8e1}{(1)(5)}&\\\\&\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-3",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-3",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\\begin{bmatrix} \\colorbox{#eec8e1}{1} & \\colorbox{#eec8e1}2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} \\colorbox{#eec8e1}{5} & 6 \\\\ \\colorbox{#eec8e1}7 & 8 \\end{bmatrix}=\\begin{bmatrix}\\colorbox{#eec8e1}{(1)(5) + (2)(7)}&\\\\&\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-4",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-4",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\\begin{bmatrix} \\colorbox{#eec8e1}{1} & \\colorbox{#eec8e1}2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} \\colorbox{#eec8e1}{5} & 6 \\\\ \\colorbox{#eec8e1}7 & 8 \\end{bmatrix}=\\begin{bmatrix}\\colorbox{#eec8e1}{19}&\\\\&\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-5",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-5",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&\\colorbox{#eec8e1}{ }\\\\&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-6",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-6",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} \\colorbox{#eec8e1}1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & \\colorbox{#eec8e1}6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&\\colorbox{#eec8e1}{(1)(6)}\\\\&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-7",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-7",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} \\colorbox{#eec8e1}1 & \\colorbox{#eec8e1}2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & \\colorbox{#eec8e1}6 \\\\ 7 & \\colorbox{#eec8e1}8 \\end{bmatrix}=\\begin{bmatrix}19&\\colorbox{#eec8e1}{(1)(6)+(2)(8)}\\\\&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-8",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-8",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} \\colorbox{#eec8e1}1 & \\colorbox{#eec8e1}2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & \\colorbox{#eec8e1}6 \\\\ 7 & \\colorbox{#eec8e1}8 \\end{bmatrix}=\\begin{bmatrix}19&\\colorbox{#eec8e1}{22}\\\\&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-9",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-9",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\\\colorbox{#eec8e1}{ }&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-10",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-10",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ \\colorbox{#eec8e1}3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} \\colorbox{#eec8e1}5 & 6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\\\colorbox{#eec8e1}{(3)(5) }&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-11",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-11",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ \\colorbox{#eec8e1}3 & \\colorbox{#eec8e1}4 \\end{bmatrix} \\times \\begin{bmatrix} \\colorbox{#eec8e1}5 & 6 \\\\ \\colorbox{#eec8e1}7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\\\colorbox{#eec8e1}{(3)(5) +(4)(7)}&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-12",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-12",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ \\colorbox{#eec8e1}3 & \\colorbox{#eec8e1}4 \\end{bmatrix} \\times \\begin{bmatrix} \\colorbox{#eec8e1}5 & 6 \\\\ \\colorbox{#eec8e1}7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\\\colorbox{#eec8e1}{43}&\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-13",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-13",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\43&\\colorbox{#eec8e1}{ }\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-14",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-14",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ \\colorbox{#eec8e1}3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & \\colorbox{#eec8e1}6 \\\\ 7 & 8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\43&\\colorbox{#eec8e1}{(3)(6) }\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-15",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-15",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ \\colorbox{#eec8e1}3 & \\colorbox{#eec8e1}4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & \\colorbox{#eec8e1}6 \\\\ 7 & \\colorbox{#eec8e1}8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\43&\\colorbox{#eec8e1}{(3)(6)+(4)(8) }\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-step-by-step-16",
    "href": "slides/02-slides.html#matrix-multiplication-step-by-step-16",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Step by Step",
    "text": "Matrix Multiplication: Step by Step\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ \\colorbox{#eec8e1}3 & \\colorbox{#eec8e1}4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & \\colorbox{#eec8e1}6 \\\\ 7 & \\colorbox{#eec8e1}8 \\end{bmatrix}=\\begin{bmatrix}19&22{}\\\\43&\\colorbox{#eec8e1}{52}\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/02-slides.html#you-try-matrix-multiplication",
    "href": "slides/02-slides.html#you-try-matrix-multiplication",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Matrix Multiplication",
    "text": "You Try: Matrix Multiplication\n\nProblem: Multiply these matrices \\(\\mathbf{A} = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\)\n\n\nYou Try: \\(\\mathbf{A} \\times \\mathbf{B} = ?\\)\n\n\nSolution:\n\\(\\mathbf{A} \\times \\mathbf{B} = \\begin{bmatrix} 8 & 3 \\\\ 9 & 4 \\end{bmatrix}\\)\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/02-slides.html#lets-verify-in-r-1",
    "href": "slides/02-slides.html#lets-verify-in-r-1",
    "title": "Matrix Algebra Fundamentals",
    "section": " Let’s verify in R",
    "text": "Let’s verify in R\n\n\nA &lt;- matrix(c(2, 3,\n              1, 4), \n            nrow = 2, byrow = TRUE)\nB &lt;- matrix(c(1, 0,\n              2, 1),\n            nrow = 2, byrow = TRUE)\nA %*% B\n\n     [,1] [,2]\n[1,]    8    3\n[2,]    9    4"
  },
  {
    "objectID": "slides/02-slides.html#matrix-multiplication-dimension-examples",
    "href": "slides/02-slides.html#matrix-multiplication-dimension-examples",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Multiplication: Dimension Examples",
    "text": "Matrix Multiplication: Dimension Examples\n\n\nValid multiplications:\n\n\\(\\mathbf{A}_{2 \\times 3} \\times \\mathbf{B}_{3 \\times 4} = \\mathbf{C}_{2 \\times 4}\\) ✓\n\\(\\mathbf{A}_{5 \\times 2} \\times \\mathbf{B}_{2 \\times 1} = \\mathbf{C}_{5 \\times 1}\\) ✓\n\n\n\n\nInvalid multiplications:\n\n\\(\\mathbf{A}_{2 \\times 3} \\times \\mathbf{B}_{4 \\times 2}\\) ✗ (3 ≠ 4)\n\\(\\mathbf{A}_{3 \\times 5} \\times \\mathbf{B}_{2 \\times 3}\\) ✗ (5 ≠ 2)\n\n\n\nRemember: Inner dimensions must match!"
  },
  {
    "objectID": "slides/02-slides.html#matrix-transpose",
    "href": "slides/02-slides.html#matrix-transpose",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Transpose",
    "text": "Matrix Transpose\n\nThe transpose of matrix \\(\\mathbf{A}\\) is denoted \\(\\mathbf{A}^T\\) or \\(\\mathbf{A}'\\)\n\n\nFlip rows and columns\n\nRow 1 becomes Column 1\nRow 2 becomes Column 2, etc."
  },
  {
    "objectID": "slides/02-slides.html#matrix-transpose-1",
    "href": "slides/02-slides.html#matrix-transpose-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Transpose",
    "text": "Matrix Transpose\n\n\n\n\\[\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2 \\times 3}\\]\n\n\n\n\\[\\mathbf{A}^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}_{3 \\times 2}\\]"
  },
  {
    "objectID": "slides/02-slides.html#you-try-matrix-transpose",
    "href": "slides/02-slides.html#you-try-matrix-transpose",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Matrix Transpose",
    "text": "You Try: Matrix Transpose\n\nFind the transpose of this matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\end{bmatrix}\\) \\(A^T= ?\\)\n\n\nSolution: \\(\\mathbf{A}^T = \\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\\\ 7 & 8 \\end{bmatrix}\\)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/02-slides.html#lets-verify-in-r-2",
    "href": "slides/02-slides.html#lets-verify-in-r-2",
    "title": "Matrix Algebra Fundamentals",
    "section": " Let’s verify in R",
    "text": "Let’s verify in R\n\n\nA &lt;- matrix(c(1, 4, 7,\n              2, 5, 8), \n            nrow = 2, byrow = TRUE)\n\nt(A)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    4    5\n[3,]    7    8"
  },
  {
    "objectID": "slides/02-slides.html#transpose-properties",
    "href": "slides/02-slides.html#transpose-properties",
    "title": "Matrix Algebra Fundamentals",
    "section": "Transpose Properties",
    "text": "Transpose Properties\nIf \\(\\mathbf{A}\\) is \\(m \\times n\\), then \\(\\mathbf{A}^T\\) is \\(n \\times m\\)"
  },
  {
    "objectID": "slides/02-slides.html#transpose-properties-1",
    "href": "slides/02-slides.html#transpose-properties-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Transpose Properties",
    "text": "Transpose Properties\n\n\\((\\mathbf{A}^T)^T = \\mathbf{A}\\)\n\\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\)\n\\((\\mathbf{AB})^T = \\mathbf{B}^T\\mathbf{A}^T\\) (order reverses!)\nSymmetric Matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)"
  },
  {
    "objectID": "slides/02-slides.html#symmetric-matrix",
    "href": "slides/02-slides.html#symmetric-matrix",
    "title": "Matrix Algebra Fundamentals",
    "section": "Symmetric Matrix",
    "text": "Symmetric Matrix\n\\(\\mathbf{A} = \\mathbf{A}^T\\)\n\\[\\mathbf{S} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#the-identity-matrix",
    "href": "slides/02-slides.html#the-identity-matrix",
    "title": "Matrix Algebra Fundamentals",
    "section": "The Identity Matrix",
    "text": "The Identity Matrix\n\nThe identity matrix \\(\\mathbf{I}\\) is the matrix equivalent of the number 1\n\n\n\nSquare matrix with 1’s on the diagonal and 0’s elsewhere\nActs as the multiplicative identity: \\(\\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}\\)"
  },
  {
    "objectID": "slides/02-slides.html#the-identity-matrix-1",
    "href": "slides/02-slides.html#the-identity-matrix-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "The Identity Matrix",
    "text": "The Identity Matrix\n\\[\\mathbf{I}_2 = \\begin{bmatrix} \\color{red}{1} & 0 \\\\ 0 & \\color{red}{1} \\end{bmatrix}, \\quad \\mathbf{I}_3 = \\begin{bmatrix} \\color{red}{1} & 0 & 0 \\\\ 0 & \\color{red}{1} & 0 \\\\ 0 & 0 & \\color{red}{1} \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#the-identity-matrix-2",
    "href": "slides/02-slides.html#the-identity-matrix-2",
    "title": "Matrix Algebra Fundamentals",
    "section": "The Identity Matrix",
    "text": "The Identity Matrix\nExample: \\[\\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#you-try-identity-matrix",
    "href": "slides/02-slides.html#you-try-identity-matrix",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Identity Matrix",
    "text": "You Try: Identity Matrix\n\nProblem: Verify that this multiplication gives the identity property \\(\\begin{bmatrix} 3 & -1 \\\\ 2 & 4 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = ?\\)\n\n\nSolution: \\(= \\begin{bmatrix} 3 & -1 \\\\ 2 & 4 \\end{bmatrix}\\) ✓ (Original matrix unchanged!)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/02-slides.html#lets-verify-in-r-3",
    "href": "slides/02-slides.html#lets-verify-in-r-3",
    "title": "Matrix Algebra Fundamentals",
    "section": " Let’s verify in R",
    "text": "Let’s verify in R\n\n\nA &lt;- matrix(c(3, -1,\n              2, 4), \n            nrow = 2, byrow = TRUE)\nI &lt;- diag(2)\nA %*% I\n\n     [,1] [,2]\n[1,]    3   -1\n[2,]    2    4"
  },
  {
    "objectID": "slides/02-slides.html#matrix-inverse",
    "href": "slides/02-slides.html#matrix-inverse",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Inverse",
    "text": "Matrix Inverse\n\nThe inverse of matrix \\(\\mathbf{A}\\) is denoted \\(\\mathbf{A}^{-1}\\)\n\n\nDefinition: \\(\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)"
  },
  {
    "objectID": "slides/02-slides.html#matrix-inverse-1",
    "href": "slides/02-slides.html#matrix-inverse-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Inverse",
    "text": "Matrix Inverse\n\nRequirements for inverse to exist:\n- Matrix must be square (n × n)\n- Matrix must be non-singular (determinant ≠ 0)"
  },
  {
    "objectID": "slides/02-slides.html#matrix-inverse-2",
    "href": "slides/02-slides.html#matrix-inverse-2",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Inverse",
    "text": "Matrix Inverse\n\n2 × 2 Formula: For \\(\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\)\n\\[\\mathbf{A}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-inverse-example",
    "href": "slides/02-slides.html#matrix-inverse-example",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Inverse: Example",
    "text": "Matrix Inverse: Example\n\n\\[\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 6 & 2 \\end{bmatrix}\\]\n\n\nStep 1: Calculate determinant \\[\\text{det}(\\mathbf{A}) = (2)(2) - (1)(6) = 4 - 6 = -2\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-inverse-example-1",
    "href": "slides/02-slides.html#matrix-inverse-example-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Inverse: Example",
    "text": "Matrix Inverse: Example\n\\[\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 6 & 2 \\end{bmatrix}\\]\n\nStep 2: Apply formula \\[\\mathbf{A}^{-1} = \\frac{1}{-2} \\begin{bmatrix} 2 & -1 \\\\ -6 & 2 \\end{bmatrix} = \\begin{bmatrix} -1 & 0.5 \\\\ 3 & -1 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#matrix-inverse-example-2",
    "href": "slides/02-slides.html#matrix-inverse-example-2",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Inverse: Example",
    "text": "Matrix Inverse: Example\n\\[\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 6 & 2 \\end{bmatrix}\\]\n\nVerify: \\[\\mathbf{AA}^{-1} = \\begin{bmatrix} 2 & 1 \\\\ 6 & 2 \\end{bmatrix} \\begin{bmatrix} -1 & 0.5 \\\\ 3 & -1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\mathbf{I}\\]"
  },
  {
    "objectID": "slides/02-slides.html#you-try-matrix-inverse",
    "href": "slides/02-slides.html#you-try-matrix-inverse",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Matrix Inverse",
    "text": "You Try: Matrix Inverse\n\nProblem: Find the inverse of this matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 \\\\ 4 & 3 \\end{bmatrix}\\)\n\n\nSolution: 1. \\(\\text{det}(\\mathbf{A}) = (1)(3) - (2)(4) = 3 - 8 = -5\\)\n2. \\(\\mathbf{A}^{-1} = \\frac{1}{-5} \\begin{bmatrix} 3 & -2 \\\\ -4 & 1 \\end{bmatrix} = \\begin{bmatrix} -0.6 & 0.4 \\\\ 0.8 & -0.2 \\end{bmatrix}\\)\nCheck: \\(\\mathbf{AA}^{-1} = \\begin{bmatrix} 1 & 2 \\\\ 4 & 3 \\end{bmatrix} \\begin{bmatrix} -0.6 & 0.4 \\\\ 0.8 & -0.2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) ✓\n\n\n\n\n−+\n06:00"
  },
  {
    "objectID": "slides/02-slides.html#lets-verify-in-r-4",
    "href": "slides/02-slides.html#lets-verify-in-r-4",
    "title": "Matrix Algebra Fundamentals",
    "section": " Let’s verify in R",
    "text": "Let’s verify in R\n\n\nA &lt;- matrix(c(1, 2, \n              4, 3), \n            nrow = 2, byrow = TRUE)\nsolve(A)\n\n     [,1] [,2]\n[1,] -0.6  0.4\n[2,]  0.8 -0.2\n\nround(A %*% solve(A), 10)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1"
  },
  {
    "objectID": "slides/02-slides.html#what-about-matrices-of-higher-dimension",
    "href": "slides/02-slides.html#what-about-matrices-of-higher-dimension",
    "title": "Matrix Algebra Fundamentals",
    "section": "What about matrices of higher dimension?",
    "text": "What about matrices of higher dimension?\n\nLife is short! Let’s just use R.\nBe sure to check the determinant! (it cannot be 0)\n\n\nA &lt;- matrix(c(1, 2,\n              2, 4),\n            nrow = 2, byrow = TRUE)\ndet(A)\n\n[1] 0"
  },
  {
    "objectID": "slides/02-slides.html#special-matrix-types",
    "href": "slides/02-slides.html#special-matrix-types",
    "title": "Matrix Algebra Fundamentals",
    "section": "Special Matrix Types",
    "text": "Special Matrix Types\n\nDiagonal Matrix: Non-zero elements only on the main diagonal\n\\[\\mathbf{D} = \\begin{bmatrix} d_1 & 0 & 0 \\\\ 0 & d_2 & 0 \\\\ 0 & 0 & d_3 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#special-matrix-types-1",
    "href": "slides/02-slides.html#special-matrix-types-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Special Matrix Types",
    "text": "Special Matrix Types\n\nOrthogonal Matrix: \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\) (columns are orthonormal)"
  },
  {
    "objectID": "slides/02-slides.html#special-matrix-types-2",
    "href": "slides/02-slides.html#special-matrix-types-2",
    "title": "Matrix Algebra Fundamentals",
    "section": "Special Matrix Types",
    "text": "Special Matrix Types\n\nPositive Definite: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x} \\neq \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/02-slides.html#special-matrix-types-3",
    "href": "slides/02-slides.html#special-matrix-types-3",
    "title": "Matrix Algebra Fundamentals",
    "section": "Special Matrix Types",
    "text": "Special Matrix Types\n\nSymmetric Matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)"
  },
  {
    "objectID": "slides/02-slides.html#you-try-special-matrices",
    "href": "slides/02-slides.html#you-try-special-matrices",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Special Matrices",
    "text": "You Try: Special Matrices\n\nProblem: Identify the type of each matrix \\(\\mathbf{A} = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 1 & 4 \\\\ 4 & 2 \\end{bmatrix}, \\quad \\mathbf{C} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\)\n\n\nSolutions: - Matrix A: Diagonal matrix (non-zero elements only on main diagonal) - Matrix B: Symmetric matrix (\\(\\mathbf{B} = \\mathbf{B}^T\\) since \\(b_{12} = b_{21} = 4\\)) - Matrix C: General matrix (no special properties)\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/02-slides.html#vector-derivatives-introduction",
    "href": "slides/02-slides.html#vector-derivatives-introduction",
    "title": "Matrix Algebra Fundamentals",
    "section": "Vector Derivatives: Introduction",
    "text": "Vector Derivatives: Introduction\n\nWhy do we need derivatives of vectors?\n\nEssential for optimization in statistics and machine learning\nKey to deriving least squares solutions\nFoundation for understanding the hat matrix"
  },
  {
    "objectID": "slides/02-slides.html#vector-derivatives-notation",
    "href": "slides/02-slides.html#vector-derivatives-notation",
    "title": "Matrix Algebra Fundamentals",
    "section": "Vector Derivatives: Notation",
    "text": "Vector Derivatives: Notation\n\nLet \\(\\mathbf{a} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix}\\) be column vectors"
  },
  {
    "objectID": "slides/02-slides.html#derivative-rule-linear-form",
    "href": "slides/02-slides.html#derivative-rule-linear-form",
    "title": "Matrix Algebra Fundamentals",
    "section": "Derivative Rule: Linear Form",
    "text": "Derivative Rule: Linear Form\n\nRule: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{a}'\\mathbf{x}) = \\mathbf{a}\\)\n\n\nIn words: The derivative of a linear form \\(\\mathbf{a}'\\mathbf{x}\\) with respect to \\(\\mathbf{x}\\) is simply \\(\\mathbf{a}\\)"
  },
  {
    "objectID": "slides/02-slides.html#derivative-rule-example",
    "href": "slides/02-slides.html#derivative-rule-example",
    "title": "Matrix Algebra Fundamentals",
    "section": "Derivative Rule: Example",
    "text": "Derivative Rule: Example\n\nLet \\(\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\)\n\n\nThen \\(\\mathbf{a}'\\mathbf{x} = 2x_1 + 3x_2 + x_3\\)\n\n\nDerivative: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(2x_1 + 3x_2 + x_3) = \\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\end{bmatrix} = \\mathbf{a}\\)"
  },
  {
    "objectID": "slides/02-slides.html#you-try-vector-derivatives",
    "href": "slides/02-slides.html#you-try-vector-derivatives",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Vector Derivatives",
    "text": "You Try: Vector Derivatives\n\nProblem: Find \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{c}'\\mathbf{x})\\) where \\(\\mathbf{c} = \\begin{bmatrix} 5 \\\\ -2 \\\\ 4 \\end{bmatrix}\\)\n\n\nSolution: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{c}'\\mathbf{x}) = \\mathbf{c} = \\begin{bmatrix} 5 \\\\ -2 \\\\ 4 \\end{bmatrix}\\)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/02-slides.html#matrix-derivatives-quadratic-forms",
    "href": "slides/02-slides.html#matrix-derivatives-quadratic-forms",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Derivatives: Quadratic Forms",
    "text": "Matrix Derivatives: Quadratic Forms\n\nQuadratic form: \\(\\mathbf{x}'\\mathbf{A}\\mathbf{x}\\) where \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix\n\n\nExample: \\(\\mathbf{x}'\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/02-slides.html#matrix-derivatives-quadratic-forms-1",
    "href": "slides/02-slides.html#matrix-derivatives-quadratic-forms-1",
    "title": "Matrix Algebra Fundamentals",
    "section": "Matrix Derivatives: Quadratic Forms",
    "text": "Matrix Derivatives: Quadratic Forms\n\nExpanding the 2×2 case: \\[\\mathbf{x}'\\mathbf{A}\\mathbf{x} = a_{11}x_1^2 + (a_{12} + a_{21})x_1x_2 + a_{22}x_2^2\\]"
  },
  {
    "objectID": "slides/02-slides.html#derivative-rule-3-quadratic-forms",
    "href": "slides/02-slides.html#derivative-rule-3-quadratic-forms",
    "title": "Matrix Algebra Fundamentals",
    "section": "Derivative Rule 3: Quadratic Forms",
    "text": "Derivative Rule 3: Quadratic Forms\n\nRule: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}')\\mathbf{x}\\)\n\n\nSpecial case: If \\(\\mathbf{A}\\) is symmetric (\\(\\mathbf{A} = \\mathbf{A}'\\)), then: \\[\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\]"
  },
  {
    "objectID": "slides/02-slides.html#derivative-rule-3-example",
    "href": "slides/02-slides.html#derivative-rule-3-example",
    "title": "Matrix Algebra Fundamentals",
    "section": "Derivative Rule 3: Example",
    "text": "Derivative Rule 3: Example\n\nLet \\(\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)\n\n\nStep 1: Find \\(\\mathbf{A} + \\mathbf{A}'\\) \\[\\mathbf{A}' = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix} \\mathbf{A} + \\mathbf{A}' = \\begin{bmatrix} 4 & 4 \\\\ 4 & 8 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#derivative-rule-3-example-cont.",
    "href": "slides/02-slides.html#derivative-rule-3-example-cont.",
    "title": "Matrix Algebra Fundamentals",
    "section": "Derivative Rule 3: Example (cont.)",
    "text": "Derivative Rule 3: Example (cont.)\nLet \\(\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)\n\nStep 2: Apply the rule \\[\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}')\\mathbf{x} = \\begin{bmatrix} 4 & 4 \\\\ 4 & 8 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 4x_1 + 4x_2 \\\\ 4x_1 + 8x_2 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/02-slides.html#symmetric-matrix-case",
    "href": "slides/02-slides.html#symmetric-matrix-case",
    "title": "Matrix Algebra Fundamentals",
    "section": "Symmetric Matrix Case",
    "text": "Symmetric Matrix Case\n\nExample: Let \\(\\mathbf{S} = \\begin{bmatrix} 3 & 2 \\\\ 2 & 5 \\end{bmatrix}\\) (symmetric)\n\n\nDerivative: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{S}\\mathbf{x}) = 2\\mathbf{S}\\mathbf{x} = 2\\begin{bmatrix} 3 & 2 \\\\ 2 & 5 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 6x_1 + 4x_2 \\\\ 4x_1 + 10x_2 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/02-slides.html#general-form-mathbfbmathbfamathbfx",
    "href": "slides/02-slides.html#general-form-mathbfbmathbfamathbfx",
    "title": "Matrix Algebra Fundamentals",
    "section": "General Form: \\(\\mathbf{b}'\\mathbf{A}\\mathbf{x}\\)",
    "text": "General Form: \\(\\mathbf{b}'\\mathbf{A}\\mathbf{x}\\)\n\nRule: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{b}'\\mathbf{A}\\mathbf{x}) = \\mathbf{A}'\\mathbf{b}\\)\n\n\nNote: \\(\\mathbf{b}'\\mathbf{A}\\mathbf{x}\\) is a scalar, and \\(\\mathbf{b}\\) is treated as a constant vector"
  },
  {
    "objectID": "slides/02-slides.html#you-try-matrix-derivatives",
    "href": "slides/02-slides.html#you-try-matrix-derivatives",
    "title": "Matrix Algebra Fundamentals",
    "section": "You Try: Matrix Derivatives",
    "text": "You Try: Matrix Derivatives\n\nProblem: Find \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{B}\\mathbf{x})\\) where \\(\\mathbf{B} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 3 \\end{bmatrix}\\)\n\n\nSolution: \\[\\mathbf{B}' = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}, \\quad \\mathbf{B} + \\mathbf{B}' = \\begin{bmatrix} 2 & 2 \\\\ 2 & 6 \\end{bmatrix}\\] \\[\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{B}\\mathbf{x}) = \\begin{bmatrix} 2 & 2 \\\\ 2 & 6 \\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} 2x_1 + 2x_2 \\\\ 2x_1 + 6x_2 \\end{bmatrix}\\]\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/02-slides.html#summary-key-rules",
    "href": "slides/02-slides.html#summary-key-rules",
    "title": "Matrix Algebra Fundamentals",
    "section": "Summary: Key Rules",
    "text": "Summary: Key Rules\n\n\nLinear forms: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{a}'\\mathbf{x}) = \\mathbf{a}\\)\n\n\n\n\nQuadratic forms: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}')\\mathbf{x}\\)\n\n\n\n\nSymmetric case: \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}'\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\) when \\(\\mathbf{A} = \\mathbf{A}'\\)"
  },
  {
    "objectID": "slides/02-slides.html#connection-to-hat-matrix",
    "href": "slides/02-slides.html#connection-to-hat-matrix",
    "title": "Matrix Algebra Fundamentals",
    "section": "Connection to Hat Matrix",
    "text": "Connection to Hat Matrix\n\nThese rules are essential for deriving:\n\nNormal equations: \\(\\mathbf{X}'\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}'\\mathbf{y}\\)\nHat matrix: \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\)\nLeast squares solution: \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\)\n\n\n\nWe’ll use these derivatives to minimize the sum of squared errors!"
  },
  {
    "objectID": "slides/04-slides.html#the-sum-of-squared-errors",
    "href": "slides/04-slides.html#the-sum-of-squared-errors",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "The Sum of Squared Errors",
    "text": "The Sum of Squared Errors\n\nRecall our objective: \\[\\text{SSE} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]\n\n\nGoal: Find \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes SSE\n\n\nMethod: Take the derivative with respect to \\(\\boldsymbol{\\beta}\\) and set equal to zero"
  },
  {
    "objectID": "slides/04-slides.html#expanding-the-sse-expression",
    "href": "slides/04-slides.html#expanding-the-sse-expression",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Expanding the SSE Expression",
    "text": "Expanding the SSE Expression\n\nStart with: \\[\\text{SSE} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]\n\n\nExpand: \\[\\text{SSE} = \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "slides/04-slides.html#expanding-the-sse-expression-1",
    "href": "slides/04-slides.html#expanding-the-sse-expression-1",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Expanding the SSE Expression",
    "text": "Expanding the SSE Expression\n\nSince \\(\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} = \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}\\) (both are scalars): \\[\\text{SSE} = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "slides/04-slides.html#taking-the-derivative",
    "href": "slides/04-slides.html#taking-the-derivative",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Taking the Derivative",
    "text": "Taking the Derivative\n\nMatrix calculus rules we need:\n\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\mathbf{a}^T\\boldsymbol{\\beta}) = \\mathbf{a}\\)\n\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\boldsymbol{\\beta}^T\\mathbf{A}\\boldsymbol{\\beta}) = 2\\mathbf{A}\\boldsymbol{\\beta}\\) (when \\(\\mathbf{A}\\) is symmetric)\n\n\n\nTaking the derivative: \\[\\frac{\\partial \\text{SSE}}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "slides/04-slides.html#setting-equal-to-zero",
    "href": "slides/04-slides.html#setting-equal-to-zero",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Setting Equal to Zero",
    "text": "Setting Equal to Zero\n\nSet the derivative equal to zero: \\[-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\\]\n\n\nDivide by 2: \\[-\\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}\\]"
  },
  {
    "objectID": "slides/04-slides.html#setting-equal-to-zero-1",
    "href": "slides/04-slides.html#setting-equal-to-zero-1",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Setting Equal to Zero",
    "text": "Setting Equal to Zero\nRearrange: \\[\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/04-slides.html#the-least-squares-solution",
    "href": "slides/04-slides.html#the-least-squares-solution",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "The Least Squares Solution",
    "text": "The Least Squares Solution\n\nSolve for \\(\\hat{\\boldsymbol{\\beta}}\\): \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/04-slides.html#verification-in-r",
    "href": "slides/04-slides.html#verification-in-r",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": " Verification in R",
    "text": "Verification in R\n\nset.seed(1)\nx &lt;- 1:10\ny &lt;- 2 + 3 * x + rnorm(10, 0, 2)\n\nX &lt;- cbind(1, x)\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\n\nmodel &lt;- lm(y ~ x)\ncbind(beta_hat, coef(model))\n\n      [,1]     [,2]\n  1.662353 1.662353\nx 3.109464 3.109464"
  },
  {
    "objectID": "slides/04-slides.html#what-is-qr-decomposition",
    "href": "slides/04-slides.html#what-is-qr-decomposition",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "What is QR Decomposition?",
    "text": "What is QR Decomposition?\n\nAny matrix \\(\\mathbf{X}\\) can be decomposed as: \\[\\mathbf{X} = \\mathbf{Q}\\mathbf{R}\\]\n\n\nWhere:\n\n\\(\\mathbf{Q}\\) is orthogonal: \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\)\n\n\\(\\mathbf{R}\\) is upper triangular"
  },
  {
    "objectID": "slides/04-slides.html#what-is-qr-decomposition-1",
    "href": "slides/04-slides.html#what-is-qr-decomposition-1",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "What is QR Decomposition?",
    "text": "What is QR Decomposition?\nAny matrix \\(\\mathbf{X}\\) can be decomposed as: \\[\\mathbf{X} = \\mathbf{Q}\\mathbf{R}\\]\nIn words: Break down \\(\\mathbf{X}\\) into perpendicular directions (\\(\\mathbf{Q}\\)) and scaling/rotation (\\(\\mathbf{R}\\))"
  },
  {
    "objectID": "slides/04-slides.html#why-use-qr-for-regression",
    "href": "slides/04-slides.html#why-use-qr-for-regression",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Why Use QR for Regression?",
    "text": "Why Use QR for Regression?\n\nTraditional approach requires: \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/04-slides.html#why-use-qr-for-regression-1",
    "href": "slides/04-slides.html#why-use-qr-for-regression-1",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Why Use QR for Regression?",
    "text": "Why Use QR for Regression?\nProblems with \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\):\n\nComputing \\(\\mathbf{X}^T\\mathbf{X}\\) can be numerically unstable\n\nMatrix inversion is computationally expensive\n\nCondition number gets squared: \\(\\kappa(\\mathbf{X}^T\\mathbf{X}) = \\kappa(\\mathbf{X})^2\\)"
  },
  {
    "objectID": "slides/04-slides.html#demo",
    "href": "slides/04-slides.html#demo",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": " Demo",
    "text": "Demo\nWhat do I mean by “unstable”\n\nx &lt;- seq(1, 500, len = 30)\nX &lt;- cbind(1, x, x^2, x^3)\nbeta &lt;- matrix(c(1, 1, 1, 1), 4, 1)\n\nset.seed(1)\ny &lt;- X%*%beta + rnorm(30)\n\n\nsolve(crossprod(X))\n\n\n\nError in solve.default(crossprod(X)) : system is computationally singular: reciprocal condition number = 3.11914e-17\n\n\n\nlog10(crossprod(X))\n\n                   x                    \n  1.477121  3.875929  6.406189  8.987506\nx 3.875929  6.406189  8.987506 11.596810\n  6.406189  8.987506 11.596810 14.223800\n  8.987506 11.596810 14.223800 16.862984"
  },
  {
    "objectID": "slides/04-slides.html#qr-solution",
    "href": "slides/04-slides.html#qr-solution",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "QR Solution",
    "text": "QR Solution\n\nIf \\(\\mathbf{X} = \\mathbf{Q}\\mathbf{R}\\), then: \\[\\mathbf{X}^T\\mathbf{X} = (\\mathbf{Q}\\mathbf{R})^T(\\mathbf{Q}\\mathbf{R}) = \\mathbf{R}^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{R} = \\mathbf{R}^T\\mathbf{R}\\]"
  },
  {
    "objectID": "slides/04-slides.html#qr-solution-1",
    "href": "slides/04-slides.html#qr-solution-1",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "QR Solution",
    "text": "QR Solution\nSo: \\[\n\\begin{align}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{R}^T\\mathbf{R})^{-1}\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{y}\n\\end{align}\n\\] ::: fragment Note that \\((\\mathbf{AB})^{-1}=\\mathbf{B}^{-1}\\mathbf{A}^{-1}\\) as long as both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are invertible (so if \\(\\mathbf{R}\\) is invertible then \\((\\mathbf{R}^T\\mathbf{R})^{-1} = \\mathbf{R}^{-1}(\\mathbf{R}^T)^{-1}\\)) :::"
  },
  {
    "objectID": "slides/04-slides.html#qr-solution-2",
    "href": "slides/04-slides.html#qr-solution-2",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "QR Solution",
    "text": "QR Solution\nSo: \\[\n\\begin{align}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{R}^T\\mathbf{R})^{-1}\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{y} \\\\&= \\mathbf{R}^{-1}(\\mathbf{R}^T)^{-1}\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{y}\\\\& = \\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/04-slides.html#qr-advantages",
    "href": "slides/04-slides.html#qr-advantages",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "QR Advantages",
    "text": "QR Advantages\n\nOnly need to solve \\(\\mathbf{R}\\hat{\\boldsymbol{\\beta}} = \\mathbf{Q}^T\\mathbf{y}\\) (back substitution)\n\nMore numerically stable\n\nAvoids explicit matrix inversion"
  },
  {
    "objectID": "slides/04-slides.html#backsolve-example",
    "href": "slides/04-slides.html#backsolve-example",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Backsolve Example",
    "text": "Backsolve Example\nGiven: \\[\\mathbf{R} = \\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 2 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \\mathbf{Q}^T\\mathbf{y} = \\begin{bmatrix} 8 \\\\ 12 \\\\ 15 \\end{bmatrix}\\]\nSolve: \\(\\mathbf{R}\\hat{\\boldsymbol{\\beta}} = \\mathbf{Q}^T\\mathbf{y}\\)"
  },
  {
    "objectID": "slides/04-slides.html#step-by-step-backsolve",
    "href": "slides/04-slides.html#step-by-step-backsolve",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Step-by-Step Backsolve",
    "text": "Step-by-Step Backsolve\n\n\\[\\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 2 \\\\ 0 & 0 & 5 \\end{bmatrix} \\begin{bmatrix} \\hat\\beta_1 \\\\ \\hat\\beta_2 \\\\ \\hat\\beta_3 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 12 \\\\ 15 \\end{bmatrix}\\]\n\n\nStep 1: Solve bottom row first \\[5\\hat\\beta_3 = 15 \\implies \\hat\\beta_3 = 3\\]"
  },
  {
    "objectID": "slides/04-slides.html#step-by-step-backsolve-1",
    "href": "slides/04-slides.html#step-by-step-backsolve-1",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Step-by-Step Backsolve",
    "text": "Step-by-Step Backsolve\n\n\\[\\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 2 \\\\ 0 & 0 & 5 \\end{bmatrix} \\begin{bmatrix} \\hat\\beta_1 \\\\ \\hat\\beta_2 \\\\ \\hat\\beta_3 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 12 \\\\ 15 \\end{bmatrix}\\]\n\nStep 2: Substitute into second row \\[4\\beta_2 + 2(3) = 12 \\implies 4\\beta_2 = 6 \\implies \\beta_2 = 1.5\\]"
  },
  {
    "objectID": "slides/04-slides.html#step-by-step-backsolve-2",
    "href": "slides/04-slides.html#step-by-step-backsolve-2",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Step-by-Step Backsolve",
    "text": "Step-by-Step Backsolve\n\n\\[\\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 2 \\\\ 0 & 0 & 5 \\end{bmatrix} \\begin{bmatrix} \\hat\\beta_1 \\\\ \\hat\\beta_2 \\\\ \\hat\\beta_3 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 12 \\\\ 15 \\end{bmatrix}\\]\n\nStep 3: Substitute into first row \\[2\\beta_1 + 1(1.5) + 3(3) = 8 \\implies 2\\beta_1 = -2.5 \\implies \\beta_1 = -1.25\\]"
  },
  {
    "objectID": "slides/04-slides.html#backsolve-solution",
    "href": "slides/04-slides.html#backsolve-solution",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "Backsolve Solution",
    "text": "Backsolve Solution\n\\[\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} -1.25 \\\\ 1.5 \\\\ 3 \\end{bmatrix}\\]\nKey insight: Work backwards through the triangular matrix, using previously solved values to eliminate unknowns in each step."
  },
  {
    "objectID": "slides/04-slides.html#you-try-qr-properties",
    "href": "slides/04-slides.html#you-try-qr-properties",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "You Try: QR Properties",
    "text": "You Try: QR Properties\n\nGiven \\(\\mathbf{X} = \\mathbf{Q}\\mathbf{R}\\), verify that:\n\n\\(\\mathbf{X}^T\\mathbf{X} = \\mathbf{R}^T\\mathbf{R}\\)\nThe hat matrix becomes \\(\\mathbf{H} = \\mathbf{Q}\\mathbf{Q}^T\\)\n\nHint: Use the fact that \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\) and remember \\((\\mathbf{AB})^{-1}=\\mathbf{B}^{-1}\\mathbf{A}^{-1}\\) as long as both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are invertible.\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/04-slides.html#qr-in-r",
    "href": "slides/04-slides.html#qr-in-r",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": " QR in R",
    "text": "QR in R\n\n# Using our previous data\n\n# QR decomposition\nqr_decomp &lt;- qr(X)\nQ &lt;- qr.Q(qr_decomp)\nR &lt;- qr.R(qr_decomp)\n\n# Solve using QR\nbeta_qr &lt;- backsolve(R, t(Q) %*% y)\nbeta_qr"
  },
  {
    "objectID": "slides/04-slides.html#why-qr-matters",
    "href": "slides/04-slides.html#why-qr-matters",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": " Why QR Matters",
    "text": "Why QR Matters\nCondition number comparison:\n\n# Compare condition numbers\nkappa(X)               # Condition of X\n\n[1] 158915720\n\nkappa(t(X) %*% X)      # Condition of X'X (much worse!)\n\n[1] 3.165203e+16"
  },
  {
    "objectID": "slides/04-slides.html#you-try-qr-properties-1",
    "href": "slides/04-slides.html#you-try-qr-properties-1",
    "title": "Deriving the Hat Matrix and QR Decomposition",
    "section": "You Try: QR Properties",
    "text": "You Try: QR Properties\nCalculate \\(\\hat{y}\\) using the QR elements.\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/06-slides.html#what-are-we-measuring",
    "href": "slides/06-slides.html#what-are-we-measuring",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What Are We Measuring?",
    "text": "What Are We Measuring?\n\nThe Big Picture: How well does our model explain the data?\n\n\nThree key quantities:\n- Total variation in the data\n- Variation explained by our model\n- Variation left unexplained (residuals)"
  },
  {
    "objectID": "slides/06-slides.html#the-linear-model-reminder",
    "href": "slides/06-slides.html#the-linear-model-reminder",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "The Linear Model Reminder",
    "text": "The Linear Model Reminder\n\nOur model: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\)\n\n\nFitted values: \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{H}\\mathbf{y}\\)\nwhere \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\) is the “hat matrix”\n\n\nResiduals: \\(\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{y} - \\hat{\\mathbf{y}}\\)"
  },
  {
    "objectID": "slides/06-slides.html#what-is-tss",
    "href": "slides/06-slides.html#what-is-tss",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is TSS?",
    "text": "What is TSS?\n\nDefinition: Total variation in the response variable \\(\\mathbf{y}\\)\n\n\nFormula: \\[\\text{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/06-slides.html#what-is-tss-1",
    "href": "slides/06-slides.html#what-is-tss-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is TSS?",
    "text": "What is TSS?\nDefinition: Total variation in the response variable \\(\\mathbf{y}\\)\nMatrix form: \\[\\text{TSS} = (\\mathbf{y} - \\bar{y}\\mathbf{1})^T(\\mathbf{y} - \\bar{y}\\mathbf{1})\\]\nwhere \\(\\mathbf{1}\\) is a vector of ones and \\(\\bar{y}\\) is the sample mean"
  },
  {
    "objectID": "slides/06-slides.html#tss-intuition",
    "href": "slides/06-slides.html#tss-intuition",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "TSS Intuition",
    "text": "TSS Intuition\n\nThink of TSS as: “How spread out are my y-values?”\n\n\nKey insight: This is what we’re trying to explain with our model\n\n\nAlternative matrix form: \\[\\text{TSS} = \\mathbf{y}^T\\mathbf{y} - n\\bar{y}^2\\]"
  },
  {
    "objectID": "slides/06-slides.html#you-try-calculate-tss",
    "href": "slides/06-slides.html#you-try-calculate-tss",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Calculate TSS",
    "text": "You Try: Calculate TSS\n\nGiven: \\(\\mathbf{y} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{bmatrix}\\)\n\n\nFind: TSS using both the definition and matrix form\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/06-slides.html#you-try-setup",
    "href": "slides/06-slides.html#you-try-setup",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Setup",
    "text": "You Try: Setup\n\nStep 1: Calculate \\(\\bar{y} = \\frac{2+4+6+8}{4} = 5\\)\n\n\nStep 2: Deviations from mean: \\(\\begin{bmatrix} -3 \\\\ -1 \\\\ 1 \\\\ 3 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/06-slides.html#you-try-solution",
    "href": "slides/06-slides.html#you-try-solution",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Solution",
    "text": "You Try: Solution\n\nMethod 1 (definition): \\[\\text{TSS} = (-3)^2 + (-1)^2 + 1^2 + 3^2 = 9 + 1 + 1 + 9 = 20\\]\n\n\nMethod 2 (matrix form): \\[\\text{TSS} = 2^2 + 4^2 + 6^2 + 8^2 - 4(5^2) = 120 - 100 = 20\\]"
  },
  {
    "objectID": "slides/06-slides.html#what-is-sse",
    "href": "slides/06-slides.html#what-is-sse",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is SSE?",
    "text": "What is SSE?\n\nDefinition: Variation left unexplained by our model\n\n\nFormula: \\[\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\hat{\\varepsilon}_i^2\\]"
  },
  {
    "objectID": "slides/06-slides.html#what-is-sse-1",
    "href": "slides/06-slides.html#what-is-sse-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is SSE?",
    "text": "What is SSE?\nDefinition: Variation left unexplained by our model\nMatrix form: \\[\\text{SSE} = \\hat{\\boldsymbol{\\varepsilon}}^T\\hat{\\boldsymbol{\\varepsilon}} = (\\mathbf{y} - \\hat{\\mathbf{y}})^T(\\mathbf{y} - \\hat{\\mathbf{y}})\\]"
  },
  {
    "objectID": "slides/06-slides.html#sse-with-hat-matrix",
    "href": "slides/06-slides.html#sse-with-hat-matrix",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "SSE with Hat Matrix",
    "text": "SSE with Hat Matrix\n\nSince \\(\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\\): \\[\\text{SSE} = (\\mathbf{y} - \\mathbf{H}\\mathbf{y})^T(\\mathbf{y} - \\mathbf{H}\\mathbf{y})\\]\n\n\nFactor out \\(\\mathbf{y}\\): \\[= \\mathbf{y}^T(\\mathbf{I} - \\mathbf{H})^T(\\mathbf{I} - \\mathbf{H})\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/06-slides.html#sse-with-hat-matrix-1",
    "href": "slides/06-slides.html#sse-with-hat-matrix-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "SSE with Hat Matrix",
    "text": "SSE with Hat Matrix\nKey property: \\(\\mathbf{I} - \\mathbf{H}\\) is symmetric and idempotent\n\\[\\text{SSE} = \\mathbf{y}^T(\\mathbf{I} - \\mathbf{H})\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/06-slides.html#sse-intuition",
    "href": "slides/06-slides.html#sse-intuition",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "SSE Intuition",
    "text": "SSE Intuition\n\nThink of SSE as: “How much did we miss with our model?”\n\n\nSmaller SSE = Better fit\nLarger SSE = Worse fit\n\n\nPerfect fit: SSE = 0 (model explains everything)"
  },
  {
    "objectID": "slides/06-slides.html#what-is-regression-sum-of-squares",
    "href": "slides/06-slides.html#what-is-regression-sum-of-squares",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is Regression Sum of Squares?",
    "text": "What is Regression Sum of Squares?\n\nDefinition: Variation explained by our model\n\n\nFormula: \\[\\text{SS}_{\\text{Reg}} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/06-slides.html#what-is-regression-sum-of-squares-1",
    "href": "slides/06-slides.html#what-is-regression-sum-of-squares-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is Regression Sum of Squares?",
    "text": "What is Regression Sum of Squares?\n\nDefinition: Variation explained by our model\n\nMatrix form: \\[\\text{SS}_{\\text{Reg}} = (\\hat{\\mathbf{y}} - \\bar{y}\\mathbf{1})^T(\\hat{\\mathbf{y}} - \\bar{y}\\mathbf{1})\\]"
  },
  {
    "objectID": "slides/06-slides.html#regression-ss-with-hat-matrix",
    "href": "slides/06-slides.html#regression-ss-with-hat-matrix",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Regression SS with Hat Matrix",
    "text": "Regression SS with Hat Matrix\n\nSince \\(\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}\\): \\[\\text{SS}_{\\text{Reg}} = (\\mathbf{H}\\mathbf{y} - \\bar{y}\\mathbf{1})^T(\\mathbf{H}\\mathbf{y} - \\bar{y}\\mathbf{1})\\]\n\n\nAlternative form: \\[\\text{SS}_{\\text{Reg}} = \\mathbf{y}^T\\mathbf{H}\\mathbf{y} - n\\bar{y}^2\\]"
  },
  {
    "objectID": "slides/06-slides.html#regression-sum-of-squares-intuition",
    "href": "slides/06-slides.html#regression-sum-of-squares-intuition",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Regression Sum of Squares Intuition",
    "text": "Regression Sum of Squares Intuition\n\nThink of \\(\\textrm{SS}_\\textrm{Reg}\\) as: “How much variation did our model capture?”\n\n\nLarger \\(\\textrm{SS}_\\textrm{Reg}\\) = Model explains more\nSmaller \\(\\textrm{SS}_\\textrm{Reg}\\) = Model explains less\n\n\nNo model: \\(\\textrm{SS}_\\textrm{Reg}\\) = 0 (just predicting the mean)"
  },
  {
    "objectID": "slides/06-slides.html#it-all-adds-up",
    "href": "slides/06-slides.html#it-all-adds-up",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "It all adds up!",
    "text": "It all adds up!\n\nThe key relationship: \\[\\text{Total Variation} = \\text{Unexplained} + \\text{Explained}\\]"
  },
  {
    "objectID": "slides/06-slides.html#you-try-prove-the-identity",
    "href": "slides/06-slides.html#you-try-prove-the-identity",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Prove the Identity",
    "text": "You Try: Prove the Identity\n\nChallenge: Show that TSS = SSE + \\(\\textrm{SS}_\\textrm{Reg}\\) using matrix algebra\n\n\nHint: Start with the definitions and use properties of the hat matrix\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/06-slides.html#proof-strategy",
    "href": "slides/06-slides.html#proof-strategy",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Proof Strategy",
    "text": "Proof Strategy\n\nStart with: TSS = \\(\\mathbf{y}^T\\mathbf{y} - n\\bar{y}^2\\)\n\n\nKey insight: We need to decompose \\(\\mathbf{y}\\) as \\(\\mathbf{y} = \\hat{\\mathbf{y}} + \\hat{\\boldsymbol{\\varepsilon}}\\)\n\n\nThen show: Cross terms vanish due to orthogonality"
  },
  {
    "objectID": "slides/06-slides.html#proof-step-by-step",
    "href": "slides/06-slides.html#proof-step-by-step",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Proof: Step by Step",
    "text": "Proof: Step by Step\n\nWrite: \\(\\mathbf{y} = \\hat{\\mathbf{y}} + \\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{H}\\mathbf{y} + (\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\)\n\n\nThen: \\[\\mathbf{y}^T\\mathbf{y} = (\\mathbf{H}\\mathbf{y} + (\\mathbf{I}-\\mathbf{H})\\mathbf{y})^T(\\mathbf{H}\\mathbf{y} + (\\mathbf{I}-\\mathbf{H})\\mathbf{y})\\]"
  },
  {
    "objectID": "slides/06-slides.html#expand-the-product",
    "href": "slides/06-slides.html#expand-the-product",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Expand the Product",
    "text": "Expand the Product\n\nFour terms: \\[= \\mathbf{y}^T\\mathbf{H}^T\\mathbf{H}\\mathbf{y} + \\mathbf{y}^T\\mathbf{H}^T(\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\] \\[+ \\mathbf{y}^T(\\mathbf{I}-\\mathbf{H})^T\\mathbf{H}\\mathbf{y} + \\mathbf{y}^T(\\mathbf{I}-\\mathbf{H})^T(\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/06-slides.html#cross-terms-vanish",
    "href": "slides/06-slides.html#cross-terms-vanish",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Cross Terms Vanish",
    "text": "Cross Terms Vanish\n\nKey property: \\(\\mathbf{H}(\\mathbf{I}-\\mathbf{H}) = \\mathbf{0}\\) (orthogonal projections)\n\n\nTherefore: The cross terms equal zero\n\n\nResult: \\[\\mathbf{y}^T\\mathbf{y} = \\mathbf{y}^T\\mathbf{H}\\mathbf{y} + \\mathbf{y}^T(\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\]"
  },
  {
    "objectID": "slides/06-slides.html#final-identity",
    "href": "slides/06-slides.html#final-identity",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Final Identity",
    "text": "Final Identity\n\nSubtract \\(n\\bar{y}^2\\) from both sides: \\[\\text{TSS} = \\text{SS}_{\\text{Reg}} + \\text{SSE}\\]\n\n\nBeautiful result: Total variation splits perfectly into explained and unexplained parts"
  },
  {
    "objectID": "slides/06-slides.html#what-is-r²",
    "href": "slides/06-slides.html#what-is-r²",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is R²?",
    "text": "What is R²?\n\nDefinition: Proportion of total variation explained by the model\n\n\nFormula: \\[R^2 = \\frac{\\text{SS}_{\\text{Reg}}}{\\text{TSS}} = 1 - \\frac{\\text{SSE}}{\\text{TSS}}\\]\n\n\nRange: \\(0 \\leq R^2 \\leq 1\\)"
  },
  {
    "objectID": "slides/06-slides.html#r²-interpretation",
    "href": "slides/06-slides.html#r²-interpretation",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "R² Interpretation",
    "text": "R² Interpretation\n\nR² = 0.8 means “80% of variation is explained by the model”\n\n\nPerfect fit: R² = 1 (model explains everything)\nNo relationship: R² = 0 (model explains nothing)\n\n\nWarning: High R² doesn’t always mean good model!"
  },
  {
    "objectID": "slides/06-slides.html#you-try-calculate-r²",
    "href": "slides/06-slides.html#you-try-calculate-r²",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Calculate R²",
    "text": "You Try: Calculate R²\n\nGiven: TSS = 100, SSE = 25\n\n\nFind: R² and interpret the result\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/06-slides.html#you-try-solution-1",
    "href": "slides/06-slides.html#you-try-solution-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Solution",
    "text": "You Try: Solution\n\nMethod 1: \\[R^2 = 1 - \\frac{\\text{SSE}}{\\text{TSS}} = 1 - \\frac{25}{100} = 0.75\\]\n\n\nMethod 2: \\[\\text{SS}_{\\text{Reg}} = \\text{TSS} - \\text{SSE} = 100 - 25 = 75\\] \\[R^2 = \\frac{75}{100} = 0.75\\]\n\n\nInterpretation: The model explains 75% of the variation in y"
  },
  {
    "objectID": "slides/06-slides.html#when-r²-misleads",
    "href": "slides/06-slides.html#when-r²-misleads",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "When R² Misleads",
    "text": "When R² Misleads\n\n# Load the famous Anscombe's Quartet\ndata(anscombe)\n\n# All have same R²!\nlm1 &lt;- lm(y1 ~ x1, data = anscombe)\nlm2 &lt;- lm(y2 ~ x2, data = anscombe) \nlm3 &lt;- lm(y3 ~ x3, data = anscombe)\nlm4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nc(summary(lm1)$r.squared, summary(lm2)$r.squared, \n  summary(lm3)$r.squared, summary(lm4)$r.squared)\n\n[1] 0.6665425 0.6662420 0.6663240 0.6667073"
  },
  {
    "objectID": "slides/06-slides.html#visual-reality-check",
    "href": "slides/06-slides.html#visual-reality-check",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Visual Reality Check",
    "text": "Visual Reality Check"
  },
  {
    "objectID": "slides/06-slides.html#the-big-questions",
    "href": "slides/06-slides.html#the-big-questions",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "The Big Questions",
    "text": "The Big Questions\n\nSingle predictor: Is \\(\\beta_1\\) significantly different from 0?\n\n\nMultiple predictors: Are any of the predictors useful?\n\n\nSubset test: Is a group of predictors jointly significant?"
  },
  {
    "objectID": "slides/06-slides.html#testing-single-coefficients",
    "href": "slides/06-slides.html#testing-single-coefficients",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Testing Single Coefficients",
    "text": "Testing Single Coefficients\n\nNull hypothesis: \\(H_0: \\beta_j = 0\\)\nAlternative: \\(H_1: \\beta_j \\neq 0\\)\n\n\n\n\nTest statistic: \\[t = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)}\\]\n\n\n\nStandard error: \\(\\text{se}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}\\)"
  },
  {
    "objectID": "slides/06-slides.html#where-does-the-standard-error-come-from",
    "href": "slides/06-slides.html#where-does-the-standard-error-come-from",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Where Does the Standard Error Come From?",
    "text": "Where Does the Standard Error Come From?\n\nRecall: \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\)\n\n\nFor individual coefficient j: \\[\\text{Var}(\\hat{\\beta}_j) = \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\]\n\n\nEstimate \\(\\sigma^2\\): \\(\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}\\) where p = number of parameters"
  },
  {
    "objectID": "slides/06-slides.html#t-distribution-result",
    "href": "slides/06-slides.html#t-distribution-result",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "t-Distribution Result",
    "text": "t-Distribution Result\n\nUnder normality assumption: \\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\text{se}(\\hat{\\beta}_j)} \\sim t_{n-p}\\]"
  },
  {
    "objectID": "slides/06-slides.html#t-distribution-result-1",
    "href": "slides/06-slides.html#t-distribution-result-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "t-Distribution Result",
    "text": "t-Distribution Result\n\nFor testing \\(H_0: \\beta_j = 0\\): \\[t = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)} \\sim t_{n-p}\\]"
  },
  {
    "objectID": "slides/06-slides.html#testing-multiple-predictors",
    "href": "slides/06-slides.html#testing-multiple-predictors",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Testing Multiple Predictors",
    "text": "Testing Multiple Predictors\n\nNull hypothesis: \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} = 0\\)\n\n\nAlternative: At least one \\(\\beta_j \\neq 0\\) (j ≠ 0)\n\n\nThis tests: “Is the model useful at all?”"
  },
  {
    "objectID": "slides/06-slides.html#f-test-statistic",
    "href": "slides/06-slides.html#f-test-statistic",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "F-Test Statistic",
    "text": "F-Test Statistic\n\nTest statistic: \\[F = \\frac{\\text{SS}_{\\text{Reg}}/(p-1)}{\\text{SSE}/(n-p)} = \\frac{\\text{Mean Square Regression}}{\\text{Mean Square Error}}\\]\n\n\nUnder \\(H_0\\): \\(F \\sim F_{p-1, n-p}\\)"
  },
  {
    "objectID": "slides/06-slides.html#f-test-intuition",
    "href": "slides/06-slides.html#f-test-intuition",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "F-Test Intuition",
    "text": "F-Test Intuition\n\nNumerator: How much variation does model explain per parameter?\n\n\nDenominator: How much unexplained variation per residual degree of freedom?\n\n\nLarge F: Model explains a lot relative to noise\nSmall F: Model doesn’t explain much more than noise"
  },
  {
    "objectID": "slides/06-slides.html#connection-to-r²",
    "href": "slides/06-slides.html#connection-to-r²",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Connection to R²",
    "text": "Connection to R²\n\nAlternative F-statistic form: \\[F = \\frac{R^2/(p-1)}{(1-R^2)/(n-p)}\\]\n\n\nThis shows: F-test is really testing whether R² is significantly different from 0"
  },
  {
    "objectID": "slides/06-slides.html#the-framework",
    "href": "slides/06-slides.html#the-framework",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "The Framework",
    "text": "The Framework\n\nGeneral form: \\(H_0: \\mathbf{C}\\boldsymbol{\\beta} = \\mathbf{d}\\)\n\n\nWhere:\n- \\(\\mathbf{C}\\) is a contrast matrix (q × p)\n- \\(\\mathbf{d}\\) is a vector of constants\n- q is the number of restrictions"
  },
  {
    "objectID": "slides/06-slides.html#examples-of-linear-hypotheses",
    "href": "slides/06-slides.html#examples-of-linear-hypotheses",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Examples of Linear Hypotheses",
    "text": "Examples of Linear Hypotheses\n\nSingle coefficient: \\(\\mathbf{C} = [0, 1, 0, 0]\\), \\(\\mathbf{d} = 0\\)\nTests: \\(\\beta_1 = 0\\)\n\n\nOverall test: \\(\\mathbf{C} = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\), \\(\\mathbf{d} = \\mathbf{0}\\)\nTests: All slope coefficients = 0\n\n\nEquality test: \\(\\mathbf{C} = [0, 1, -1, 0]\\), \\(\\mathbf{d} = 0\\)\nTests: \\(\\beta_1 = \\beta_2\\)"
  },
  {
    "objectID": "slides/06-slides.html#the-general-f-test",
    "href": "slides/06-slides.html#the-general-f-test",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "The General F-Test",
    "text": "The General F-Test\n\nTest statistic: \\[F = \\frac{(\\mathbf{C}\\hat{\\boldsymbol{\\beta}} - \\mathbf{d})^T[\\mathbf{C}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{C}^T]^{-1}(\\mathbf{C}\\hat{\\boldsymbol{\\beta}} - \\mathbf{d})/q}{\\text{SSE}/(n-p)}\\]\n\n\nUnder \\(H_0\\): \\(F \\sim F_{q, n-p}\\)"
  },
  {
    "objectID": "slides/06-slides.html#you-try-set-up-hypothesis-test",
    "href": "slides/06-slides.html#you-try-set-up-hypothesis-test",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Set Up Hypothesis Test",
    "text": "You Try: Set Up Hypothesis Test\n\nScenario: Three predictors, want to test if the last two coefficients are both zero\n\n\nModel: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\varepsilon\\)\n\n\nQuestion: What are \\(\\mathbf{C}\\) and \\(\\mathbf{d}\\) for \\(H_0: \\beta_2 = \\beta_3 = 0\\)?\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/06-slides.html#you-try-solution-2",
    "href": "slides/06-slides.html#you-try-solution-2",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Solution",
    "text": "You Try: Solution\n\nAnswer: \\[\\mathbf{C} = \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}, \\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\]\n\n\nCheck: \\(\\mathbf{C}\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_2 \\\\ \\beta_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) ✓"
  },
  {
    "objectID": "slides/06-slides.html#the-anova-table",
    "href": "slides/06-slides.html#the-anova-table",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "The ANOVA Table",
    "text": "The ANOVA Table\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSum of Squares\nMean Square\nF\n\n\n\n\nRegression\np-1\n\\(\\textrm{SS}_\\textrm{Reg}\\)\n\\(\\textrm{MS}_\\textrm{Reg}\\)\n\\(\\textrm{MS}_\\textrm{Reg}\\)/MSE\n\n\nError\nn-p\nSSE\nMSE\n\n\n\nTotal\nn-1\nTSS"
  },
  {
    "objectID": "slides/06-slides.html#matrix-summary",
    "href": "slides/06-slides.html#matrix-summary",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Matrix Summary",
    "text": "Matrix Summary\n\nKey relationships:\n- TSS = \\(\\mathbf{y}^T\\mathbf{y} - n\\bar{y}^2\\)\n- \\(\\textrm{SS}_\\textrm{Reg}\\) = \\(\\mathbf{y}^T\\mathbf{H}\\mathbf{y} - n\\bar{y}^2\\)\n- SSE = \\(\\mathbf{y}^T(\\mathbf{I} - \\mathbf{H})\\mathbf{y}\\)\n- TSS = \\(\\textrm{SS}_\\textrm{Reg}\\) + SSE\n\n\nR² = \\(\\textrm{SS}_\\textrm{Reg}\\)/TSS = 1 - SSE/TSS"
  },
  {
    "objectID": "slides/06-slides.html#testing-hierarchy",
    "href": "slides/06-slides.html#testing-hierarchy",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Testing Hierarchy",
    "text": "Testing Hierarchy\n\nStep 1: Overall F-test (is model useful?)\n\n\nStep 2: Subset F-tests (are groups of predictors significant?)\n\n\nStep 3: Individual t-tests (which predictors matter?)"
  },
  {
    "objectID": "slides/06-slides.html#what-is-a-p-value",
    "href": "slides/06-slides.html#what-is-a-p-value",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "What is a P-value?",
    "text": "What is a P-value?\n\nDefinition: The probability of observing a test statistic as extreme or more extreme than what we observed, assuming the null hypothesis is true\n\n\nIn other words: “How surprising is our result if \\(H_0\\) were true?”"
  },
  {
    "objectID": "slides/06-slides.html#p-values-for-t-tests",
    "href": "slides/06-slides.html#p-values-for-t-tests",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "P-values for t-tests",
    "text": "P-values for t-tests\n\nFor testing \\(H_0: \\beta_j = 0\\):\n\n\nTest statistic: \\(t = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)}\\)\n\n\nTwo-sided p-value:\n\\[\\text{p-value} = P(|T| \\geq |t|) = 2 \\times P(T \\geq |t|)\\] where \\(T \\sim t_{n-p}\\)"
  },
  {
    "objectID": "slides/06-slides.html#p-values-for-f-tests",
    "href": "slides/06-slides.html#p-values-for-f-tests",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "P-values for F-tests",
    "text": "P-values for F-tests\n\nFor overall test \\(H_0:\\) all slopes = 0:\n\n\nTest statistic: \\(F = \\frac{\\text{SS}_{\\text{Reg}}/(p-1)}{\\text{SSE}/(n-p)}\\)\n\n\nOne-sided p-value:\n\\[\\text{p-value} = P(F_{p-1,n-p} \\geq f)\\] where \\(f\\) is our observed F-statistic"
  },
  {
    "objectID": "slides/06-slides.html#you-try-interpret-p-values",
    "href": "slides/06-slides.html#you-try-interpret-p-values",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Interpret P-values",
    "text": "You Try: Interpret P-values\n\nScenario: Testing \\(H_0: \\beta_1 = 0\\) with t = 2.8 and df = 18\n\n\nGiven: p-value = 0.012\n\n\nQuestions:\n\nWhat does this p-value mean?\n\nWould you reject \\(H_0\\) at α = 0.05?\n\nWould you reject \\(H_0\\) at α = 0.01?\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/06-slides.html#you-try-solution-3",
    "href": "slides/06-slides.html#you-try-solution-3",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Solution",
    "text": "You Try: Solution\n\nInterpretation: If \\(\\beta_1 = 0\\) were true, there’s only a 1.2% chance of seeing a t-statistic as extreme as ±2.8 or more extreme\n\n\nAt α = 0.05: Reject \\(H_0\\) (p = 0.012 &lt; 0.05) - Evidence suggests \\(\\beta_1 \\neq 0\\)\n\n\nAt α = 0.01: Fail to reject \\(H_0\\) (p = 0.012 &gt; 0.01) - Not enough evidence at this stricter level"
  },
  {
    "objectID": "slides/06-slides.html#the-key-functions",
    "href": "slides/06-slides.html#the-key-functions",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "The Key Functions",
    "text": "The Key Functions\n\nFor t-tests: Use pt() function\n\n\nFor F-tests: Use pf() function\n\n\nBoth give: Cumulative distribution function (CDF) values"
  },
  {
    "objectID": "slides/06-slides.html#t-test-p-values-with-pt",
    "href": "slides/06-slides.html#t-test-p-values-with-pt",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "t-test P-values with pt()",
    "text": "t-test P-values with pt()\n\nTwo-sided test: \\(H_0: \\beta_j = 0\\)\n\n\n# Example: t = 2.8, df = 18\nt_stat &lt;- 2.8\ndf &lt;- 18\n\n# Two-sided p-value\np_value_t &lt;- 2 * (1 - pt(abs(t_stat), df))\np_value_t\n\n[1] 0.01183672"
  },
  {
    "objectID": "slides/06-slides.html#t-test-p-values-with-pt-1",
    "href": "slides/06-slides.html#t-test-p-values-with-pt-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "t-test P-values with pt()",
    "text": "t-test P-values with pt()\n\nWhy the formula?\n\npt(2.8, 18) gives P(T ≤ 2.8)\n\n1 - pt(2.8, 18) gives P(T &gt; 2.8)\n\nMultiply by 2 for both tails"
  },
  {
    "objectID": "slides/06-slides.html#f-test-p-values-with-pf",
    "href": "slides/06-slides.html#f-test-p-values-with-pf",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "F-test P-values with pf()",
    "text": "F-test P-values with pf()\n\nOne-sided test: \\(H_0:\\) all slopes = 0\n\n\n# Example: F = 12.44, df1 = 3, df2 = 16\nf_stat &lt;- 12.44\ndf1 &lt;- 3  # numerator df (p-1)\ndf2 &lt;- 16 # denominator df (n-p)\n\n# One-sided p-value\np_value_f &lt;- 1 - pf(f_stat, df1, df2)\np_value_f\n\n[1] 0.0001879013\n\n# One-sided p-value\np_value_f &lt;- pf(f_stat, df1, df2, lower.tail = FALSE)\np_value_f\n\n[1] 0.0001879013"
  },
  {
    "objectID": "slides/06-slides.html#f-test-p-values-with-pf-1",
    "href": "slides/06-slides.html#f-test-p-values-with-pf-1",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "F-test P-values with pf()",
    "text": "F-test P-values with pf()\n\nWhy one-sided? F-statistics are always ≥ 0, so we only care about the right tail"
  },
  {
    "objectID": "slides/06-slides.html#you-try-calculate-p-values",
    "href": "slides/06-slides.html#you-try-calculate-p-values",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Calculate P-values",
    "text": "You Try: Calculate P-values\n\nGiven: - t-statistic = -1.96, df = 24 - F-statistic = 8.5, df1 = 2, df2 = 20\n\n\nCalculate both p-values using R\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/06-slides.html#you-try-solution-4",
    "href": "slides/06-slides.html#you-try-solution-4",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "You Try: Solution",
    "text": "You Try: Solution\n\n# t-test p-value (two-sided)\nt_val &lt;- -1.96\ndf_t &lt;- 24\np_t &lt;- 2 * pt(abs(t_val), df_t, lower.tail = FALSE)\ncat(\"t-test p-value:\", round(p_t, 4))\n\nt-test p-value: 0.0617\n\n# F-test p-value (one-sided)\nf_val &lt;- 8.5\ndf1_f &lt;- 2\ndf2_f &lt;- 20\np_f &lt;- pf(f_val, df1_f, df2_f, lower.tail = FALSE)\ncat(\"\\nF-test p-value:\", round(p_f, 4))\n\n\nF-test p-value: 0.0021\n\n\n\nInterpretation: - t-test: p = 0.0614 (not significant at α = 0.05) - F-test: p = 0.0021 (significant at α = 0.05)"
  },
  {
    "objectID": "slides/06-slides.html#p-value-cautions",
    "href": "slides/06-slides.html#p-value-cautions",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "P-value Cautions",
    "text": "P-value Cautions\n\nWhat p-values DON’T tell us:\n\nHow large or important the effect is\n\nWhether the relationship is causal\n\nWhether the model assumptions are met\n\n\n\nRemember: Statistical significance ≠ practical significance\n\n\nBest practice: Report both p-values AND effect sizes (\\(\\hat{\\beta}_j\\))"
  },
  {
    "objectID": "slides/06-slides.html#key-takeaways",
    "href": "slides/06-slides.html#key-takeaways",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nP-values help us:\n\nQuantify evidence against \\(H_0\\)\n\nMake consistent decisions about significance\n\nCommunicate strength of statistical evidence\n\n\n\nBut remember:\n\nContext matters more than arbitrary cutoffs\n\nConsider effect size alongside significance\n\nCheck model assumptions first"
  },
  {
    "objectID": "slides/06-slides.html#final-you-try-complete-analysis",
    "href": "slides/06-slides.html#final-you-try-complete-analysis",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Final You Try: Complete Analysis",
    "text": "Final You Try: Complete Analysis\n\nGiven: n = 20, p = 4, TSS = 1000, SSE = 300\n\n\nCalculate:\n- R²\n- Overall F-statistic\n- Conclude at α = 0.05 level\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/06-slides.html#final-solution",
    "href": "slides/06-slides.html#final-solution",
    "title": "Sums of Squares and Hypothesis Testing",
    "section": "Final Solution",
    "text": "Final Solution\n\nR² calculation: \\[R^2 = 1 - \\frac{SSE}{TSS} = 1 - \\frac{300}{1000} = 0.7\\]\n\n\nF-statistic: \\[F = \\frac{SS_{Reg}/(p-1)}{SSE/(n-p)} = \\frac{700/3}{300/16} = \\frac{233.33}{18.75} = 12.44\\]\n\n\nCritical value: \\(F_{0.05, 3, 16} = 3.24\\)\n\n\nConclusion: Reject \\(H_0\\). The model is statistically significant."
  },
  {
    "objectID": "slides/08-slides.html#the-basic-formula",
    "href": "slides/08-slides.html#the-basic-formula",
    "title": "Confidence Intervals",
    "section": "The Basic Formula",
    "text": "The Basic Formula\n\nFor coefficient \\(\\beta_j\\):\n\\[\\hat{\\beta}_j \\pm t_{\\alpha/2, n-p} \\times \\text{se}(\\hat{\\beta}_j)\\]\n\n\nWhere:\n- \\(t_{\\alpha/2, n-p}\\) is the critical value from t-distribution\n- \\(\\text{se}(\\hat{\\beta}_j)\\) is the standard error\n- \\(n - p\\) are the degrees of freedom"
  },
  {
    "objectID": "slides/08-slides.html#where-does-this-come-from",
    "href": "slides/08-slides.html#where-does-this-come-from",
    "title": "Confidence Intervals",
    "section": "Where Does This Come From?",
    "text": "Where Does This Come From?\n\n\\(\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1})\\)\n\n\nFor individual coefficient:\n\\[\\hat{\\beta}_j \\sim N(\\beta_j, \\sigma^2[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj})\\]"
  },
  {
    "objectID": "slides/08-slides.html#where-does-this-come-from-1",
    "href": "slides/08-slides.html#where-does-this-come-from-1",
    "title": "Confidence Intervals",
    "section": "Where Does This Come From?",
    "text": "Where Does This Come From?\n\\(\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1})\\)\nStandardize:\n\\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\sigma\\sqrt{[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/08-slides.html#the-t-distribution-connection",
    "href": "slides/08-slides.html#the-t-distribution-connection",
    "title": "Confidence Intervals",
    "section": "The t-Distribution Connection",
    "text": "The t-Distribution Connection\n\nProblem: We don’t know \\(\\sigma\\), so we estimate it with \\(\\hat{\\sigma}\\)\n\n\nResult: Replace \\(\\sigma\\) with \\(\\hat{\\sigma}\\) and the distribution changes\n\\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\hat{\\sigma}\\sqrt{[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}} \\sim t_{n-p}\\]"
  },
  {
    "objectID": "slides/08-slides.html#standard-error-calculation",
    "href": "slides/08-slides.html#standard-error-calculation",
    "title": "Confidence Intervals",
    "section": "Standard Error Calculation",
    "text": "Standard Error Calculation\n\n\\[\\text{se}(\\hat{\\beta}_j) = \\hat{\\sigma}\\sqrt{[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}}\\]\n\n\nWhere:\n- \\(\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}\\) (MSE)\n- \\([(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj}\\) is the \\(j\\)-th diagonal element"
  },
  {
    "objectID": "slides/08-slides.html#you-try-calculate-standard-error",
    "href": "slides/08-slides.html#you-try-calculate-standard-error",
    "title": "Confidence Intervals",
    "section": "You Try: Calculate Standard Error",
    "text": "You Try: Calculate Standard Error\n\nGiven:\n\\[(\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{bmatrix} 0.25 & -0.10 \\\\ -0.10 & 0.05 \\end{bmatrix}\\] \\[\\hat{\\sigma}^2 = 16\\]\n\n\nFind: \\(\\text{se}(\\hat{\\beta}_1)\\)\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/08-slides.html#constructing-the-95-ci",
    "href": "slides/08-slides.html#constructing-the-95-ci",
    "title": "Confidence Intervals",
    "section": "Constructing the 95% CI",
    "text": "Constructing the 95% CI\n\nFormula:\n\\[\\text{CI}_{95\\%} = \\hat{\\beta}_j \\pm t_{0.025, n-p} \\times \\text{se}(\\hat{\\beta}_j)\\]"
  },
  {
    "objectID": "slides/08-slides.html#example-ci-calculation",
    "href": "slides/08-slides.html#example-ci-calculation",
    "title": "Confidence Intervals",
    "section": "Example: CI Calculation",
    "text": "Example: CI Calculation\n\nGiven: \\(\\hat{\\beta}_1 = 3.2\\), \\(\\text{se}(\\hat{\\beta}_1) = 0.8\\), \\(n = 30\\), \\(p = 3\\)\n\n\nStep 1: Find critical value\n\\(t_{0.025, 27} = 2.052\\) (qt(0.975, 27))\n\n\nStep 2: Construct interval\n\\[\\text{CI}_{95\\%} = [3.2 - 2.052\\times0.8, 3.2 + 2.052\\times0.8] = [1.6, 4.8]\\]"
  },
  {
    "objectID": "slides/08-slides.html#interpreting-confidence-intervals",
    "href": "slides/08-slides.html#interpreting-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\n\nWhat this means:\nIf we repeated this procedure many times with different samples from the same population, 95% of the intervals would contain the true \\(\\beta_1\\)\n\n\nWhat we CANNOT say:\n“There is a 95% probability that \\(\\beta_1\\) is in this interval”\n(The parameter is fixed, the interval is random!)"
  },
  {
    "objectID": "slides/08-slides.html#you-try-build-a-confidence-interval",
    "href": "slides/08-slides.html#you-try-build-a-confidence-interval",
    "title": "Confidence Intervals",
    "section": "You Try: Build a Confidence Interval",
    "text": "You Try: Build a Confidence Interval\n\nGiven:\n- \\(\\hat{\\beta}_2 = 5.6\\)\n- \\(\\text{se}(\\hat{\\beta}_2) = 1.2\\)\n- \\(n = 25\\), \\(p = 4\\)\n- \\(t_{0.025, 21} = 2.08\\)\n\n\nFind: 95% confidence interval for \\(\\hat\\beta_2\\)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/08-slides.html#lets-do-it-in-r",
    "href": "slides/08-slides.html#lets-do-it-in-r",
    "title": "Confidence Intervals",
    "section": "Let’s do it in R",
    "text": "Let’s do it in R\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2.1, 3.9, 6.2, 7.8, 10.1)\nmodel &lt;- lm(y ~ x)\n\nconfint(model)\n\n                 2.5 %    97.5 %\n(Intercept) -0.5803601 0.6803601\nx            1.7999393 2.1800607\n\n# 90% confidence intervals\nconfint(model, level = 0.90)\n\n                   5 %      95 %\n(Intercept) -0.4161403 0.5161403\nx            1.8494534 2.1305466"
  },
  {
    "objectID": "slides/08-slides.html#manual-calculation-in-r",
    "href": "slides/08-slides.html#manual-calculation-in-r",
    "title": "Confidence Intervals",
    "section": "Manual Calculation in R",
    "text": "Manual Calculation in R\n\nbeta_hat &lt;- coef(model)\nse_beta &lt;- summary(model)$coefficients[, \"Std. Error\"]\nn &lt;- length(y)\np &lt;- length(beta_hat)\ndf &lt;- n - p\n\nt_crit &lt;- qt(0.975, df)  \n\nlower &lt;- beta_hat - t_crit * se_beta\nupper &lt;- beta_hat + t_crit * se_beta\n\ncbind(lower, upper)\n\n                 lower     upper\n(Intercept) -0.5803601 0.6803601\nx            1.7999393 2.1800607"
  },
  {
    "objectID": "slides/08-slides.html#your-turn",
    "href": "slides/08-slides.html#your-turn",
    "title": "Confidence Intervals",
    "section": "Your turn",
    "text": "Your turn\nUsing the data below fit a model and get 99% confidence intervals.\n\n# Data\nx1 &lt;- c(1, 2, 3, 4, 5, 6)\nx2 &lt;- c(2, 3, 3, 5, 6, 7)\ny &lt;- c(3, 5, 6, 8, 10, 11)\n\n# Your turn: fit model and get 99% CIs\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/08-slides.html#what-is-the-bootstrap",
    "href": "slides/08-slides.html#what-is-the-bootstrap",
    "title": "Confidence Intervals",
    "section": "What is the Bootstrap?",
    "text": "What is the Bootstrap?\n\nWe can use the data to understand its own variability\n\n\nHow it works:\n1. Resample your data (with replacement)\n2. Fit model to resampled data\n3. Record \\(\\hat{\\beta}\\)\n4. Repeat many times (typically 1000+)\n5. Use distribution of \\(\\hat{\\beta}\\) values to build CI"
  },
  {
    "objectID": "slides/08-slides.html#types-of-bootstrap-cis",
    "href": "slides/08-slides.html#types-of-bootstrap-cis",
    "title": "Confidence Intervals",
    "section": "Types of Bootstrap CIs",
    "text": "Types of Bootstrap CIs\n\n1. Percentile method Take quantiles of bootstrap distribution\n\n\n2. Bootstrap t\nTake the SE from bootstrap distribution and use a critical t-value to construct an interval\n\n\n3. Bias-corrected and accelerated (BCa)\nAdjusts for bias and skewness (most accurate)"
  },
  {
    "objectID": "slides/08-slides.html#percentile-bootstrap-method",
    "href": "slides/08-slides.html#percentile-bootstrap-method",
    "title": "Confidence Intervals",
    "section": "Percentile Bootstrap Method",
    "text": "Percentile Bootstrap Method\n\nAlgorithm:\n1. Generate B bootstrap samples (e.g., B = 1000)\n2. For each sample, compute \\(\\hat{\\beta}_j^{(b)}\\)\n3. Sort all B values\n4. Take 2.5th and 97.5th percentiles for 95% CI\n\n\nCI formula:\n\\[\\text{CI}_{95\\%} = [Q_{0.025}, Q_{0.975}]\\] where \\(Q_\\alpha\\) is the \\(\\alpha\\)-quantile of bootstrap distribution"
  },
  {
    "objectID": "slides/08-slides.html#bootstrap-algorithm-detail",
    "href": "slides/08-slides.html#bootstrap-algorithm-detail",
    "title": "Confidence Intervals",
    "section": "Bootstrap Algorithm Detail",
    "text": "Bootstrap Algorithm Detail\n\nFor each bootstrap iteration \\(b = 1, \\ldots, B\\):\n\n\n\nResample indices: \\(i_1^{(b)}, \\ldots, i_n^{(b)}\\) from \\(\\{1, \\ldots, n\\}\\) with replacement\n\n\n\n\n\nCreate bootstrap sample: \\(\\mathbf{y}^{(b)} = (y_{i_1^{(b)}}, \\ldots, y_{i_n^{(b)}})^T\\)\n\\(\\mathbf{X}^{(b)} = (\\mathbf{x}_{i_1^{(b)}}, \\ldots, \\mathbf{x}_{i_n^{(b)}})^T\\)\n\n\n\n\n\nFit model: \\(\\hat{\\boldsymbol{\\beta}}^{(b)} = ((\\mathbf{X}^{(b)})^T\\mathbf{X}^{(b)})^{-1}(\\mathbf{X}^{(b)})^T\\mathbf{y}^{(b)}\\)"
  },
  {
    "objectID": "slides/08-slides.html#implementing-bootstrap-in-r",
    "href": "slides/08-slides.html#implementing-bootstrap-in-r",
    "title": "Confidence Intervals",
    "section": "Implementing Bootstrap in R",
    "text": "Implementing Bootstrap in R\n\nset.seed(1)\nx &lt;- 1:20\ny &lt;- 2 + 3*x + rnorm(20, 0, 5)\ndata &lt;- data.frame(x, y)\n\nmodel &lt;- lm(y ~ x, data)\nbeta_hat &lt;- coef(model)\n\nB &lt;- 1000\n\nbootstrap_one &lt;- function(data) {\n  boot_data &lt;- data[sample(1:nrow(data), replace = TRUE), ]\n  boot_model &lt;- lm(y ~ x, data = boot_data)\n  boot_beta_hat &lt;- coef(boot_model)\n}\n\nboot_betas &lt;- purrr::map(1:B, ~bootstrap_one(data)) |&gt; \n  dplyr::bind_rows()\nboot_betas\n\n# A tibble: 1,000 × 2\n   `(Intercept)`     x\n           &lt;dbl&gt; &lt;dbl&gt;\n 1        -2.01   3.28\n 2         2.64   3.02\n 3         0.256  3.27\n 4        -0.131  3.08\n 5         2.07   3.17\n 6         1.60   3.11\n 7         0.584  3.18\n 8         4.55   2.80\n 9         4.14   2.86\n10         2.25   3.01\n# ℹ 990 more rows"
  },
  {
    "objectID": "slides/08-slides.html#bootstrap-cis",
    "href": "slides/08-slides.html#bootstrap-cis",
    "title": "Confidence Intervals",
    "section": "Bootstrap CIs",
    "text": "Bootstrap CIs\n\nboot_ci_intercept &lt;- quantile(boot_betas$`(Intercept)`, c(0.025, 0.975))\nboot_ci_slope &lt;- quantile(boot_betas$x, c(0.025, 0.975))\n\ncat(\"Bootstrap CI for intercept:\", round(boot_ci_intercept, 3), \"\\n\")\n\nBootstrap CI for intercept: -1.261 5.968 \n\ncat(\"Bootstrap CI for slope:\", round(boot_ci_slope, 3), \"\\n\")\n\nBootstrap CI for slope: 2.755 3.351 \n\n# Compare to traditional CI\nconfint(model)\n\n                2.5 %   97.5 %\n(Intercept) -2.714035 6.353106\nx            2.729458 3.486368"
  },
  {
    "objectID": "slides/08-slides.html#visualizing-bootstrap-distribution",
    "href": "slides/08-slides.html#visualizing-bootstrap-distribution",
    "title": "Confidence Intervals",
    "section": "Visualizing Bootstrap Distribution",
    "text": "Visualizing Bootstrap Distribution"
  },
  {
    "objectID": "slides/08-slides.html#your-turn-1",
    "href": "slides/08-slides.html#your-turn-1",
    "title": "Confidence Intervals",
    "section": "Your Turn",
    "text": "Your Turn\nCompute a bootstrap 90% CI for the slope\n\n# Use this data\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)\ny &lt;- c(2.1, 3.8, 6.2, 7.9, 9.8, 12.1, 14.3, 15.8)\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/08-slides.html#what-is-coverage",
    "href": "slides/08-slides.html#what-is-coverage",
    "title": "Confidence Intervals",
    "section": "What is Coverage?",
    "text": "What is Coverage?\n\nDefinition: The proportion of confidence intervals that contain the true parameter\n\n\nIdeal coverage: Should match confidence level\n- 95% CI should have 95% coverage\n- 90% CI should have 90% coverage\n\n\nIn practice: Coverage can be lower due to:\n- Violated assumptions\n- Small sample sizes\n- Model misspecification"
  },
  {
    "objectID": "slides/08-slides.html#checking-coverage-via-simulation",
    "href": "slides/08-slides.html#checking-coverage-via-simulation",
    "title": "Confidence Intervals",
    "section": "Checking Coverage via Simulation",
    "text": "Checking Coverage via Simulation\n\nThe procedure:\n1. Choose true parameter values\n2. Generate data from the model\n3. Compute confidence interval\n4. Check if true value is inside\n5. Repeat many times\n6. Calculate proportion containing truth"
  },
  {
    "objectID": "slides/08-slides.html#coverage-simulation-example",
    "href": "slides/08-slides.html#coverage-simulation-example",
    "title": "Confidence Intervals",
    "section": "Coverage Simulation Example",
    "text": "Coverage Simulation Example\n\nset.seed(1)\nn_sims &lt;- 1000\n\nsim &lt;- function(true_beta0 = 2, true_beta1 = 1, sigma = 5, n = 30) {\n  # Generate data\n  x &lt;- runif(n, 0, 10)\n  y &lt;- true_beta0 + true_beta1 * x + rnorm(n, 0, sigma)\n  \n  # Fit model and get CI\n  model &lt;- lm(y ~ x)\n  ci &lt;- confint(model, level = 0.95)[2, ]  # CI for slope\n  \n  # Check coverage\n  ci[1] &lt;= true_beta1 && true_beta1 &lt;= ci[2]\n}\n\ncoverage &lt;- purrr::map_lgl(1:n_sims, ~sim())\nmean(coverage)\n\n[1] 0.957"
  },
  {
    "objectID": "slides/08-slides.html#interpreting-coverage-results",
    "href": "slides/08-slides.html#interpreting-coverage-results",
    "title": "Confidence Intervals",
    "section": "Interpreting Coverage Results",
    "text": "Interpreting Coverage Results\n\nOur result: ~95% coverage (as expected!)\n\n\nIf coverage is too low (&lt;95%):\n- Standard errors may be underestimated\n- Assumptions may be violated\n- Consider robust methods or bootstrap\n\n\nIf coverage is too high (&gt;95%):\n- CIs are conservative (too wide)\n- Losing power\n- May be okay, but inefficient"
  },
  {
    "objectID": "slides/08-slides.html#factors-affecting-coverage",
    "href": "slides/08-slides.html#factors-affecting-coverage",
    "title": "Confidence Intervals",
    "section": "Factors Affecting Coverage",
    "text": "Factors Affecting Coverage\n\nSample size:\n- Small n can have poor coverage\n- Large n coverage approaches nominal level\n\n\nAssumption violations:\n- Non-normality (less important with large n)\n- Heteroscedasticity - Outliers\n\n\nModel misspecification:\n- Wrong functional form\n- Missing important variables"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Policies",
    "section": "",
    "text": "This syllabus and the dates herein are subject to change.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#class",
    "href": "syllabus.html#class",
    "title": " Policies",
    "section": "Class",
    "text": "Class\nThis class is meant to be hands on. Therefore, you are responsible for reading pertinent material prior to each class.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#attendance",
    "href": "syllabus.html#attendance",
    "title": " Policies",
    "section": "Attendance",
    "text": "Attendance\nIf you feel unwell please do not come to class. All class material will be posted after class and I would be happy to meet outside of our class time to help you catch up if needbe.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#accessibility-policy",
    "href": "syllabus.html#accessibility-policy",
    "title": " Policies",
    "section": "Accessibility Policy",
    "text": "Accessibility Policy\nWake Forest University provides reasonable accommodations for students with disabilities. Academic accommodations are coordinated through the Center for Learning, Access, and Student Success (CLASS). If you would like to request accommodations for this course, you should contact CLASS as early in the semester as possible (class.wfu.edu,118 Reynolda Hall). Please contact me privately after sending notifications through the CLASS office Student Portal system. Retroactive accommodations will not be provided.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#how-to-get-help",
    "href": "syllabus.html#how-to-get-help",
    "title": " Policies",
    "section": "How to get help",
    "text": "How to get help\n\nDiscussion:\nAll course discussion will be on Canvas.\nThis is a place to post your course-related questions. I encourage you to try to answer each other’s questions. At the end of the semester, I will tally up the total number of questions answered and you can get up to 1 point extra credit on your final grade.\n\n\nMath & Stats Center:\n\nMake an appointment: https://mathandstatscenter.wfu.edu/",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#honor-code",
    "href": "syllabus.html#honor-code",
    "title": " Policies",
    "section": "Honor code",
    "text": "Honor code\nWake Forest University is committed to a culture of academic integrity. As a part of this community, you share the responsibility for creating a place of honesty, intellectual curiosity, and individual accountability. As you committed to with your honor pledge signature, you agree “not to deceive any member of the community; not to steal, cheat, or plagiarize on academic work; and not to engage in any other form of academic misconduct.” If you have questions about documenting your work, working with external sources, working with peers on assigned work, etc., consult with me as soon as possible. Instances of academic dishonesty will be referred to the Honor and Ethics Council.\n\nSharing code & responses\n\nYou may use online resources (e.g., StackOverflow, ChatGPT) for help. For projects, you must explicitly cite any borrowed or adapted code. This includes both directly copied code and code you modified for your own work. Un-cited reuse of code will be treated as plagiarism.\n\nFor exercises and in-class worked problems, you are welcome to use resources freely, but copying without understanding is strongly discouraged.\n\nRather than relying on someone else’s work, ask for help — you are not alone in this course.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#course-components",
    "href": "syllabus.html#course-components",
    "title": " Policies",
    "section": "Course components",
    "text": "Course components\n\nExercises\nPractice problems designed to reinforce concepts. These are graded for completion.\n\n\nIn-class worked problems\nOn most Wednesdays, class time will be devoted to group work on the assigned exercise problems. Each week, you will be placed in groups of 2–3 students. One problem from the previous week’s homework will be randomly assigned to your group. One member of the group will then present the solution to the class. Over the course of the semester, each student is required to present at least three times.\nGrading Rubric (Presenter):\n1 – Minimal: Attempted presentation but showed little preparation; major errors or unclear explanation.\n2 – Adequate: Mostly correct solution with a few minor errors.\n3 – Mastered: Fully correct solution; clear, accurate, and organized explanation; addressed the key steps of the solution; responded adequately to questions.\nGrading Rubric (Group Member):\n1 – Present\n0 – Absent (unexcused)\n\n\nMidterm\nWe will have one in-class midterm.\n\n\nFinal Project\nA cumulative project integrating skills and knowledge from across the course.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": " Policies",
    "section": "Grading",
    "text": "Grading\nYour final grade will be determined as follows:\n\n\n\nComponent\nWeight\n\n\n\n\nExercises\n10%\n\n\nIn-class worked problems\n30%\n\n\nMidterm\n30%\n\n\nFinal Project\n30%\n\n\nTotal\n100%\n\n\n\nGrades conversion:\n\n\n\nLetter\nNumeric\n\n\n\n\nA\n&gt; 94\n\n\nA-\n90 - 94\n\n\nB+\n87 - 89\n\n\nB\n83 - 86\n\n\nB-\n80 - 82\n\n\nC+\n77 - 79\n\n\nC\n73 - 76\n\n\nC-\n70 - 72\n\n\nD+\n67 - 69\n\n\nD\n65 - 66\n\n\nF\nBelow 65",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  },
  {
    "objectID": "syllabus.html#late-missed-work",
    "href": "syllabus.html#late-missed-work",
    "title": " Policies",
    "section": "Late / missed work",
    "text": "Late / missed work\n\nLate work policy for labs:\n\nlate, but within 24 hours of due date/time: -50%\nany later: no credit\n\nNo make-up assessments will be given.\nAll regrade requests must be discussed with the professor within one week of receiving your grade. There will be no grade changes after the final class.",
    "crumbs": [
      "{{< fa calendar >}} Schedule",
      "Course Details",
      "Course Policies"
    ]
  }
]